{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Churn Prediction with XGBoost\n",
    "\n",
    "**Objective**: Maximize Balanced Accuracy\n",
    "\n",
    "**Key considerations**:\n",
    "- No data leakage: removed `days_since_last_activity` (consequence of churn, not cause)\n",
    "- No `cancel` page features (cancel = churn confirmation)\n",
    "- Activity patterns instead of recency features\n",
    "- XGBoost with threshold optimization for balanced accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, roc_auc_score, f1_score, \n",
    "    confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Constants\n",
    "REFERENCE_DATE = pd.to_datetime('2018-11-20')\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df_train = pd.read_csv('df_train_sample.csv')\n",
    "df_train['time'] = pd.to_datetime(df_train['time'])\n",
    "df_train['registration'] = pd.to_datetime(df_train['registration'])\n",
    "\n",
    "print(f\"ğŸ“Š Training data: {len(df_train):,} events\")\n",
    "print(f\"ğŸ‘¥ Unique users: {df_train['userId'].nunique():,}\")\n",
    "print(f\"ğŸ¯ Churn rate: {df_train.groupby('userId')['will_churn_10days'].first().mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "### Key design decisions:\n",
    "1. **No `days_since_last_activity`**: This leaks information because churners naturally have longer inactivity (consequence, not cause)\n",
    "2. **No `cancel` page features**: Cancel confirmation = churn, so it's direct leakage\n",
    "3. **Activity patterns**: Use average gaps, consistency scores, and regularity metrics instead of recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, reference_date):\n",
    "    \"\"\"\n",
    "    Create user-level features for churn prediction.\n",
    "    \n",
    "    Args:\n",
    "        df: Event-level DataFrame\n",
    "        reference_date: Date to calculate features up to\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with one row per user and all features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter events up to reference date\n",
    "    df_filtered = df[df['time'] <= reference_date].copy()\n",
    "    \n",
    "    # Initialize user DataFrame\n",
    "    user_features = pd.DataFrame()\n",
    "    user_features['userId'] = df_filtered['userId'].unique()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # BASIC USER INFO\n",
    "    # =========================================================================\n",
    "    \n",
    "    user_info = df_filtered.groupby('userId').agg({\n",
    "        'gender': 'first',\n",
    "        'level': 'last',  # Current subscription level\n",
    "        'registration': 'first',\n",
    "        'will_churn_10days': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    user_features = user_features.merge(user_info, on='userId', how='left')\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    user_features['gender_encoded'] = LabelEncoder().fit_transform(\n",
    "        user_features['gender'].fillna('unknown')\n",
    "    )\n",
    "    user_features['level_encoded'] = LabelEncoder().fit_transform(\n",
    "        user_features['level'].fillna('unknown')\n",
    "    )\n",
    "    user_features['is_paid'] = (user_features['level'] == 'paid').astype(int)\n",
    "    \n",
    "    # Account age\n",
    "    user_features['days_since_registration'] = (\n",
    "        reference_date - user_features['registration']\n",
    "    ).dt.days\n",
    "    \n",
    "    # =========================================================================\n",
    "    # GLOBAL ACTIVITY METRICS\n",
    "    # =========================================================================\n",
    "    \n",
    "    activity = df_filtered.groupby('userId').agg({\n",
    "        'ts': 'count',\n",
    "        'sessionId': 'nunique',\n",
    "        'time': ['min', 'max'],\n",
    "        'length': ['sum', 'mean', 'count'],\n",
    "    })\n",
    "    activity.columns = [\n",
    "        'total_events', 'unique_sessions', \n",
    "        'first_activity', 'last_activity',\n",
    "        'total_listening_time', 'avg_song_length', 'songs_with_length'\n",
    "    ]\n",
    "    activity = activity.reset_index()\n",
    "    user_features = user_features.merge(activity, on='userId', how='left')\n",
    "    \n",
    "    # Activity span (how long the user has been active)\n",
    "    user_features['activity_span_days'] = (\n",
    "        pd.to_datetime(user_features['last_activity']) - \n",
    "        pd.to_datetime(user_features['first_activity'])\n",
    "    ).dt.days + 1\n",
    "    \n",
    "    # Average activity rates\n",
    "    user_features['events_per_day'] = (\n",
    "        user_features['total_events'] / user_features['activity_span_days']\n",
    "    ).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    user_features['sessions_per_day'] = (\n",
    "        user_features['unique_sessions'] / user_features['activity_span_days']\n",
    "    ).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    user_features['events_per_session'] = (\n",
    "        user_features['total_events'] / user_features['unique_sessions']\n",
    "    ).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    user_features['listening_time_per_session'] = (\n",
    "        user_features['total_listening_time'] / user_features['unique_sessions']\n",
    "    ).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # ACTIVITY PATTERN FEATURES (replacing days_since_last_activity)\n",
    "    # These capture user behavior patterns without leaking churn information\n",
    "    # =========================================================================\n",
    "    \n",
    "    def calculate_activity_patterns(group):\n",
    "        \"\"\"Calculate session gap patterns for a user.\"\"\"\n",
    "        sessions = group.groupby('sessionId')['time'].min().sort_values()\n",
    "        \n",
    "        if len(sessions) < 2:\n",
    "            return pd.Series({\n",
    "                'avg_gap_between_sessions': 0,\n",
    "                'std_gap_between_sessions': 0,\n",
    "                'max_gap_between_sessions': 0,\n",
    "                'min_gap_between_sessions': 0,\n",
    "                'median_gap_between_sessions': 0,\n",
    "            })\n",
    "        \n",
    "        # Calculate gaps between consecutive sessions (in days)\n",
    "        gaps = sessions.diff().dropna().dt.total_seconds() / (24 * 3600)\n",
    "        \n",
    "        return pd.Series({\n",
    "            'avg_gap_between_sessions': gaps.mean(),\n",
    "            'std_gap_between_sessions': gaps.std() if len(gaps) > 1 else 0,\n",
    "            'max_gap_between_sessions': gaps.max(),\n",
    "            'min_gap_between_sessions': gaps.min(),\n",
    "            'median_gap_between_sessions': gaps.median(),\n",
    "        })\n",
    "    \n",
    "    activity_patterns = df_filtered.groupby('userId').apply(\n",
    "        calculate_activity_patterns\n",
    "    ).reset_index()\n",
    "    user_features = user_features.merge(activity_patterns, on='userId', how='left')\n",
    "    \n",
    "    # Consistency score: lower std/mean ratio = more consistent user\n",
    "    user_features['consistency_score'] = 1 / (\n",
    "        1 + user_features['std_gap_between_sessions'] / \n",
    "        (user_features['avg_gap_between_sessions'] + 0.1)\n",
    "    )\n",
    "    \n",
    "    # Gap variability (coefficient of variation)\n",
    "    user_features['gap_variability'] = (\n",
    "        user_features['std_gap_between_sessions'] / \n",
    "        (user_features['avg_gap_between_sessions'] + 0.1)\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PAGE INTERACTIONS (excluding 'Cancel' - it's leakage)\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Pages to track (NO CANCEL)\n",
    "    pages_to_track = [\n",
    "        'NextSong', 'Home', 'Thumbs Up', 'Thumbs Down', \n",
    "        'Add to Playlist', 'Add Friend', 'Roll Advert', \n",
    "        'Downgrade', 'Submit Downgrade', 'Upgrade',\n",
    "        'Error', 'Help', 'Settings', 'Logout'\n",
    "    ]\n",
    "    \n",
    "    page_counts = df_filtered.groupby(['userId', 'page']).size().unstack(fill_value=0)\n",
    "    \n",
    "    for page in pages_to_track:\n",
    "        col_name = f'page_{page.lower().replace(\" \", \"_\")}'\n",
    "        if page in page_counts.columns:\n",
    "            temp = page_counts[[page]].reset_index()\n",
    "            temp.columns = ['userId', col_name]\n",
    "            user_features = user_features.merge(temp, on='userId', how='left')\n",
    "            user_features[col_name] = user_features[col_name].fillna(0)\n",
    "        else:\n",
    "            user_features[col_name] = 0\n",
    "    \n",
    "    # =========================================================================\n",
    "    # ENGAGEMENT METRICS\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Positive interactions\n",
    "    user_features['positive_interactions'] = (\n",
    "        user_features['page_thumbs_up'] + \n",
    "        user_features['page_add_to_playlist'] + \n",
    "        user_features['page_add_friend']\n",
    "    )\n",
    "    \n",
    "    # Negative interactions\n",
    "    user_features['negative_interactions'] = (\n",
    "        user_features['page_thumbs_down'] + \n",
    "        user_features['page_downgrade'] + \n",
    "        user_features['page_submit_downgrade']\n",
    "    )\n",
    "    \n",
    "    # Interaction rates\n",
    "    user_features['positive_interaction_rate'] = (\n",
    "        user_features['positive_interactions'] / user_features['total_events']\n",
    "    ).fillna(0)\n",
    "    \n",
    "    user_features['negative_interaction_rate'] = (\n",
    "        user_features['negative_interactions'] / user_features['total_events']\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Thumbs ratio (sentiment indicator)\n",
    "    user_features['thumbs_ratio'] = (\n",
    "        user_features['page_thumbs_down'] / \n",
    "        (user_features['page_thumbs_up'] + 1)\n",
    "    )\n",
    "    \n",
    "    # Song listening ratio\n",
    "    user_features['song_event_ratio'] = (\n",
    "        user_features['page_nextsong'] / user_features['total_events']\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Error rate\n",
    "    user_features['error_rate'] = (\n",
    "        user_features['page_error'] / user_features['total_events']\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Help seeking rate\n",
    "    user_features['help_rate'] = (\n",
    "        user_features['page_help'] / user_features['total_events']\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Churn signals (without cancel)\n",
    "    user_features['churn_signals'] = (\n",
    "        user_features['page_downgrade'] + \n",
    "        user_features['page_submit_downgrade']\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TEMPORAL FEATURES (multi-period activity)\n",
    "    # =========================================================================\n",
    "    \n",
    "    periods = [3, 7, 14, 21]\n",
    "    \n",
    "    for days in periods:\n",
    "        period_start = reference_date - pd.Timedelta(days=days)\n",
    "        period_data = df_filtered[df_filtered['time'] >= period_start]\n",
    "        \n",
    "        # Events in period\n",
    "        period_events = period_data.groupby('userId').size().reset_index()\n",
    "        period_events.columns = ['userId', f'events_last_{days}d']\n",
    "        user_features = user_features.merge(period_events, on='userId', how='left')\n",
    "        user_features[f'events_last_{days}d'] = user_features[f'events_last_{days}d'].fillna(0)\n",
    "        \n",
    "        # Sessions in period\n",
    "        period_sessions = period_data.groupby('userId')['sessionId'].nunique().reset_index()\n",
    "        period_sessions.columns = ['userId', f'sessions_last_{days}d']\n",
    "        user_features = user_features.merge(period_sessions, on='userId', how='left')\n",
    "        user_features[f'sessions_last_{days}d'] = user_features[f'sessions_last_{days}d'].fillna(0)\n",
    "        \n",
    "        # Thumbs down in period\n",
    "        period_thumbs_down = period_data[period_data['page'] == 'Thumbs Down'].groupby('userId').size().reset_index()\n",
    "        period_thumbs_down.columns = ['userId', f'thumbs_down_last_{days}d']\n",
    "        user_features = user_features.merge(period_thumbs_down, on='userId', how='left')\n",
    "        user_features[f'thumbs_down_last_{days}d'] = user_features[f'thumbs_down_last_{days}d'].fillna(0)\n",
    "    \n",
    "    # Activity trends (comparing recent vs older activity)\n",
    "    user_features['trend_7d_vs_14d'] = (\n",
    "        user_features['events_last_7d'] / \n",
    "        (user_features['events_last_14d'] - user_features['events_last_7d'] + 1)\n",
    "    ).replace([np.inf, -np.inf], 0).clip(-10, 10)\n",
    "    \n",
    "    user_features['trend_3d_vs_7d'] = (\n",
    "        user_features['events_last_3d'] / \n",
    "        (user_features['events_last_7d'] - user_features['events_last_3d'] + 1)\n",
    "    ).replace([np.inf, -np.inf], 0).clip(-10, 10)\n",
    "    \n",
    "    # Activity acceleration (is decline speeding up?)\n",
    "    user_features['activity_acceleration'] = (\n",
    "        user_features['trend_3d_vs_7d'] - user_features['trend_7d_vs_14d']\n",
    "    )\n",
    "    \n",
    "    # Recent activity ratio\n",
    "    user_features['recent_activity_ratio'] = (\n",
    "        user_features['events_last_7d'] / (user_features['total_events'] + 1)\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # NEGATIVE ACTIONS PATTERNS\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Average negative actions per day (overall)\n",
    "    user_features['avg_negative_actions_per_day'] = (\n",
    "        user_features['negative_interactions'] / user_features['activity_span_days']\n",
    "    ).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    # Negative actions last 7d compared to average\n",
    "    # First calculate negative actions in last 7 days\n",
    "    last_7d_start = reference_date - pd.Timedelta(days=7)\n",
    "    negative_pages = ['Thumbs Down', 'Downgrade', 'Submit Downgrade']\n",
    "    negative_last_7d = df_filtered[\n",
    "        (df_filtered['time'] >= last_7d_start) & \n",
    "        (df_filtered['page'].isin(negative_pages))\n",
    "    ].groupby('userId').size().reset_index()\n",
    "    negative_last_7d.columns = ['userId', 'negative_actions_last_7d']\n",
    "    user_features = user_features.merge(negative_last_7d, on='userId', how='left')\n",
    "    user_features['negative_actions_last_7d'] = user_features['negative_actions_last_7d'].fillna(0)\n",
    "    \n",
    "    # Ratio: recent negative actions vs average\n",
    "    user_features['negative_actions_last7d_vs_avg'] = (\n",
    "        (user_features['negative_actions_last_7d'] / 7) / \n",
    "        (user_features['avg_negative_actions_per_day'] + 0.01)\n",
    "    ).replace([np.inf, -np.inf], 0).clip(0, 100)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # THUMBS UP RECENCY (without leakage)\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Days since last thumbs up (calculated from activity span, not reference date)\n",
    "    thumbs_up_events = df_filtered[df_filtered['page'] == 'Thumbs Up']\n",
    "    last_thumbs_up = thumbs_up_events.groupby('userId')['time'].max().reset_index()\n",
    "    last_thumbs_up.columns = ['userId', 'last_thumbs_up_time']\n",
    "    user_features = user_features.merge(last_thumbs_up, on='userId', how='left')\n",
    "    \n",
    "    # Days between last thumbs up and last activity (not reference date - avoids leakage)\n",
    "    user_features['days_without_thumbs_up'] = (\n",
    "        pd.to_datetime(user_features['last_activity']) - \n",
    "        pd.to_datetime(user_features['last_thumbs_up_time'])\n",
    "    ).dt.days\n",
    "    \n",
    "    # For users who never gave thumbs up, use activity span\n",
    "    user_features['days_without_thumbs_up'] = user_features['days_without_thumbs_up'].fillna(\n",
    "        user_features['activity_span_days']\n",
    "    )\n",
    "    \n",
    "    # Normalize by activity span\n",
    "    user_features['thumbs_up_recency_ratio'] = (\n",
    "        user_features['days_without_thumbs_up'] / \n",
    "        (user_features['activity_span_days'] + 1)\n",
    "    ).clip(0, 1)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # BINARY RISK INDICATORS\n",
    "    # =========================================================================\n",
    "    \n",
    "    user_features['has_downgraded'] = (user_features['page_downgrade'] > 0).astype(int)\n",
    "    user_features['has_errors'] = (user_features['page_error'] > 0).astype(int)\n",
    "    user_features['has_sought_help'] = (user_features['page_help'] > 0).astype(int)\n",
    "    user_features['is_low_engagement'] = (user_features['positive_interactions'] < 1).astype(int)\n",
    "    user_features['is_declining'] = (user_features['trend_7d_vs_14d'] < 0.5).astype(int)\n",
    "    user_features['has_negative_sentiment'] = (user_features['thumbs_ratio'] > 0.5).astype(int)\n",
    "    user_features['is_inactive_7d'] = (user_features['events_last_7d'] == 0).astype(int)\n",
    "    user_features['is_inactive_14d'] = (user_features['events_last_14d'] == 0).astype(int)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # COMPOSITE SCORES\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Frustration score\n",
    "    user_features['frustration_score'] = (\n",
    "        user_features['thumbs_ratio'] * 2 +\n",
    "        user_features['help_rate'] * 3 +\n",
    "        user_features['error_rate'] * 2 +\n",
    "        user_features['has_downgraded'] * 5\n",
    "    )\n",
    "    \n",
    "    # Engagement score (higher = more engaged)\n",
    "    user_features['engagement_score'] = (\n",
    "        user_features['positive_interaction_rate'] * 3 +\n",
    "        user_features['song_event_ratio'] * 2 +\n",
    "        user_features['consistency_score'] * 2 +\n",
    "        (1 - user_features['thumbs_up_recency_ratio']) * 1\n",
    "    )\n",
    "    \n",
    "    # Risk score\n",
    "    user_features['risk_score'] = (\n",
    "        user_features['frustration_score'] +\n",
    "        user_features['negative_actions_last7d_vs_avg'] * 2 +\n",
    "        user_features['is_declining'] * 3 +\n",
    "        user_features['is_inactive_7d'] * 2\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CLEANUP\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    cols_to_drop = [\n",
    "        'registration', 'first_activity', 'last_activity', \n",
    "        'gender', 'level', 'last_thumbs_up_time'\n",
    "    ]\n",
    "    user_features = user_features.drop(columns=[c for c in cols_to_drop if c in user_features.columns])\n",
    "    \n",
    "    # Fill any remaining NaN\n",
    "    numeric_cols = user_features.select_dtypes(include=[np.number]).columns\n",
    "    user_features[numeric_cols] = user_features[numeric_cols].fillna(0)\n",
    "    \n",
    "    return user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "user_df = create_features(df_train, REFERENCE_DATE)\n",
    "\n",
    "print(f\"âœ… Features created: {user_df.shape}\")\n",
    "print(f\"\\nğŸ“Š Feature columns ({len(user_df.columns) - 2} features + userId + target):\")\n",
    "print(user_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data check\n",
    "print(\"ğŸ“Š Target distribution:\")\n",
    "print(user_df['will_churn_10days'].value_counts())\n",
    "print(f\"\\nChurn rate: {user_df['will_churn_10days'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (exclude userId and target)\n",
    "exclude_cols = ['userId', 'will_churn_10days']\n",
    "feature_cols = [col for col in user_df.columns if col not in exclude_cols]\n",
    "\n",
    "X = user_df[feature_cols]\n",
    "y = user_df['will_churn_10days']\n",
    "\n",
    "print(f\"ğŸ“Š Features: {X.shape[1]}\")\n",
    "print(f\"ğŸ“Š Samples: {X.shape[0]}\")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ”¹ Training set: {len(X_train)} samples\")\n",
    "print(f\"ğŸ”¹ Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scale_pos_weight for imbalanced data\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"âš–ï¸ Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=RANDOM_STATE,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=cv, scoring='balanced_accuracy')\n",
    "\n",
    "print(f\"\\nğŸ“Š Cross-validation Balanced Accuracy:\")\n",
    "print(f\"   Mean: {cv_scores.mean():.4f}\")\n",
    "print(f\"   Std:  {cv_scores.std():.4f}\")\n",
    "print(f\"   Scores: {cv_scores.round(4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate with default threshold (0.5)\n",
    "print(\"ğŸ“Š Results with default threshold (0.5):\")\n",
    "print(f\"   Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"   ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "print(f\"   F1 Score: {f1_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Threshold Optimization for Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_proba, metric='balanced_accuracy'):\n",
    "    \"\"\"\n",
    "    Find the optimal threshold to maximize balanced accuracy.\n",
    "    \"\"\"\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    scores = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_proba >= thresh).astype(int)\n",
    "        score = balanced_accuracy_score(y_true, y_pred)\n",
    "        scores.append(score)\n",
    "    \n",
    "    best_idx = np.argmax(scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_score = scores[best_idx]\n",
    "    \n",
    "    return best_threshold, best_score, thresholds, scores\n",
    "\n",
    "# Find optimal threshold\n",
    "best_thresh, best_score, thresholds, scores = find_optimal_threshold(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"ğŸ¯ Optimal threshold: {best_thresh:.2f}\")\n",
    "print(f\"ğŸ¯ Best Balanced Accuracy: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize threshold optimization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Balanced accuracy vs threshold\n",
    "ax1 = axes[0]\n",
    "ax1.plot(thresholds, scores, 'b-', linewidth=2)\n",
    "ax1.axvline(x=best_thresh, color='red', linestyle='--', label=f'Optimal: {best_thresh:.2f}')\n",
    "ax1.axvline(x=0.5, color='gray', linestyle=':', label='Default: 0.50')\n",
    "ax1.scatter([best_thresh], [best_score], color='red', s=100, zorder=5)\n",
    "ax1.set_xlabel('Threshold', fontsize=12)\n",
    "ax1.set_ylabel('Balanced Accuracy', fontsize=12)\n",
    "ax1.set_title('Threshold Optimization', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Confusion matrix with optimal threshold\n",
    "ax2 = axes[1]\n",
    "y_pred_optimal = (y_pred_proba >= best_thresh).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'])\n",
    "ax2.set_xlabel('Predicted', fontsize=12)\n",
    "ax2.set_ylabel('Actual', fontsize=12)\n",
    "ax2.set_title(f'Confusion Matrix (threshold={best_thresh:.2f})', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('threshold_optimization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation with optimal threshold\n",
    "y_pred_optimal = (y_pred_proba >= best_thresh).astype(int)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š FINAL RESULTS (Optimal Threshold)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ¯ Threshold: {best_thresh:.2f}\")\n",
    "print(f\"\\nğŸ“ˆ Metrics:\")\n",
    "print(f\"   Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_optimal):.4f}\")\n",
    "print(f\"   ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "print(f\"   F1 Score: {f1_score(y_test, y_pred_optimal):.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_optimal, target_names=['No Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"ğŸ“Š Top 20 Most Important Features:\")\n",
    "print(importance_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "top_20 = importance_df.head(20)\n",
    "colors = plt.cm.viridis(np.linspace(0.8, 0.2, len(top_20)))\n",
    "\n",
    "ax.barh(range(len(top_20)), top_20['importance'], color=colors)\n",
    "ax.set_yticks(range(len(top_20)))\n",
    "ax.set_yticklabels(top_20['feature'])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title('Top 20 Feature Importances (XGBoost)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_xgb.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Final Model on All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all data for submission\n",
    "print(\"ğŸ¯ Training final model on all data...\")\n",
    "\n",
    "final_model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=RANDOM_STATE,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Store optimal threshold\n",
    "OPTIMAL_THRESHOLD = best_thresh\n",
    "\n",
    "print(f\"âœ… Final model trained on {len(X)} samples\")\n",
    "print(f\"âœ… Optimal threshold: {OPTIMAL_THRESHOLD:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“¤ 9. Kaggle Submission\n",
    "\n",
    "Generate predictions for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(df_test, reference_date, model, feature_cols, threshold=0.5, output_file='submission.csv'):\n",
    "    \"\"\"\n",
    "    Generate Kaggle submission file.\n",
    "    \n",
    "    Args:\n",
    "        df_test: Test event-level DataFrame\n",
    "        reference_date: Reference date for feature calculation\n",
    "        model: Trained model\n",
    "        feature_cols: List of feature columns\n",
    "        threshold: Classification threshold\n",
    "        output_file: Output filename\n",
    "    \n",
    "    Returns:\n",
    "        Submission DataFrame\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ Creating features for test data...\")\n",
    "    \n",
    "    # Create features (reuse the same function)\n",
    "    test_features = create_features(df_test, reference_date)\n",
    "    \n",
    "    # Ensure all required columns exist\n",
    "    for col in feature_cols:\n",
    "        if col not in test_features.columns:\n",
    "            test_features[col] = 0\n",
    "    \n",
    "    # Prepare X\n",
    "    X_test = test_features[feature_cols]\n",
    "    \n",
    "    # Predictions\n",
    "    print(\"ğŸ¯ Generating predictions...\")\n",
    "    probas = model.predict_proba(X_test)[:, 1]\n",
    "    predictions = (probas >= threshold).astype(int)\n",
    "    \n",
    "    # Create submission file\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_features['userId'].astype(int),\n",
    "        'target': predictions\n",
    "    })\n",
    "    \n",
    "    # Save\n",
    "    submission.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… Submission saved: {output_file}\")\n",
    "    print(f\"ğŸ“Š Statistics:\")\n",
    "    print(f\"   Total users: {len(submission):,}\")\n",
    "    print(f\"   Predicted churn: {submission['target'].sum():,} ({submission['target'].mean()*100:.1f}%)\")\n",
    "    print(f\"   Predicted no-churn: {(submission['target']==0).sum():,}\")\n",
    "    \n",
    "    return submission, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ¯ GENERATE KAGGLE SUBMISSION\n",
    "# ============================================================\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_parquet('df_test.parquet')\n",
    "df_test['time'] = pd.to_datetime(df_test['time'])\n",
    "df_test['registration'] = pd.to_datetime(df_test['registration'])\n",
    "\n",
    "print(f\"ğŸ“Š Test data loaded: {len(df_test):,} events\")\n",
    "print(f\"ğŸ‘¥ Unique users: {df_test['userId'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission with optimal threshold\n",
    "submission, test_probas = create_submission(\n",
    "    df_test=df_test,\n",
    "    reference_date=REFERENCE_DATE,\n",
    "    model=final_model,\n",
    "    feature_cols=feature_cols,\n",
    "    threshold=OPTIMAL_THRESHOLD,\n",
    "    output_file='submission.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview submission\n",
    "print(\"\\nğŸ“‹ Submission preview:\")\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Probability distribution\n",
    "ax1 = axes[0]\n",
    "ax1.hist(test_probas, bins=50, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(x=OPTIMAL_THRESHOLD, color='red', linestyle='--', \n",
    "            linewidth=2, label=f'Threshold: {OPTIMAL_THRESHOLD:.2f}')\n",
    "ax1.set_xlabel('Churn Probability', fontsize=12)\n",
    "ax1.set_ylabel('Number of Users', fontsize=12)\n",
    "ax1.set_title('Prediction Probability Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction counts\n",
    "ax2 = axes[1]\n",
    "counts = submission['target'].value_counts().sort_index()\n",
    "bars = ax2.bar(['No Churn (0)', 'Churn (1)'], counts.values, \n",
    "               color=['#2ecc71', '#e74c3c'], edgecolor='black')\n",
    "ax2.set_ylabel('Number of Users', fontsize=12)\n",
    "ax2.set_title('Prediction Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar, count in zip(bars, counts.values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, \n",
    "             f'{count:,}\\n({count/len(submission)*100:.1f}%)', \n",
    "             ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('submission_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                         PIPELINE SUMMARY                             â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ“Š DATA\n",
    "{'â”€'*50}\n",
    "â€¢ Training events: {len(df_train):,}\n",
    "â€¢ Training users: {user_df.shape[0]:,}\n",
    "â€¢ Features: {len(feature_cols)}\n",
    "â€¢ Churn rate: {y.mean()*100:.2f}%\n",
    "\n",
    "ğŸ¯ MODEL\n",
    "{'â”€'*50}\n",
    "â€¢ Algorithm: XGBoost\n",
    "â€¢ Optimal threshold: {OPTIMAL_THRESHOLD:.2f}\n",
    "â€¢ CV Balanced Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\n",
    "â€¢ Test Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_optimal):.4f}\n",
    "â€¢ Test ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\n",
    "\n",
    "âš ï¸ LEAKAGE PREVENTION\n",
    "{'â”€'*50}\n",
    "â€¢ Removed: days_since_last_activity (consequence of churn)\n",
    "â€¢ Removed: cancel page features (cancel = churn)\n",
    "â€¢ Added: Activity pattern features (gaps, consistency)\n",
    "\n",
    "ğŸ“¤ SUBMISSION\n",
    "{'â”€'*50}\n",
    "â€¢ File: submission.csv\n",
    "â€¢ Users: {len(submission):,}\n",
    "â€¢ Predicted churn rate: {submission['target'].mean()*100:.1f}%\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸ” Top 10 Features:\")\n",
    "for i, row in importance_df.head(10).iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
