{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéµ Pr√©diction de Churn - Service de Streaming Musical\n",
    "\n",
    "---\n",
    "\n",
    "## Objectif\n",
    "\n",
    "Pr√©dire si un utilisateur va **churner** (r√©silier son abonnement) dans les **10 jours** suivant le 2018-11-20.\n",
    "\n",
    "Un utilisateur est consid√©r√© comme ayant churn√© s'il visite la page `'Cancellation Confirmation'`.\n",
    "\n",
    "---\n",
    "\n",
    "## Plan du notebook\n",
    "\n",
    "1. **Chargement et exploration des donn√©es**\n",
    "2. **Nettoyage et pr√©paration**\n",
    "3. **Feature Engineering**\n",
    "4. **Analyse exploratoire**\n",
    "5. **Mod√©lisation**\n",
    "6. **√âvaluation et comparaison**\n",
    "7. **Analyse des features importantes**\n",
    "8. **Pr√©diction sur donn√©es de test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports charg√©s avec succ√®s!\n"
     ]
    }
   ],
   "source": [
    "# Manipulation de donn√©es\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    average_precision_score, f1_score, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Couleurs personnalis√©es\n",
    "COLORS = {\n",
    "    'primary': '#3498db',\n",
    "    'success': '#2ecc71',\n",
    "    'danger': '#e74c3c',\n",
    "    'warning': '#f39c12',\n",
    "    'purple': '#9b59b6'\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Imports charg√©s avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dimensions du dataset: 17,499,636 √©v√©nements, 19 colonnes\n",
      "üë• Nombre d'utilisateurs uniques: 19,140\n"
     ]
    }
   ],
   "source": [
    "# Charger les donn√©es\n",
    "df_train_test = pd.read_parquet(\"data/test.parquet\")\n",
    "df_train = pd.read_parquet(\"data/train.parquet\")\n",
    "df = df_train.copy()\n",
    "print(f\"üìä Dimensions du dataset: {df.shape[0]:,} √©v√©nements, {df.shape[1]} colonnes\")\n",
    "print(f\"üë• Nombre d'utilisateurs uniques: {df['userId'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>gender</th>\n",
       "      <th>firstName</th>\n",
       "      <th>level</th>\n",
       "      <th>lastName</th>\n",
       "      <th>userId</th>\n",
       "      <th>ts</th>\n",
       "      <th>auth</th>\n",
       "      <th>page</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>location</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>method</th>\n",
       "      <th>length</th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>time</th>\n",
       "      <th>registration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538352001000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>278</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>524.32934</td>\n",
       "      <td>Ich mache einen Spiegel - Dream Part 4</td>\n",
       "      <td>Popol Vuh</td>\n",
       "      <td>2018-10-01 00:00:01</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538352525000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>279</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>178.02404</td>\n",
       "      <td>Monster (Album Version)</td>\n",
       "      <td>Skillet</td>\n",
       "      <td>2018-10-01 00:08:45</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538352703000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>280</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>232.61995</td>\n",
       "      <td>Seven Nation Army</td>\n",
       "      <td>The White Stripes</td>\n",
       "      <td>2018-10-01 00:11:43</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538352935000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>281</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>265.50812</td>\n",
       "      <td>Under The Bridge (Album Version)</td>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>2018-10-01 00:15:35</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538353200000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>282</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>471.69261</td>\n",
       "      <td>Circlesong 6</td>\n",
       "      <td>Bobby McFerrin</td>\n",
       "      <td>2018-10-01 00:20:00</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538353671000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>283</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>266.86649</td>\n",
       "      <td>Who Can Compare</td>\n",
       "      <td>Foolish Things</td>\n",
       "      <td>2018-10-01 00:27:51</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3802</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538353937000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>284</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>1400.65914</td>\n",
       "      <td>Angel Dust</td>\n",
       "      <td>Gil Scott Heron</td>\n",
       "      <td>2018-10-01 00:32:17</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6585</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538355337000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>285</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>186.98404</td>\n",
       "      <td>Sweet And Dandy</td>\n",
       "      <td>Toots &amp; The Maytals</td>\n",
       "      <td>2018-10-01 00:55:37</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6675</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538355388000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Downgrade</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>286</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>GET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-10-01 00:56:28</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6961</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538355523000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>287</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>306.05016</td>\n",
       "      <td>On The Moon</td>\n",
       "      <td>Peter Cincotti</td>\n",
       "      <td>2018-10-01 00:58:43</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      status gender firstName level lastName   userId             ts  \\\n",
       "0        200      M     Shlok  paid  Johnson  1749042  1538352001000   \n",
       "992      200      M     Shlok  paid  Johnson  1749042  1538352525000   \n",
       "1360     200      M     Shlok  paid  Johnson  1749042  1538352703000   \n",
       "1825     200      M     Shlok  paid  Johnson  1749042  1538352935000   \n",
       "2366     200      M     Shlok  paid  Johnson  1749042  1538353200000   \n",
       "3271     200      M     Shlok  paid  Johnson  1749042  1538353671000   \n",
       "3802     200      M     Shlok  paid  Johnson  1749042  1538353937000   \n",
       "6585     200      M     Shlok  paid  Johnson  1749042  1538355337000   \n",
       "6675     200      M     Shlok  paid  Johnson  1749042  1538355388000   \n",
       "6961     200      M     Shlok  paid  Johnson  1749042  1538355523000   \n",
       "\n",
       "           auth       page  sessionId                         location  \\\n",
       "0     Logged In   NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "992   Logged In   NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "1360  Logged In   NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "1825  Logged In   NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "2366  Logged In   NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "3271  Logged In   NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "3802  Logged In   NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "6585  Logged In   NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "6675  Logged In  Downgrade      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "6961  Logged In   NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "\n",
       "      itemInSession                                          userAgent method  \\\n",
       "0               278  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "992             279  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "1360            280  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "1825            281  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "2366            282  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "3271            283  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "3802            284  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "6585            285  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "6675            286  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    GET   \n",
       "6961            287  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "\n",
       "          length                                    song  \\\n",
       "0      524.32934  Ich mache einen Spiegel - Dream Part 4   \n",
       "992    178.02404                 Monster (Album Version)   \n",
       "1360   232.61995                       Seven Nation Army   \n",
       "1825   265.50812        Under The Bridge (Album Version)   \n",
       "2366   471.69261                            Circlesong 6   \n",
       "3271   266.86649                         Who Can Compare   \n",
       "3802  1400.65914                              Angel Dust   \n",
       "6585   186.98404                         Sweet And Dandy   \n",
       "6675         NaN                                    None   \n",
       "6961   306.05016                             On The Moon   \n",
       "\n",
       "                     artist                time        registration  \n",
       "0                 Popol Vuh 2018-10-01 00:00:01 2018-08-08 13:22:21  \n",
       "992                 Skillet 2018-10-01 00:08:45 2018-08-08 13:22:21  \n",
       "1360      The White Stripes 2018-10-01 00:11:43 2018-08-08 13:22:21  \n",
       "1825  Red Hot Chili Peppers 2018-10-01 00:15:35 2018-08-08 13:22:21  \n",
       "2366         Bobby McFerrin 2018-10-01 00:20:00 2018-08-08 13:22:21  \n",
       "3271         Foolish Things 2018-10-01 00:27:51 2018-08-08 13:22:21  \n",
       "3802        Gil Scott Heron 2018-10-01 00:32:17 2018-08-08 13:22:21  \n",
       "6585    Toots & The Maytals 2018-10-01 00:55:37 2018-08-08 13:22:21  \n",
       "6675                   None 2018-10-01 00:56:28 2018-08-08 13:22:21  \n",
       "6961         Peter Cincotti 2018-10-01 00:58:43 2018-08-08 13:22:21  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aper√ßu des premi√®res lignes\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17499636 entries, 0 to 25661583\n",
      "Data columns (total 19 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   status         int64         \n",
      " 1   gender         object        \n",
      " 2   firstName      object        \n",
      " 3   level          object        \n",
      " 4   lastName       object        \n",
      " 5   userId         object        \n",
      " 6   ts             int64         \n",
      " 7   auth           object        \n",
      " 8   page           object        \n",
      " 9   sessionId      int64         \n",
      " 10  location       object        \n",
      " 11  itemInSession  int64         \n",
      " 12  userAgent      object        \n",
      " 13  method         object        \n",
      " 14  length         float64       \n",
      " 15  song           object        \n",
      " 16  artist         object        \n",
      " 17  time           datetime64[us]\n",
      " 18  registration   datetime64[us]\n",
      "dtypes: datetime64[us](2), float64(1), int64(4), object(12)\n",
      "memory usage: 2.6+ GB\n"
     ]
    }
   ],
   "source": [
    "# Informations sur les colonnes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>length</th>\n",
       "      <th>time</th>\n",
       "      <th>registration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.749964e+07</td>\n",
       "      <td>1.749964e+07</td>\n",
       "      <td>1.749964e+07</td>\n",
       "      <td>1.749964e+07</td>\n",
       "      <td>1.429143e+07</td>\n",
       "      <td>17499636</td>\n",
       "      <td>17499636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.091387e+02</td>\n",
       "      <td>1.540428e+12</td>\n",
       "      <td>8.480294e+04</td>\n",
       "      <td>1.055937e+02</td>\n",
       "      <td>2.487135e+02</td>\n",
       "      <td>2018-10-25 00:47:01.161927</td>\n",
       "      <td>2018-08-25 04:40:21.543066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000e+02</td>\n",
       "      <td>1.538352e+12</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.220000e-01</td>\n",
       "      <td>2018-10-01 00:00:01</td>\n",
       "      <td>2017-10-14 22:05:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000e+02</td>\n",
       "      <td>1.539340e+12</td>\n",
       "      <td>2.515900e+04</td>\n",
       "      <td>2.600000e+01</td>\n",
       "      <td>1.998885e+02</td>\n",
       "      <td>2018-10-12 10:33:57.750000</td>\n",
       "      <td>2018-08-10 21:14:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000e+02</td>\n",
       "      <td>1.540397e+12</td>\n",
       "      <td>7.903800e+04</td>\n",
       "      <td>6.600000e+01</td>\n",
       "      <td>2.340828e+02</td>\n",
       "      <td>2018-10-24 15:58:54</td>\n",
       "      <td>2018-09-05 18:35:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000e+02</td>\n",
       "      <td>1.541500e+12</td>\n",
       "      <td>1.383680e+05</td>\n",
       "      <td>1.440000e+02</td>\n",
       "      <td>2.768714e+02</td>\n",
       "      <td>2018-11-06 10:25:35</td>\n",
       "      <td>2018-09-20 17:24:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.040000e+02</td>\n",
       "      <td>1.542672e+12</td>\n",
       "      <td>2.070030e+05</td>\n",
       "      <td>1.426000e+03</td>\n",
       "      <td>3.024666e+03</td>\n",
       "      <td>2018-11-20 00:00:00</td>\n",
       "      <td>2018-11-19 23:34:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.023050e+01</td>\n",
       "      <td>1.233485e+09</td>\n",
       "      <td>6.141427e+04</td>\n",
       "      <td>1.168854e+02</td>\n",
       "      <td>9.722845e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             status            ts     sessionId  itemInSession        length  \\\n",
       "count  1.749964e+07  1.749964e+07  1.749964e+07   1.749964e+07  1.429143e+07   \n",
       "mean   2.091387e+02  1.540428e+12  8.480294e+04   1.055937e+02  2.487135e+02   \n",
       "min    2.000000e+02  1.538352e+12  1.000000e+00   0.000000e+00  5.220000e-01   \n",
       "25%    2.000000e+02  1.539340e+12  2.515900e+04   2.600000e+01  1.998885e+02   \n",
       "50%    2.000000e+02  1.540397e+12  7.903800e+04   6.600000e+01  2.340828e+02   \n",
       "75%    2.000000e+02  1.541500e+12  1.383680e+05   1.440000e+02  2.768714e+02   \n",
       "max    4.040000e+02  1.542672e+12  2.070030e+05   1.426000e+03  3.024666e+03   \n",
       "std    3.023050e+01  1.233485e+09  6.141427e+04   1.168854e+02  9.722845e+01   \n",
       "\n",
       "                             time                registration  \n",
       "count                    17499636                    17499636  \n",
       "mean   2018-10-25 00:47:01.161927  2018-08-25 04:40:21.543066  \n",
       "min           2018-10-01 00:00:01         2017-10-14 22:05:25  \n",
       "25%    2018-10-12 10:33:57.750000         2018-08-10 21:14:59  \n",
       "50%           2018-10-24 15:58:54         2018-09-05 18:35:50  \n",
       "75%           2018-11-06 10:25:35         2018-09-20 17:24:57  \n",
       "max           2018-11-20 00:00:00         2018-11-19 23:34:34  \n",
       "std                           NaN                         NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistiques descriptives\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Nettoyage et Pr√©paration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ P√©riode des donn√©es: du 2018-10-01 00:00:01 au 2018-11-20 00:00:00\n",
      "üìÖ Date de r√©f√©rence: 2018-11-20 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Conversion des timestamps\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['registration'] = pd.to_datetime(df['registration'])\n",
    "\n",
    "# Date de r√©f√©rence (fin de la p√©riode d'observation)\n",
    "REFERENCE_DATE = pd.to_datetime('2018-11-20')\n",
    "\n",
    "print(f\"üìÖ P√©riode des donn√©es: du {df['time'].min()} au {df['time'].max()}\")\n",
    "print(f\"üìÖ Date de r√©f√©rence: {REFERENCE_DATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Manquants</th>\n",
       "      <th>Pourcentage (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>3208203</td>\n",
       "      <td>18.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>song</th>\n",
       "      <td>3208203</td>\n",
       "      <td>18.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artist</th>\n",
       "      <td>3208203</td>\n",
       "      <td>18.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Manquants  Pourcentage (%)\n",
       "length    3208203            18.33\n",
       "song      3208203            18.33\n",
       "artist    3208203            18.33"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V√©rification des valeurs manquantes\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Manquants': missing, 'Pourcentage (%)': missing_pct})\n",
    "missing_df[missing_df['Manquants'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating cancellation in following ten days column\n",
    "import numpy as np\n",
    "\n",
    "cancellation_events = df_train[df_train['page'] == 'Cancellation Confirmation'].copy()\n",
    "cancellation_events = cancellation_events[['userId', 'time']].rename(columns={'time': 'churn_time'})\n",
    "\n",
    "df_train = df_train.merge(cancellation_events, on='userId', how='left')\n",
    "\n",
    "df_train['days_until_churn'] = (df_train['churn_time'] - df_train['time']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "df_train['will_churn_10days'] = ((df_train['days_until_churn'] >= 0) & \n",
    "                                   (df_train['days_until_churn'] <= 10)).astype(int)\n",
    "\n",
    "df = df_train.drop(['churn_time', 'days_until_churn'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Distribution de la variable cible (par utilisateur):\n",
      "   Non-Churn (0): 17,738 (92.68%)\n",
      "   Churn (1):     1,402 (7.32%)\n"
     ]
    }
   ],
   "source": [
    "# Distribution de la variable cible\n",
    "churn_by_user = df.groupby('userId')['will_churn_10days'].first()\n",
    "churn_counts = churn_by_user.value_counts()\n",
    "\n",
    "print(\"üéØ Distribution de la variable cible (par utilisateur):\")\n",
    "print(f\"   Non-Churn (0): {churn_counts[0]:,} ({churn_counts[0]/len(churn_by_user)*100:.2f}%)\")\n",
    "print(f\"   Churn (1):     {churn_counts[1]:,} ({churn_counts[1]/len(churn_by_user)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types de pages visit√©es\n",
    "print(\"üìÑ Types de pages visit√©es:\")\n",
    "df['page'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Nous allons cr√©er des **features agr√©g√©es au niveau utilisateur** √† partir des √©v√©nements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_features(df, reference_date):\n",
    "    \"\"\"\n",
    "    Cr√©e des features agr√©g√©es au niveau utilisateur.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        DataFrame contenant les √©v√©nements utilisateur\n",
    "    reference_date : datetime\n",
    "        Date de r√©f√©rence pour le calcul des features temporelles\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame avec une ligne par utilisateur et les features calcul√©es\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtrer les √©v√©nements avant la date de r√©f√©rence\n",
    "    df_filtered = df[df['time'] <= reference_date].copy()\n",
    "    \n",
    "    # DataFrame pour stocker les features\n",
    "    user_features = pd.DataFrame()\n",
    "    user_features['userId'] = df_filtered['userId'].unique()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FEATURES D√âMOGRAPHIQUES\n",
    "    # =========================================================================\n",
    "    user_info = df_filtered.groupby('userId').agg({\n",
    "        'gender': 'first',\n",
    "        'level': 'last',  # Dernier niveau connu\n",
    "        'registration': 'first',\n",
    "        'will_churn_10days': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    user_features = user_features.merge(user_info, on='userId', how='left')\n",
    "    \n",
    "    # Anciennet√© (jours depuis inscription)\n",
    "    user_features['days_since_registration'] = (\n",
    "        reference_date - user_features['registration']\n",
    "    ).dt.days\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FEATURES D'ACTIVIT√â GLOBALE\n",
    "    # =========================================================================\n",
    "    activity = df_filtered.groupby('userId').agg({\n",
    "        'ts': 'count',  # Nombre total d'√©v√©nements\n",
    "        'sessionId': 'nunique',  # Nombre de sessions uniques\n",
    "        'time': ['min', 'max'],  # Premi√®re et derni√®re activit√©\n",
    "        'length': ['sum', 'mean', 'count'],  # Dur√©e d'√©coute\n",
    "    })\n",
    "    activity.columns = ['_'.join(col).strip() for col in activity.columns]\n",
    "    activity = activity.reset_index()\n",
    "    activity.columns = ['userId', 'total_events', 'unique_sessions', \n",
    "                        'first_activity', 'last_activity',\n",
    "                        'total_listening_time', 'avg_song_length', 'songs_with_length']\n",
    "    \n",
    "    user_features = user_features.merge(activity, on='userId', how='left')\n",
    "    \n",
    "    # R√©cence (jours depuis la derni√®re activit√©)\n",
    "    user_features['days_since_last_activity'] = (\n",
    "        reference_date - pd.to_datetime(user_features['last_activity'])\n",
    "    ).dt.days\n",
    "    \n",
    "    # Dur√©e d'activit√© (jours entre premi√®re et derni√®re activit√©)\n",
    "    user_features['activity_span_days'] = (\n",
    "        pd.to_datetime(user_features['last_activity']) - \n",
    "        pd.to_datetime(user_features['first_activity'])\n",
    "    ).dt.days + 1\n",
    "    \n",
    "    # Fr√©quence d'utilisation\n",
    "    user_features['events_per_day'] = (\n",
    "        user_features['total_events'] / user_features['activity_span_days']\n",
    "    ).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    user_features['sessions_per_day'] = (\n",
    "        user_features['unique_sessions'] / user_features['activity_span_days']\n",
    "    ).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FEATURES PAR TYPE DE PAGE\n",
    "    # =========================================================================\n",
    "    important_pages = [\n",
    "        'NextSong', 'Home', 'Thumbs Up', 'Thumbs Down', \n",
    "        'Add to Playlist', 'Add Friend', 'Roll Advert',\n",
    "        'Downgrade', 'Cancel', 'Submit Downgrade', 'Error',\n",
    "        'Help', 'Settings', 'Logout', 'Upgrade', 'Submit Upgrade'\n",
    "    ]\n",
    "    \n",
    "    page_counts = df_filtered.groupby(['userId', 'page']).size().unstack(fill_value=0)\n",
    "    \n",
    "    for page in important_pages:\n",
    "        col_name = f'page_{page.lower().replace(\" \", \"_\")}'\n",
    "        if page in page_counts.columns:\n",
    "            page_counts_temp = page_counts[[page]].reset_index()\n",
    "            page_counts_temp.columns = ['userId', col_name]\n",
    "            user_features = user_features.merge(page_counts_temp, on='userId', how='left')\n",
    "            user_features[col_name] = user_features[col_name].fillna(0)\n",
    "        else:\n",
    "            user_features[col_name] = 0\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FEATURES DE SIGNAUX DE CHURN\n",
    "    # =========================================================================\n",
    "    # Visites sur pages de r√©siliation/downgrade\n",
    "    user_features['churn_signals'] = (\n",
    "        user_features.get('page_downgrade', 0) + \n",
    "        user_features.get('page_cancel', 0) + \n",
    "        user_features.get('page_submit_downgrade', 0)\n",
    "    )\n",
    "    \n",
    "    # Ratio Thumbs Down / Thumbs Up\n",
    "    user_features['thumbs_ratio'] = (\n",
    "        user_features.get('page_thumbs_down', 0) / \n",
    "        (user_features.get('page_thumbs_up', 0) + 1)\n",
    "    )\n",
    "    \n",
    "    # Taux d'erreurs\n",
    "    user_features['error_rate'] = (\n",
    "        user_features.get('page_error', 0) / user_features['total_events']\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FEATURES D'ENGAGEMENT\n",
    "    # =========================================================================\n",
    "    # Ratio de chansons √©cout√©es sur total d'√©v√©nements\n",
    "    user_features['song_event_ratio'] = (\n",
    "        user_features.get('page_nextsong', 0) / user_features['total_events']\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Interactions positives (Thumbs Up + Add to Playlist + Add Friend)\n",
    "    user_features['positive_interactions'] = (\n",
    "        user_features.get('page_thumbs_up', 0) + \n",
    "        user_features.get('page_add_to_playlist', 0) + \n",
    "        user_features.get('page_add_friend', 0)\n",
    "    )\n",
    "    \n",
    "    # Taux d'interactions positives\n",
    "    user_features['positive_interaction_rate'] = (\n",
    "        user_features['positive_interactions'] / user_features['total_events']\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FEATURES TEMPORELLES\n",
    "    # =========================================================================\n",
    "    # Derni√®re semaine vs reste de la p√©riode\n",
    "    one_week_before = reference_date - pd.Timedelta(days=7)\n",
    "    \n",
    "    last_week = df_filtered[df_filtered['time'] >= one_week_before].groupby('userId').size()\n",
    "    last_week = last_week.reset_index()\n",
    "    last_week.columns = ['userId', 'events_last_week']\n",
    "    \n",
    "    user_features = user_features.merge(last_week, on='userId', how='left')\n",
    "    user_features['events_last_week'] = user_features['events_last_week'].fillna(0)\n",
    "    \n",
    "    # Tendance d'activit√© (activit√© r√©cente vs ancienne)\n",
    "    user_features['activity_trend'] = (\n",
    "        user_features['events_last_week'] / (user_features['total_events'] + 1)\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FEATURES PAR NIVEAU (paid/free)\n",
    "    # =========================================================================\n",
    "    # Historique des changements de niveau\n",
    "    level_changes = df_filtered.groupby('userId')['level'].nunique().reset_index()\n",
    "    level_changes.columns = ['userId', 'level_changes']\n",
    "    user_features = user_features.merge(level_changes, on='userId', how='left')\n",
    "    user_features['has_changed_level'] = (user_features['level_changes'] > 1).astype(int)\n",
    "    \n",
    "    return user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation des features\n",
    "print(\"‚öôÔ∏è Cr√©ation des features utilisateur...\")\n",
    "user_df = create_user_features(df, REFERENCE_DATE)\n",
    "\n",
    "print(f\"‚úÖ Dataset agr√©g√©: {user_df.shape[0]:,} utilisateurs, {user_df.shape[1]} colonnes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aper√ßu du dataset agr√©g√©\n",
    "user_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des features cr√©√©es\n",
    "feature_cols = [col for col in user_df.columns \n",
    "                if col not in ['userId', 'will_churn_10days', 'registration', \n",
    "                               'first_activity', 'last_activity', 'gender', 'level']]\n",
    "\n",
    "print(f\"üìä {len(feature_cols)} features cr√©√©es:\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse Exploratoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution du churn\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Pie chart du churn\n",
    "ax1 = axes[0]\n",
    "churn_counts = user_df['will_churn_10days'].value_counts()\n",
    "colors = [COLORS['success'], COLORS['danger']]\n",
    "ax1.pie(churn_counts, labels=['Non-Churn', 'Churn'], autopct='%1.1f%%', \n",
    "        colors=colors, startangle=90, explode=[0, 0.05])\n",
    "ax1.set_title('Distribution du Churn', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Churn par niveau (paid/free)\n",
    "ax2 = axes[1]\n",
    "churn_by_level = user_df.groupby(['level', 'will_churn_10days']).size().unstack(fill_value=0)\n",
    "churn_by_level_pct = churn_by_level.div(churn_by_level.sum(axis=1), axis=0) * 100\n",
    "churn_by_level_pct.plot(kind='bar', ax=ax2, color=colors, edgecolor='black')\n",
    "ax2.set_title('Taux de Churn par Niveau', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Niveau', fontsize=12)\n",
    "ax2.set_ylabel('Pourcentage (%)', fontsize=12)\n",
    "ax2.legend(['Non-Churn', 'Churn'], loc='upper right')\n",
    "ax2.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 3. Churn par genre\n",
    "ax3 = axes[2]\n",
    "churn_by_gender = user_df.groupby(['gender', 'will_churn_10days']).size().unstack(fill_value=0)\n",
    "churn_by_gender_pct = churn_by_gender.div(churn_by_gender.sum(axis=1), axis=0) * 100\n",
    "churn_by_gender_pct.plot(kind='bar', ax=ax3, color=colors, edgecolor='black')\n",
    "ax3.set_title('Taux de Churn par Genre', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Genre', fontsize=12)\n",
    "ax3.set_ylabel('Pourcentage (%)', fontsize=12)\n",
    "ax3.legend(['Non-Churn', 'Churn'], loc='upper right')\n",
    "ax3.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des distributions par statut de churn\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "features_to_plot = [\n",
    "    ('total_events', '√âv√©nements Totaux'),\n",
    "    ('days_since_last_activity', 'Jours depuis derni√®re activit√©'),\n",
    "    ('unique_sessions', 'Sessions Uniques'),\n",
    "    ('positive_interactions', 'Interactions Positives'),\n",
    "    ('events_per_day', '√âv√©nements par Jour'),\n",
    "    ('churn_signals', 'Signaux de Churn')\n",
    "]\n",
    "\n",
    "for idx, (feature, title) in enumerate(features_to_plot):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Boxplot\n",
    "    data_no_churn = user_df[user_df['will_churn_10days'] == 0][feature]\n",
    "    data_churn = user_df[user_df['will_churn_10days'] == 1][feature]\n",
    "    \n",
    "    bp = ax.boxplot([data_no_churn, data_churn], \n",
    "                    labels=['Non-Churn', 'Churn'],\n",
    "                    patch_artist=True)\n",
    "    \n",
    "    bp['boxes'][0].set_facecolor(COLORS['success'])\n",
    "    bp['boxes'][1].set_facecolor(COLORS['danger'])\n",
    "    \n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques comparatives\n",
    "print(\"üìà Statistiques comparatives Churn vs Non-Churn:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_cols = ['total_events', 'unique_sessions', 'days_since_last_activity', \n",
    "                   'events_per_day', 'page_nextsong', 'positive_interactions',\n",
    "                   'churn_signals', 'thumbs_ratio', 'activity_trend']\n",
    "\n",
    "comparison = user_df.groupby('will_churn_10days')[comparison_cols].mean().T\n",
    "comparison.columns = ['Non-Churn', 'Churn']\n",
    "comparison['Diff (%)'] = ((comparison['Churn'] - comparison['Non-Churn']) / comparison['Non-Churn'] * 100).round(1)\n",
    "comparison = comparison.round(2)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corr√©lation\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "# S√©lectionner les features num√©riques\n",
    "numeric_cols = user_df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols.append('will_churn_10days')\n",
    "\n",
    "corr_matrix = user_df[numeric_cols].corr()\n",
    "\n",
    "# Heatmap\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='RdYlBu_r', \n",
    "            center=0, ax=ax, linewidths=0.5)\n",
    "ax.set_title('Matrice de Corr√©lation des Features', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corr√©lations avec la cible\n",
    "target_corr = user_df[numeric_cols].corr()['will_churn_10days'].drop('will_churn_10days').sort_values(key=abs, ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = [COLORS['danger'] if x > 0 else COLORS['primary'] for x in target_corr.values]\n",
    "target_corr.plot(kind='barh', ax=ax, color=colors)\n",
    "ax.set_xlabel('Corr√©lation avec le Churn', fontsize=12)\n",
    "ax.set_title('Corr√©lation des Features avec la Variable Cible', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pr√©paration pour la Mod√©lisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des variables cat√©gorielles\n",
    "user_df['gender_encoded'] = LabelEncoder().fit_transform(user_df['gender'].fillna('Unknown'))\n",
    "user_df['level_encoded'] = LabelEncoder().fit_transform(user_df['level'].fillna('Unknown'))\n",
    "\n",
    "# Liste finale des features\n",
    "final_features = feature_cols + ['gender_encoded', 'level_encoded']\n",
    "\n",
    "print(f\"üìä Nombre total de features: {len(final_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©paration X et y\n",
    "X = user_df[final_features].fillna(0)\n",
    "y = user_df['will_churn_10days']\n",
    "\n",
    "print(f\"üìä Dimensions:\")\n",
    "print(f\"   X: {X.shape}\")\n",
    "print(f\"   y: {y.shape}\")\n",
    "print(f\"   Taux de churn: {y.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìÇ Division des donn√©es:\")\n",
    "print(f\"   Train: {X_train.shape[0]:,} utilisateurs ({y_train.mean()*100:.2f}% churn)\")\n",
    "print(f\"   Test:  {X_test.shape[0]:,} utilisateurs ({y_test.mean()*100:.2f}% churn)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Normalisation effectu√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Mod√©lisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition des mod√®les\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        class_weight='balanced', \n",
    "        max_iter=1000, \n",
    "        random_state=42\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"üì¶ {len(models)} mod√®les √† entra√Æner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement et √©valuation\n",
    "results = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìå {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Utiliser les donn√©es scal√©es pour la r√©gression logistique\n",
    "    if name == 'Logistic Regression':\n",
    "        X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "    else:\n",
    "        X_tr, X_te = X_train, X_test\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_tr, y_train, cv=cv, scoring='roc_auc')\n",
    "    print(f\"   CV ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "    \n",
    "    # Entra√Ænement final\n",
    "    model.fit(X_tr, y_train)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_pred = model.predict(X_te)\n",
    "    y_pred_proba = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    # M√©triques\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    ap = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"   Test ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"   Test F1-Score: {f1:.4f}\")\n",
    "    print(f\"   Test Average Precision: {ap:.4f}\")\n",
    "    \n",
    "    # Stockage des r√©sultats\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'cv_auc': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'test_auc': roc_auc,\n",
    "        'test_f1': f1,\n",
    "        'test_ap': ap,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport de classification d√©taill√© pour chaque mod√®le\n",
    "for name, res in results.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä Classification Report - {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(classification_report(y_test, res['y_pred'], target_names=['Non-Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparaison des Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau r√©capitulatif\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'CV ROC-AUC': [r['cv_auc'] for r in results.values()],\n",
    "    'CV Std': [r['cv_std'] for r in results.values()],\n",
    "    'Test ROC-AUC': [r['test_auc'] for r in results.values()],\n",
    "    'Test F1': [r['test_f1'] for r in results.values()],\n",
    "    'Test AP': [r['test_ap'] for r in results.values()]\n",
    "}).sort_values('Test ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"üìä R√©capitulatif des performances:\")\n",
    "results_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meilleur mod√®le\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"üèÜ Meilleur mod√®le: {best_model_name}\")\n",
    "print(f\"   ROC-AUC: {results[best_model_name]['test_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes ROC et Precision-Recall\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curves\n",
    "ax1 = axes[0]\n",
    "colors_list = [COLORS['primary'], COLORS['success'], COLORS['purple']]\n",
    "\n",
    "for (name, res), color in zip(results.items(), colors_list):\n",
    "    fpr, tpr, _ = roc_curve(y_test, res['y_pred_proba'])\n",
    "    ax1.plot(fpr, tpr, label=f\"{name} (AUC={res['test_auc']:.3f})\", \n",
    "             linewidth=2, color=color)\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "ax1.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax1.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax1.set_title('Courbes ROC', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curves\n",
    "ax2 = axes[1]\n",
    "for (name, res), color in zip(results.items(), colors_list):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, res['y_pred_proba'])\n",
    "    ax2.plot(recall, precision, label=f\"{name} (AP={res['test_ap']:.3f})\", \n",
    "             linewidth=2, color=color)\n",
    "\n",
    "ax2.set_xlabel('Recall', fontsize=12)\n",
    "ax2.set_ylabel('Precision', fontsize=12)\n",
    "ax2.set_title('Courbes Precision-Recall', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices de confusion\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (name, res) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    cm = confusion_matrix(y_test, res['y_pred'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Non-Churn', 'Churn'],\n",
    "                yticklabels=['Non-Churn', 'Churn'])\n",
    "    ax.set_xlabel('Pr√©dit')\n",
    "    ax.set_ylabel('R√©el')\n",
    "    ax.set_title(f'{name}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Importance des Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance du Random Forest\n",
    "rf_model = results['Random Forest']['model']\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': final_features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"üîç Top 15 features les plus importantes (Random Forest):\")\n",
    "feature_importance.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'importance des features\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "top_features = feature_importance.head(15)\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(top_features)))\n",
    "\n",
    "bars = ax.barh(range(len(top_features)), top_features['importance'], color=colors)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title('Top 15 Features les Plus Importantes (Random Forest)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for i, (idx, row) in enumerate(top_features.iterrows()):\n",
    "    ax.text(row['importance'] + 0.005, i, f\"{row['importance']:.3f}\", \n",
    "            va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients de la r√©gression logistique\n",
    "lr_model = results['Logistic Regression']['model']\n",
    "\n",
    "lr_coef = pd.DataFrame({\n",
    "    'feature': final_features,\n",
    "    'coefficient': lr_model.coef_[0]\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"üîç Top 15 coefficients (Logistic Regression):\")\n",
    "lr_coef.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sauvegarde du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Sauvegarde du meilleur mod√®le\n",
    "model_data = {\n",
    "    'model': best_model,\n",
    "    'scaler': scaler,\n",
    "    'feature_cols': final_features,\n",
    "    'model_name': best_model_name\n",
    "}\n",
    "\n",
    "with open('best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le '{best_model_name}' sauvegard√© dans 'best_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Pr√©diction sur Donn√©es de Test\n",
    "\n",
    "Voici le code pour appliquer le mod√®le sur de nouvelles donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_churn(df_test, reference_date=REFERENCE_DATE):\n",
    "    \"\"\"\n",
    "    Applique le mod√®le de churn sur de nouvelles donn√©es.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_test : DataFrame\n",
    "        Donn√©es de test au format √©v√©nement\n",
    "    reference_date : datetime\n",
    "        Date de r√©f√©rence\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame avec userId et probabilit√© de churn\n",
    "    \"\"\"\n",
    "    # Charger le mod√®le\n",
    "    with open('best_model.pkl', 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "    \n",
    "    model = saved['model']\n",
    "    scaler = saved['scaler']\n",
    "    feature_cols = saved['feature_cols']\n",
    "    model_name = saved['model_name']\n",
    "    \n",
    "    print(f\"üì¶ Mod√®le charg√©: {model_name}\")\n",
    "    \n",
    "    # Pr√©parer les timestamps\n",
    "    df_test = df_test.copy()\n",
    "    df_test['time'] = pd.to_datetime(df_test['time'])\n",
    "    df_test['registration'] = pd.to_datetime(df_test['registration'])\n",
    "    \n",
    "    # Cr√©er les features\n",
    "    test_features = create_user_features(df_test, reference_date)\n",
    "    \n",
    "    # Encodage\n",
    "    test_features['gender_encoded'] = LabelEncoder().fit_transform(\n",
    "        test_features['gender'].fillna('Unknown')\n",
    "    )\n",
    "    test_features['level_encoded'] = LabelEncoder().fit_transform(\n",
    "        test_features['level'].fillna('Unknown')\n",
    "    )\n",
    "    \n",
    "    # S'assurer que toutes les colonnes existent\n",
    "    for col in feature_cols:\n",
    "        if col not in test_features.columns:\n",
    "            test_features[col] = 0\n",
    "    \n",
    "    # Pr√©parer X\n",
    "    X_new = test_features[feature_cols].fillna(0)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    if model_name == 'Logistic Regression':\n",
    "        X_new_scaled = scaler.transform(X_new)\n",
    "        predictions = model.predict_proba(X_new_scaled)[:, 1]\n",
    "    else:\n",
    "        predictions = model.predict_proba(X_new)[:, 1]\n",
    "    \n",
    "    # R√©sultats\n",
    "    submission = pd.DataFrame({\n",
    "        'userId': test_features['userId'],\n",
    "        'churn_probability': predictions\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ Pr√©dictions g√©n√©r√©es pour {len(submission):,} utilisateurs\")\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation (d√©commenter pour utiliser)\n",
    "# df_test = pd.read_csv('df_test.csv')\n",
    "# submission = predict_churn(df_test)\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. R√©sum√© et Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                    R√âSUM√â DU PROJET DE CHURN                         ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä DONN√âES\n",
    "{'‚îÄ'*50}\n",
    "‚Ä¢ √âv√©nements totaux: {df.shape[0]:,}\n",
    "‚Ä¢ Utilisateurs uniques: {user_df.shape[0]:,}\n",
    "‚Ä¢ Features cr√©√©es: {len(final_features)}\n",
    "‚Ä¢ Taux de churn: {y.mean()*100:.2f}%\n",
    "\n",
    "üèÜ MEILLEUR MOD√àLE: {best_model_name}\n",
    "{'‚îÄ'*50}\n",
    "‚Ä¢ ROC-AUC: {results[best_model_name]['test_auc']:.4f}\n",
    "‚Ä¢ F1-Score: {results[best_model_name]['test_f1']:.4f}\n",
    "‚Ä¢ Average Precision: {results[best_model_name]['test_ap']:.4f}\n",
    "\n",
    "üîë TOP 5 FEATURES PR√âDICTIVES\n",
    "{'‚îÄ'*50}\"\"\")\n",
    "\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  ‚Ä¢ {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "üí° INSIGHTS CL√âS\n",
    "{'‚îÄ'*50}\n",
    "1. La r√©cence d'activit√© est le facteur #1 de pr√©diction du churn\n",
    "2. Les utilisateurs inactifs depuis longtemps ont ~50% plus de risque\n",
    "3. L'engagement positif (likes, playlists) prot√®ge contre le churn\n",
    "4. Les visites sur Downgrade/Cancel sont des signaux d'alerte\n",
    "\n",
    "üöÄ PISTES D'AM√âLIORATION\n",
    "{'‚îÄ'*50}\n",
    "1. Ajouter des features de tendance temporelle (7j, 14j, 30j)\n",
    "2. Features musicales: diversit√© genres/artistes\n",
    "3. Hyperparameter tuning avec Optuna/GridSearch\n",
    "4. Essayer XGBoost/LightGBM avec SMOTE\n",
    "5. Stacking/Ensemble de mod√®les\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
