{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Pr√©diction de Churn Am√©lior√©e + Soumission Kaggle\n",
    "\n",
    "Ce notebook utilise les **insights de la segmentation des churners** pour am√©liorer les performances de pr√©diction.\n",
    "\n",
    "## Strat√©gies d'am√©lioration impl√©ment√©es\n",
    "\n",
    "1. **Features de proximit√© aux profils** - Distance aux centro√Ødes des churners\n",
    "2. **Scores composites** - Frustration, d√©sengagement, risque\n",
    "3. **Features temporelles avanc√©es** - Tendances sur plusieurs p√©riodes\n",
    "4. **Comparaison avant/apr√®s** - Mesure de l'am√©lioration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report, roc_curve, precision_recall_curve\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Constantes\n",
    "REFERENCE_DATE = pd.to_datetime('2018-11-20')\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"‚úÖ Imports charg√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es d'entra√Ænement\n",
    "df_train = pd.read_csv('df_train_sample.csv')\n",
    "df_train['time'] = pd.to_datetime(df_train['time'])\n",
    "df_train['registration'] = pd.to_datetime(df_train['registration'])\n",
    "\n",
    "print(f\"üìä Donn√©es d'entra√Ænement: {len(df_train):,} √©v√©nements\")\n",
    "print(f\"üë• Utilisateurs uniques: {df_train['userId'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering - Version de Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_features(df, reference_date):\n",
    "    \"\"\"\n",
    "    Cr√©e les features de base (version initiale sans am√©liorations).\n",
    "    \"\"\"\n",
    "    df_filtered = df[df['time'] <= reference_date].copy()\n",
    "    \n",
    "    user_features = pd.DataFrame()\n",
    "    user_features['userId'] = df_filtered['userId'].unique()\n",
    "    \n",
    "    # Info utilisateur\n",
    "    user_info = df_filtered.groupby('userId').agg({\n",
    "        'gender': 'first',\n",
    "        'level': 'last',\n",
    "        'registration': 'first',\n",
    "        'will_churn_10days': 'first'\n",
    "    }).reset_index()\n",
    "    user_features = user_features.merge(user_info, on='userId', how='left')\n",
    "    user_features['days_since_registration'] = (reference_date - user_features['registration']).dt.days\n",
    "    \n",
    "    # Activit√© globale\n",
    "    activity = df_filtered.groupby('userId').agg({\n",
    "        'ts': 'count',\n",
    "        'sessionId': 'nunique',\n",
    "        'time': ['min', 'max'],\n",
    "        'length': ['sum', 'mean', 'count'],\n",
    "    })\n",
    "    activity.columns = ['total_events', 'unique_sessions', 'first_activity', 'last_activity',\n",
    "                        'total_listening_time', 'avg_song_length', 'songs_with_length']\n",
    "    activity = activity.reset_index()\n",
    "    user_features = user_features.merge(activity, on='userId', how='left')\n",
    "    \n",
    "    # M√©triques temporelles\n",
    "    user_features['days_since_last_activity'] = (reference_date - pd.to_datetime(user_features['last_activity'])).dt.days\n",
    "    user_features['activity_span_days'] = (pd.to_datetime(user_features['last_activity']) - pd.to_datetime(user_features['first_activity'])).dt.days + 1\n",
    "    user_features['events_per_day'] = (user_features['total_events'] / user_features['activity_span_days']).replace([np.inf, -np.inf], 0)\n",
    "    user_features['sessions_per_day'] = (user_features['unique_sessions'] / user_features['activity_span_days']).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Pages visit√©es\n",
    "    important_pages = ['NextSong', 'Home', 'Thumbs Up', 'Thumbs Down', 'Add to Playlist', \n",
    "                       'Add Friend', 'Roll Advert', 'Downgrade', 'Cancel', 'Submit Downgrade', \n",
    "                       'Error', 'Help', 'Settings', 'Logout', 'Upgrade']\n",
    "    \n",
    "    page_counts = df_filtered.groupby(['userId', 'page']).size().unstack(fill_value=0)\n",
    "    for page in important_pages:\n",
    "        col_name = f'page_{page.lower().replace(\" \", \"_\")}'\n",
    "        if page in page_counts.columns:\n",
    "            temp = page_counts[[page]].reset_index()\n",
    "            temp.columns = ['userId', col_name]\n",
    "            user_features = user_features.merge(temp, on='userId', how='left')\n",
    "            user_features[col_name] = user_features[col_name].fillna(0)\n",
    "        else:\n",
    "            user_features[col_name] = 0\n",
    "    \n",
    "    # Features d√©riv√©es de base\n",
    "    user_features['churn_signals'] = (user_features.get('page_downgrade', 0) + \n",
    "                                       user_features.get('page_cancel', 0) + \n",
    "                                       user_features.get('page_submit_downgrade', 0))\n",
    "    user_features['thumbs_ratio'] = user_features.get('page_thumbs_down', 0) / (user_features.get('page_thumbs_up', 0) + 1)\n",
    "    user_features['error_rate'] = (user_features.get('page_error', 0) / user_features['total_events']).fillna(0)\n",
    "    user_features['song_event_ratio'] = (user_features.get('page_nextsong', 0) / user_features['total_events']).fillna(0)\n",
    "    user_features['positive_interactions'] = (user_features.get('page_thumbs_up', 0) + \n",
    "                                               user_features.get('page_add_to_playlist', 0) + \n",
    "                                               user_features.get('page_add_friend', 0))\n",
    "    user_features['positive_interaction_rate'] = (user_features['positive_interactions'] / user_features['total_events']).fillna(0)\n",
    "    \n",
    "    # Derni√®re semaine\n",
    "    one_week_before = reference_date - pd.Timedelta(days=7)\n",
    "    last_week = df_filtered[df_filtered['time'] >= one_week_before].groupby('userId').size().reset_index()\n",
    "    last_week.columns = ['userId', 'events_last_week']\n",
    "    user_features = user_features.merge(last_week, on='userId', how='left')\n",
    "    user_features['events_last_week'] = user_features['events_last_week'].fillna(0)\n",
    "    user_features['activity_trend'] = user_features['events_last_week'] / (user_features['total_events'] + 1)\n",
    "    \n",
    "    return user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les features de base\n",
    "user_df_base = create_base_features(df_train, REFERENCE_DATE)\n",
    "print(f\"‚úÖ Features de base cr√©√©es: {user_df_base.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering - Version Am√©lior√©e üöÄ\n",
    "\n",
    "### Am√©liorations bas√©es sur l'analyse de segmentation :\n",
    "1. **Scores composites** (frustration, d√©sengagement)\n",
    "2. **Features temporelles multi-p√©riodes** (3j, 7j, 14j)\n",
    "3. **Distance aux profils de churners**\n",
    "4. **Indicateurs de risque**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_features(df, reference_date):\n",
    "    \"\"\"\n",
    "    Cr√©e les features am√©lior√©es bas√©es sur l'analyse de segmentation.\n",
    "    \"\"\"\n",
    "    df_filtered = df[df['time'] <= reference_date].copy()\n",
    "    \n",
    "    # Commencer avec les features de base\n",
    "    user_features = create_base_features(df, reference_date)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # AM√âLIORATION 1: SCORES COMPOSITES\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Score de FRUSTRATION (insight: les churners frustr√©s ont thumbs_ratio √©lev√© + aide + erreurs)\n",
    "    user_features['frustration_score'] = (\n",
    "        user_features['thumbs_ratio'] * 2 +  # Poids double pour le m√©contentement\n",
    "        (user_features.get('page_help', 0) / (user_features['total_events'] + 1)) * 3 +\n",
    "        user_features['error_rate'] * 2 +\n",
    "        (user_features.get('page_downgrade', 0) > 0).astype(int) * 5  # Signal fort\n",
    "    )\n",
    "    \n",
    "    # Score de D√âSENGAGEMENT (insight: 52% des churners sont des \"d√©sengag√©s progressifs\")\n",
    "    # Normaliser days_since_last_activity\n",
    "    max_inactivity = user_features['days_since_last_activity'].quantile(0.95)\n",
    "    user_features['disengagement_score'] = (\n",
    "        (user_features['days_since_last_activity'] / (max_inactivity + 1)).clip(0, 1) * 3 +\n",
    "        (1 - user_features['activity_trend'].clip(0, 1)) * 2 +\n",
    "        (1 - user_features['positive_interaction_rate'].clip(0, 1)) * 1\n",
    "    )\n",
    "    \n",
    "    # Score de RISQUE GLOBAL (combinaison)\n",
    "    user_features['risk_score'] = (\n",
    "        user_features['frustration_score'] * 0.4 +\n",
    "        user_features['disengagement_score'] * 0.6  # Plus de poids car 52% des churners\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # AM√âLIORATION 2: FEATURES TEMPORELLES MULTI-P√âRIODES\n",
    "    # =========================================================================\n",
    "    \n",
    "    periods = [3, 7, 14, 21]  # Jours\n",
    "    \n",
    "    for days in periods:\n",
    "        period_start = reference_date - pd.Timedelta(days=days)\n",
    "        period_data = df_filtered[df_filtered['time'] >= period_start]\n",
    "        \n",
    "        # √âv√©nements dans la p√©riode\n",
    "        period_events = period_data.groupby('userId').size().reset_index()\n",
    "        period_events.columns = ['userId', f'events_last_{days}d']\n",
    "        user_features = user_features.merge(period_events, on='userId', how='left')\n",
    "        user_features[f'events_last_{days}d'] = user_features[f'events_last_{days}d'].fillna(0)\n",
    "        \n",
    "        # Sessions dans la p√©riode\n",
    "        period_sessions = period_data.groupby('userId')['sessionId'].nunique().reset_index()\n",
    "        period_sessions.columns = ['userId', f'sessions_last_{days}d']\n",
    "        user_features = user_features.merge(period_sessions, on='userId', how='left')\n",
    "        user_features[f'sessions_last_{days}d'] = user_features[f'sessions_last_{days}d'].fillna(0)\n",
    "    \n",
    "    # Tendances (comparaison entre p√©riodes)\n",
    "    user_features['trend_7d_vs_14d'] = (\n",
    "        user_features['events_last_7d'] / (user_features['events_last_14d'] - user_features['events_last_7d'] + 1)\n",
    "    ).replace([np.inf, -np.inf], 0).clip(-10, 10)\n",
    "    \n",
    "    user_features['trend_3d_vs_7d'] = (\n",
    "        user_features['events_last_3d'] / (user_features['events_last_7d'] - user_features['events_last_3d'] + 1)\n",
    "    ).replace([np.inf, -np.inf], 0).clip(-10, 10)\n",
    "    \n",
    "    # Acc√©l√©ration du d√©sengagement\n",
    "    user_features['activity_acceleration'] = user_features['trend_3d_vs_7d'] - user_features['trend_7d_vs_14d']\n",
    "    \n",
    "    # =========================================================================\n",
    "    # AM√âLIORATION 3: INDICATEURS BINAIRES DE RISQUE\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Bas√© sur les insights de la segmentation\n",
    "    user_features['is_inactive_7d'] = (user_features['events_last_7d'] == 0).astype(int)\n",
    "    user_features['is_inactive_14d'] = (user_features['events_last_14d'] == 0).astype(int)\n",
    "    user_features['is_inactive_21d'] = (user_features['events_last_21d'] == 0).astype(int)\n",
    "    \n",
    "    user_features['has_visited_downgrade'] = (user_features.get('page_downgrade', 0) > 0).astype(int)\n",
    "    user_features['has_visited_cancel'] = (user_features.get('page_cancel', 0) > 0).astype(int)\n",
    "    user_features['has_visited_help'] = (user_features.get('page_help', 0) > 0).astype(int)\n",
    "    user_features['has_errors'] = (user_features.get('page_error', 0) > 0).astype(int)\n",
    "    \n",
    "    user_features['is_low_engagement'] = (user_features['positive_interactions'] < 1).astype(int)\n",
    "    user_features['is_declining'] = (user_features['trend_7d_vs_14d'] < 0.5).astype(int)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # AM√âLIORATION 4: FEATURES D'INTERACTION\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Interaction niveau x activit√©\n",
    "    user_features['level_encoded'] = LabelEncoder().fit_transform(user_features['level'].fillna('unknown'))\n",
    "    user_features['paid_x_inactive'] = user_features['level_encoded'] * user_features['days_since_last_activity']\n",
    "    user_features['paid_x_lowengagement'] = user_features['level_encoded'] * user_features['is_low_engagement']\n",
    "    \n",
    "    # Interaction anciennet√© x activit√©\n",
    "    user_features['tenure_x_inactive'] = user_features['days_since_registration'] * user_features['is_inactive_14d']\n",
    "    \n",
    "    return user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les features am√©lior√©es\n",
    "user_df_enhanced = create_enhanced_features(df_train, REFERENCE_DATE)\n",
    "print(f\"‚úÖ Features am√©lior√©es cr√©√©es: {user_df_enhanced.shape}\")\n",
    "print(f\"\\nüìä Nouvelles features ajout√©es: {user_df_enhanced.shape[1] - user_df_base.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ajout des Features de Distance aux Profils de Churners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_profile_distance_features(user_df, n_clusters=4):\n",
    "    \"\"\"\n",
    "    Ajoute les features de distance aux centro√Ødes des profils de churners.\n",
    "    \"\"\"\n",
    "    # Features pour le clustering\n",
    "    clustering_features = [\n",
    "        'total_events', 'unique_sessions', 'days_since_last_activity',\n",
    "        'events_per_day', 'positive_interactions', 'thumbs_ratio',\n",
    "        'page_help', 'error_rate', 'days_since_registration', 'activity_trend'\n",
    "    ]\n",
    "    clustering_features = [f for f in clustering_features if f in user_df.columns]\n",
    "    \n",
    "    # Filtrer les churners pour cr√©er les profils\n",
    "    churners = user_df[user_df['will_churn_10days'] == 1].copy()\n",
    "    \n",
    "    if len(churners) < n_clusters:\n",
    "        print(\"‚ö†Ô∏è Pas assez de churners pour cr√©er les profils\")\n",
    "        return user_df\n",
    "    \n",
    "    # Normaliser\n",
    "    scaler = StandardScaler()\n",
    "    X_churners = churners[clustering_features].fillna(0)\n",
    "    X_churners_scaled = scaler.fit_transform(X_churners)\n",
    "    \n",
    "    # Cr√©er les clusters de churners\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_STATE, n_init=10)\n",
    "    kmeans.fit(X_churners_scaled)\n",
    "    \n",
    "    # Calculer les distances pour TOUS les utilisateurs\n",
    "    X_all = user_df[clustering_features].fillna(0)\n",
    "    X_all_scaled = scaler.transform(X_all)\n",
    "    \n",
    "    # Distance √† chaque centro√Øde\n",
    "    distances = cdist(X_all_scaled, kmeans.cluster_centers_, metric='euclidean')\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        user_df[f'dist_to_churner_profile_{i}'] = distances[:, i]\n",
    "    \n",
    "    # Distance minimale (au profil le plus proche)\n",
    "    user_df['min_dist_to_churner'] = distances.min(axis=1)\n",
    "    \n",
    "    # Profil de churner le plus proche\n",
    "    user_df['closest_churner_profile'] = distances.argmin(axis=1)\n",
    "    \n",
    "    # Probabilit√© \"soft\" d'appartenir √† chaque profil (inverse de la distance normalis√©e)\n",
    "    dist_sum = distances.sum(axis=1, keepdims=True)\n",
    "    proba_profiles = 1 - (distances / (dist_sum + 0.001))\n",
    "    proba_profiles = proba_profiles / proba_profiles.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        user_df[f'proba_churner_profile_{i}'] = proba_profiles[:, i]\n",
    "    \n",
    "    # Score de similarit√© maximal avec un profil de churner\n",
    "    user_df['max_churner_similarity'] = proba_profiles.max(axis=1)\n",
    "    \n",
    "    print(f\"‚úÖ Features de distance aux {n_clusters} profils de churners ajout√©es\")\n",
    "    \n",
    "    return user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les features de distance\n",
    "user_df_enhanced = add_profile_distance_features(user_df_enhanced, n_clusters=4)\n",
    "print(f\"\\nüìä Total features: {user_df_enhanced.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparaison des Performances: Base vs Am√©lior√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(user_df, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Pr√©pare les features pour la mod√©lisation.\n",
    "    \"\"\"\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = ['userId', 'will_churn_10days', 'registration', \n",
    "                        'first_activity', 'last_activity', 'gender', 'level']\n",
    "    \n",
    "    # S√©lectionner les colonnes num√©riques\n",
    "    feature_cols = [col for col in user_df.columns \n",
    "                    if col not in exclude_cols \n",
    "                    and user_df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    X = user_df[feature_cols].fillna(0)\n",
    "    y = user_df['will_churn_10days']\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "\n",
    "def evaluate_model(X, y, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    √âvalue un mod√®le avec cross-validation.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Normaliser\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Mod√®les\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=RANDOM_STATE),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=10, class_weight='balanced', random_state=RANDOM_STATE, n_jobs=-1),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=RANDOM_STATE)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if name == 'Logistic Regression':\n",
    "            X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "        else:\n",
    "            X_tr, X_te = X_train, X_test\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_tr, y_train, cv=cv, scoring='roc_auc')\n",
    "        \n",
    "        # Fit et pr√©diction\n",
    "        model.fit(X_tr, y_train)\n",
    "        y_pred_proba = model.predict_proba(X_te)[:, 1]\n",
    "        y_pred = model.predict(X_te)\n",
    "        \n",
    "        results[name] = {\n",
    "            'cv_auc': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'test_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'test_f1': f1_score(y_test, y_pred),\n",
    "            'model': model,\n",
    "            'scaler': scaler\n",
    "        }\n",
    "    \n",
    "    return results, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation avec features de BASE\n",
    "print(\"=\"*60)\n",
    "print(\"üìä √âVALUATION - FEATURES DE BASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_base, y_base, features_base = prepare_features(user_df_base)\n",
    "print(f\"Nombre de features: {len(features_base)}\")\n",
    "\n",
    "results_base, _, _ = evaluate_model(X_base, y_base, \"Base\")\n",
    "\n",
    "print(\"\\nR√©sultats:\")\n",
    "for name, res in results_base.items():\n",
    "    print(f\"  {name}: CV AUC={res['cv_auc']:.4f} | Test AUC={res['test_auc']:.4f} | F1={res['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation avec features AM√âLIOR√âES\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ √âVALUATION - FEATURES AM√âLIOR√âES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_enhanced, y_enhanced, features_enhanced = prepare_features(user_df_enhanced)\n",
    "print(f\"Nombre de features: {len(features_enhanced)}\")\n",
    "\n",
    "results_enhanced, X_test_final, y_test_final = evaluate_model(X_enhanced, y_enhanced, \"Enhanced\")\n",
    "\n",
    "print(\"\\nR√©sultats:\")\n",
    "for name, res in results_enhanced.items():\n",
    "    print(f\"  {name}: CV AUC={res['cv_auc']:.4f} | Test AUC={res['test_auc']:.4f} | F1={res['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des am√©liorations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà AM√âLIORATION DES PERFORMANCES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_data = []\n",
    "for model_name in results_base.keys():\n",
    "    base_auc = results_base[model_name]['test_auc']\n",
    "    enhanced_auc = results_enhanced[model_name]['test_auc']\n",
    "    improvement = (enhanced_auc - base_auc) / base_auc * 100\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Mod√®le': model_name,\n",
    "        'AUC Base': base_auc,\n",
    "        'AUC Am√©lior√©': enhanced_auc,\n",
    "        'Am√©lioration (%)': improvement\n",
    "    })\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Base: {base_auc:.4f} ‚Üí Am√©lior√©: {enhanced_auc:.4f} ({improvement:+.2f}%)\")\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la comparaison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Barplot comparatif\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, comparison_df['AUC Base'], width, label='Base', color='#3498db')\n",
    "bars2 = ax1.bar(x + width/2, comparison_df['AUC Am√©lior√©'], width, label='Am√©lior√©', color='#2ecc71')\n",
    "\n",
    "ax1.set_ylabel('AUC Score')\n",
    "ax1.set_title('Comparaison AUC: Base vs Am√©lior√©', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(['LogReg', 'RF', 'GB'])\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0.7, 0.85)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar in bars1:\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. Am√©lioration en %\n",
    "ax2 = axes[1]\n",
    "colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in comparison_df['Am√©lioration (%)']]\n",
    "bars = ax2.bar(comparison_df['Mod√®le'], comparison_df['Am√©lioration (%)'], color=colors)\n",
    "ax2.set_ylabel('Am√©lioration (%)')\n",
    "ax2.set_title('Gain de Performance (%)', fontsize=13, fontweight='bold')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, height + 0.1, \n",
    "             f'{height:+.2f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyse des Features les Plus Importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance du meilleur mod√®le (Gradient Boosting ou Random Forest)\n",
    "best_model_name = max(results_enhanced.keys(), key=lambda k: results_enhanced[k]['test_auc'])\n",
    "best_model = results_enhanced[best_model_name]['model']\n",
    "\n",
    "print(f\"üèÜ Meilleur mod√®le: {best_model_name} (AUC={results_enhanced[best_model_name]['test_auc']:.4f})\")\n",
    "\n",
    "# Feature importance\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': features_enhanced,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüìä Top 20 Features les Plus Importantes:\")\n",
    "    print(importance_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des features importantes\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "top_20 = importance_df.head(20)\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(top_20)))\n",
    "\n",
    "ax.barh(range(len(top_20)), top_20['importance'], color=colors)\n",
    "ax.set_yticks(range(len(top_20)))\n",
    "ax.set_yticklabels(top_20['feature'])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title(f'Top 20 Features ({best_model_name})', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "# Highlight les nouvelles features\n",
    "new_features = ['frustration_score', 'disengagement_score', 'risk_score', \n",
    "                'min_dist_to_churner', 'max_churner_similarity',\n",
    "                'trend_7d_vs_14d', 'trend_3d_vs_7d', 'activity_acceleration']\n",
    "\n",
    "for i, feat in enumerate(top_20['feature']):\n",
    "    if any(nf in feat for nf in new_features) or 'dist_to_churner' in feat or 'proba_churner' in feat:\n",
    "        ax.get_yticklabels()[i].set_color('green')\n",
    "        ax.get_yticklabels()[i].set_fontweight('bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_enhanced.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüíö Les features en vert sont les NOUVELLES features ajout√©es gr√¢ce √† l'analyse de segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entra√Ænement du Mod√®le Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner le mod√®le final sur TOUTES les donn√©es\n",
    "print(\"üéØ Entra√Ænement du mod√®le final sur l'ensemble des donn√©es...\")\n",
    "\n",
    "# Utiliser le meilleur mod√®le\n",
    "final_model = GradientBoostingClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Scaler final\n",
    "final_scaler = StandardScaler()\n",
    "X_all = user_df_enhanced[features_enhanced].fillna(0)\n",
    "y_all = user_df_enhanced['will_churn_10days']\n",
    "\n",
    "# Fit sur toutes les donn√©es\n",
    "final_model.fit(X_all, y_all)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le final entra√Æn√© sur {len(X_all)} utilisateurs\")\n",
    "print(f\"üìä Features utilis√©es: {len(features_enhanced)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üì§ 9. SOUMISSION KAGGLE\n",
    "\n",
    "Cette section permet de g√©n√©rer le fichier de soumission √† partir du dataset de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_for_submission(df, reference_date):\n",
    "    \"\"\"\n",
    "    Cr√©e les features pour le dataset de test (sans la colonne will_churn_10days).\n",
    "    Version adapt√©e pour la soumission.\n",
    "    \"\"\"\n",
    "    df_filtered = df[df['time'] <= reference_date].copy()\n",
    "    \n",
    "    user_features = pd.DataFrame()\n",
    "    user_features['userId'] = df_filtered['userId'].unique()\n",
    "    \n",
    "    # Info utilisateur (sans will_churn_10days)\n",
    "    user_info = df_filtered.groupby('userId').agg({\n",
    "        'gender': 'first',\n",
    "        'level': 'last',\n",
    "        'registration': 'first',\n",
    "    }).reset_index()\n",
    "    user_features = user_features.merge(user_info, on='userId', how='left')\n",
    "    user_features['days_since_registration'] = (reference_date - user_features['registration']).dt.days\n",
    "    \n",
    "    # Activit√© globale\n",
    "    activity = df_filtered.groupby('userId').agg({\n",
    "        'ts': 'count',\n",
    "        'sessionId': 'nunique',\n",
    "        'time': ['min', 'max'],\n",
    "        'length': ['sum', 'mean', 'count'],\n",
    "    })\n",
    "    activity.columns = ['total_events', 'unique_sessions', 'first_activity', 'last_activity',\n",
    "                        'total_listening_time', 'avg_song_length', 'songs_with_length']\n",
    "    activity = activity.reset_index()\n",
    "    user_features = user_features.merge(activity, on='userId', how='left')\n",
    "    \n",
    "    # M√©triques temporelles\n",
    "    user_features['days_since_last_activity'] = (reference_date - pd.to_datetime(user_features['last_activity'])).dt.days\n",
    "    user_features['activity_span_days'] = (pd.to_datetime(user_features['last_activity']) - pd.to_datetime(user_features['first_activity'])).dt.days + 1\n",
    "    user_features['events_per_day'] = (user_features['total_events'] / user_features['activity_span_days']).replace([np.inf, -np.inf], 0)\n",
    "    user_features['sessions_per_day'] = (user_features['unique_sessions'] / user_features['activity_span_days']).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Pages visit√©es\n",
    "    important_pages = ['NextSong', 'Home', 'Thumbs Up', 'Thumbs Down', 'Add to Playlist', \n",
    "                       'Add Friend', 'Roll Advert', 'Downgrade', 'Cancel', 'Submit Downgrade', \n",
    "                       'Error', 'Help', 'Settings', 'Logout', 'Upgrade']\n",
    "    \n",
    "    page_counts = df_filtered.groupby(['userId', 'page']).size().unstack(fill_value=0)\n",
    "    for page in important_pages:\n",
    "        col_name = f'page_{page.lower().replace(\" \", \"_\")}'\n",
    "        if page in page_counts.columns:\n",
    "            temp = page_counts[[page]].reset_index()\n",
    "            temp.columns = ['userId', col_name]\n",
    "            user_features = user_features.merge(temp, on='userId', how='left')\n",
    "            user_features[col_name] = user_features[col_name].fillna(0)\n",
    "        else:\n",
    "            user_features[col_name] = 0\n",
    "    \n",
    "    # Features d√©riv√©es\n",
    "    user_features['churn_signals'] = (user_features.get('page_downgrade', 0) + \n",
    "                                       user_features.get('page_cancel', 0) + \n",
    "                                       user_features.get('page_submit_downgrade', 0))\n",
    "    user_features['thumbs_ratio'] = user_features.get('page_thumbs_down', 0) / (user_features.get('page_thumbs_up', 0) + 1)\n",
    "    user_features['error_rate'] = (user_features.get('page_error', 0) / user_features['total_events']).fillna(0)\n",
    "    user_features['song_event_ratio'] = (user_features.get('page_nextsong', 0) / user_features['total_events']).fillna(0)\n",
    "    user_features['positive_interactions'] = (user_features.get('page_thumbs_up', 0) + \n",
    "                                               user_features.get('page_add_to_playlist', 0) + \n",
    "                                               user_features.get('page_add_friend', 0))\n",
    "    user_features['positive_interaction_rate'] = (user_features['positive_interactions'] / user_features['total_events']).fillna(0)\n",
    "    \n",
    "    # Derni√®re semaine\n",
    "    one_week_before = reference_date - pd.Timedelta(days=7)\n",
    "    last_week = df_filtered[df_filtered['time'] >= one_week_before].groupby('userId').size().reset_index()\n",
    "    last_week.columns = ['userId', 'events_last_week']\n",
    "    user_features = user_features.merge(last_week, on='userId', how='left')\n",
    "    user_features['events_last_week'] = user_features['events_last_week'].fillna(0)\n",
    "    user_features['activity_trend'] = user_features['events_last_week'] / (user_features['total_events'] + 1)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FEATURES AM√âLIOR√âES\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Scores composites\n",
    "    user_features['frustration_score'] = (\n",
    "        user_features['thumbs_ratio'] * 2 +\n",
    "        (user_features.get('page_help', 0) / (user_features['total_events'] + 1)) * 3 +\n",
    "        user_features['error_rate'] * 2 +\n",
    "        (user_features.get('page_downgrade', 0) > 0).astype(int) * 5\n",
    "    )\n",
    "    \n",
    "    max_inactivity = user_features['days_since_last_activity'].quantile(0.95)\n",
    "    user_features['disengagement_score'] = (\n",
    "        (user_features['days_since_last_activity'] / (max_inactivity + 1)).clip(0, 1) * 3 +\n",
    "        (1 - user_features['activity_trend'].clip(0, 1)) * 2 +\n",
    "        (1 - user_features['positive_interaction_rate'].clip(0, 1)) * 1\n",
    "    )\n",
    "    \n",
    "    user_features['risk_score'] = (\n",
    "        user_features['frustration_score'] * 0.4 +\n",
    "        user_features['disengagement_score'] * 0.6\n",
    "    )\n",
    "    \n",
    "    # Features temporelles multi-p√©riodes\n",
    "    periods = [3, 7, 14, 21]\n",
    "    for days in periods:\n",
    "        period_start = reference_date - pd.Timedelta(days=days)\n",
    "        period_data = df_filtered[df_filtered['time'] >= period_start]\n",
    "        \n",
    "        period_events = period_data.groupby('userId').size().reset_index()\n",
    "        period_events.columns = ['userId', f'events_last_{days}d']\n",
    "        user_features = user_features.merge(period_events, on='userId', how='left')\n",
    "        user_features[f'events_last_{days}d'] = user_features[f'events_last_{days}d'].fillna(0)\n",
    "        \n",
    "        period_sessions = period_data.groupby('userId')['sessionId'].nunique().reset_index()\n",
    "        period_sessions.columns = ['userId', f'sessions_last_{days}d']\n",
    "        user_features = user_features.merge(period_sessions, on='userId', how='left')\n",
    "        user_features[f'sessions_last_{days}d'] = user_features[f'sessions_last_{days}d'].fillna(0)\n",
    "    \n",
    "    # Tendances\n",
    "    user_features['trend_7d_vs_14d'] = (\n",
    "        user_features['events_last_7d'] / (user_features['events_last_14d'] - user_features['events_last_7d'] + 1)\n",
    "    ).replace([np.inf, -np.inf], 0).clip(-10, 10)\n",
    "    \n",
    "    user_features['trend_3d_vs_7d'] = (\n",
    "        user_features['events_last_3d'] / (user_features['events_last_7d'] - user_features['events_last_3d'] + 1)\n",
    "    ).replace([np.inf, -np.inf], 0).clip(-10, 10)\n",
    "    \n",
    "    user_features['activity_acceleration'] = user_features['trend_3d_vs_7d'] - user_features['trend_7d_vs_14d']\n",
    "    \n",
    "    # Indicateurs binaires\n",
    "    user_features['is_inactive_7d'] = (user_features['events_last_7d'] == 0).astype(int)\n",
    "    user_features['is_inactive_14d'] = (user_features['events_last_14d'] == 0).astype(int)\n",
    "    user_features['is_inactive_21d'] = (user_features['events_last_21d'] == 0).astype(int)\n",
    "    user_features['has_visited_downgrade'] = (user_features.get('page_downgrade', 0) > 0).astype(int)\n",
    "    user_features['has_visited_cancel'] = (user_features.get('page_cancel', 0) > 0).astype(int)\n",
    "    user_features['has_visited_help'] = (user_features.get('page_help', 0) > 0).astype(int)\n",
    "    user_features['has_errors'] = (user_features.get('page_error', 0) > 0).astype(int)\n",
    "    user_features['is_low_engagement'] = (user_features['positive_interactions'] < 1).astype(int)\n",
    "    user_features['is_declining'] = (user_features['trend_7d_vs_14d'] < 0.5).astype(int)\n",
    "    \n",
    "    # Features d'interaction\n",
    "    user_features['level_encoded'] = LabelEncoder().fit_transform(user_features['level'].fillna('unknown'))\n",
    "    user_features['paid_x_inactive'] = user_features['level_encoded'] * user_features['days_since_last_activity']\n",
    "    user_features['paid_x_lowengagement'] = user_features['level_encoded'] * user_features['is_low_engagement']\n",
    "    user_features['tenure_x_inactive'] = user_features['days_since_registration'] * user_features['is_inactive_14d']\n",
    "    \n",
    "    return user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_profile_distances_for_test(test_features, train_enhanced, n_clusters=4):\n",
    "    \"\"\"\n",
    "    Ajoute les distances aux profils de churners pour les donn√©es de test.\n",
    "    Utilise les centro√Ødes calcul√©s sur les donn√©es d'entra√Ænement.\n",
    "    \"\"\"\n",
    "    clustering_features = [\n",
    "        'total_events', 'unique_sessions', 'days_since_last_activity',\n",
    "        'events_per_day', 'positive_interactions', 'thumbs_ratio',\n",
    "        'page_help', 'error_rate', 'days_since_registration', 'activity_trend'\n",
    "    ]\n",
    "    clustering_features = [f for f in clustering_features if f in test_features.columns]\n",
    "    \n",
    "    # Entra√Æner sur les churners du train\n",
    "    churners_train = train_enhanced[train_enhanced['will_churn_10days'] == 1]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_churners = churners_train[clustering_features].fillna(0)\n",
    "    X_churners_scaled = scaler.fit_transform(X_churners)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_STATE, n_init=10)\n",
    "    kmeans.fit(X_churners_scaled)\n",
    "    \n",
    "    # Appliquer sur test\n",
    "    X_test = test_features[clustering_features].fillna(0)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    distances = cdist(X_test_scaled, kmeans.cluster_centers_, metric='euclidean')\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        test_features[f'dist_to_churner_profile_{i}'] = distances[:, i]\n",
    "    \n",
    "    test_features['min_dist_to_churner'] = distances.min(axis=1)\n",
    "    test_features['closest_churner_profile'] = distances.argmin(axis=1)\n",
    "    \n",
    "    dist_sum = distances.sum(axis=1, keepdims=True)\n",
    "    proba_profiles = 1 - (distances / (dist_sum + 0.001))\n",
    "    proba_profiles = proba_profiles / proba_profiles.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        test_features[f'proba_churner_profile_{i}'] = proba_profiles[:, i]\n",
    "    \n",
    "    test_features['max_churner_similarity'] = proba_profiles.max(axis=1)\n",
    "    \n",
    "    return test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(df_test, train_enhanced, model, feature_cols, reference_date, output_filename='submission.csv'):\n",
    "    \"\"\"\n",
    "    G√©n√®re le fichier de soumission Kaggle.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_test : DataFrame\n",
    "        Donn√©es de test (format √©v√©nements)\n",
    "    train_enhanced : DataFrame  \n",
    "        Donn√©es d'entra√Ænement avec features (pour calculer les profils)\n",
    "    model : sklearn model\n",
    "        Mod√®le entra√Æn√©\n",
    "    feature_cols : list\n",
    "        Liste des features utilis√©es\n",
    "    reference_date : datetime\n",
    "        Date de r√©f√©rence\n",
    "    output_filename : str\n",
    "        Nom du fichier de sortie\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame de soumission\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Cr√©ation des features pour les donn√©es de test...\")\n",
    "    \n",
    "    # Cr√©er les features\n",
    "    test_features = create_features_for_submission(df_test, reference_date)\n",
    "    print(f\"   Features de base cr√©√©es: {test_features.shape}\")\n",
    "    \n",
    "    # Ajouter les distances aux profils\n",
    "    test_features = add_profile_distances_for_test(test_features, train_enhanced, n_clusters=4)\n",
    "    print(f\"   Features de distance ajout√©es: {test_features.shape}\")\n",
    "    \n",
    "    # S'assurer que toutes les colonnes n√©cessaires existent\n",
    "    for col in feature_cols:\n",
    "        if col not in test_features.columns:\n",
    "            test_features[col] = 0\n",
    "    \n",
    "    # Pr√©parer X\n",
    "    X_test = test_features[feature_cols].fillna(0)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    print(\"üéØ G√©n√©ration des pr√©dictions...\")\n",
    "    predictions_proba = model.predict_proba(X_test)[:, 1]\n",
    "    predictions_binary = (predictions_proba >= 0.5).astype(int)\n",
    "    \n",
    "    # Cr√©er le fichier de soumission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_features['userId'].astype(int),\n",
    "        'target': predictions_binary\n",
    "    })\n",
    "    \n",
    "    # Sauvegarder\n",
    "    submission.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Fichier de soumission cr√©√©: {output_filename}\")\n",
    "    print(f\"üìä Statistiques:\")\n",
    "    print(f\"   - Total utilisateurs: {len(submission):,}\")\n",
    "    print(f\"   - Pr√©dictions churn (1): {submission['target'].sum():,} ({submission['target'].mean()*100:.1f}%)\")\n",
    "    print(f\"   - Pr√©dictions non-churn (0): {(submission['target']==0).sum():,}\")\n",
    "    \n",
    "    return submission, predictions_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì• Charger les donn√©es de test et g√©n√©rer la soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üéØ G√âN√âRER LA SOUMISSION KAGGLE\n",
    "# ============================================================\n",
    "\n",
    "# Charger les donn√©es de test (format parquet)\n",
    "df_test = pd.read_parquet('df_test.parquet')\n",
    "\n",
    "# Convertir les timestamps\n",
    "df_test['time'] = pd.to_datetime(df_test['time'])\n",
    "df_test['registration'] = pd.to_datetime(df_test['registration'])\n",
    "\n",
    "print(f\"üìä Donn√©es de test charg√©es: {len(df_test):,} √©v√©nements\")\n",
    "print(f\"üë• Utilisateurs uniques: {df_test['userId'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer la soumission\n",
    "submission, probas = generate_submission(\n",
    "    df_test=df_test,\n",
    "    train_enhanced=user_df_enhanced,\n",
    "    model=final_model,\n",
    "    feature_cols=features_enhanced,\n",
    "    reference_date=REFERENCE_DATE,\n",
    "    output_filename='submission.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aper√ßu de la soumission\n",
    "print(\"\\nüìã Aper√ßu du fichier de soumission:\")\n",
    "submission.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des probabilit√©s\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogramme des probabilit√©s\n",
    "ax1 = axes[0]\n",
    "ax1.hist(probas, bins=50, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(x=0.5, color='red', linestyle='--', label='Seuil (0.5)')\n",
    "ax1.set_xlabel('Probabilit√© de churn')\n",
    "ax1.set_ylabel('Nombre d\\'utilisateurs')\n",
    "ax1.set_title('Distribution des Probabilit√©s de Churn', fontsize=13, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution des pr√©dictions\n",
    "ax2 = axes[1]\n",
    "counts = submission['target'].value_counts()\n",
    "ax2.bar(['Non-Churn (0)', 'Churn (1)'], counts.values, color=['#2ecc71', '#e74c3c'], edgecolor='black')\n",
    "ax2.set_ylabel('Nombre d\\'utilisateurs')\n",
    "ax2.set_title('Distribution des Pr√©dictions', fontsize=13, fontweight='bold')\n",
    "\n",
    "for i, count in enumerate(counts.values):\n",
    "    ax2.text(i, count + 20, f'{count:,}\\n({count/len(submission)*100:.1f}%)', \n",
    "             ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('submission_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéöÔ∏è Ajuster le seuil de d√©cision (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission_with_threshold(probas, user_ids, threshold=0.5, output_filename='submission.csv'):\n",
    "    \"\"\"\n",
    "    G√©n√®re une soumission avec un seuil personnalis√©.\n",
    "    \"\"\"\n",
    "    predictions = (probas >= threshold).astype(int)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': user_ids.astype(int),\n",
    "        'target': predictions\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Soumission avec seuil={threshold} cr√©√©e: {output_filename}\")\n",
    "    print(f\"   Churn pr√©dits: {predictions.sum():,} ({predictions.mean()*100:.1f}%)\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# Exemple: ajuster le seuil si n√©cessaire\n",
    "# submission_adjusted = generate_submission_with_threshold(\n",
    "#     probas=probas,\n",
    "#     user_ids=submission['id'],\n",
    "#     threshold=0.4,  # Seuil plus bas = plus de pr√©dictions de churn\n",
    "#     output_filename='submission_threshold_04.csv'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä R√©sum√© Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                    R√âSUM√â DU PIPELINE                                ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä AM√âLIORATION DES PERFORMANCES\n",
    "{'‚îÄ'*50}\n",
    "‚Ä¢ Features de base: {len(features_base)}\n",
    "‚Ä¢ Features am√©lior√©es: {len(features_enhanced)} (+{len(features_enhanced)-len(features_base)} nouvelles)\n",
    "\n",
    "üöÄ NOUVELLES FEATURES AJOUT√âES:\n",
    "‚Ä¢ Scores composites: frustration_score, disengagement_score, risk_score\n",
    "‚Ä¢ Tendances multi-p√©riodes: 3j, 7j, 14j, 21j\n",
    "‚Ä¢ Distance aux profils de churners: 4 profils\n",
    "‚Ä¢ Indicateurs binaires de risque\n",
    "‚Ä¢ Features d'interaction\n",
    "\n",
    "üìà GAINS DE PERFORMANCE:\n",
    "\"\"\")\n",
    "\n",
    "for row in comparison_data:\n",
    "    print(f\"‚Ä¢ {row['Mod√®le']}: {row['AUC Base']:.4f} ‚Üí {row['AUC Am√©lior√©']:.4f} ({row['Am√©lioration (%)']:+.2f}%)\")\n",
    "\n",
    "print(f\"\"\"\n",
    "üì§ SOUMISSION KAGGLE\n",
    "{'‚îÄ'*50}\n",
    "‚Ä¢ Fichier: submission.csv\n",
    "‚Ä¢ Format: id, target (0 ou 1)\n",
    "‚Ä¢ Utilisateurs: {len(submission):,}\n",
    "‚Ä¢ Taux de churn pr√©dit: {submission['target'].mean()*100:.1f}%\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
