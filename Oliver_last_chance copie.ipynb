{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a83f3d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 175059 rows, 18048 users\n",
      "Churners: 3435\n",
      "Non-churners: 14613\n",
      "\n",
      "Balanced dataset:\n",
      "Total: 18048 rows (one per user)\n",
      "Churners: 3435\n",
      "Non-churners: 14613\n",
      "Churn rate: 19.0%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FIX: Create better training data\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your features with target\n",
    "features = pd.read_csv('features_with_target.csv')\n",
    "features['session_time'] = pd.to_datetime(features['session_time'])\n",
    "\n",
    "print(f\"Original features: {len(features)} rows, {features['userId'].nunique()} users\")\n",
    "\n",
    "# Identify churners\n",
    "churner_ids = features[features['will_churn_10days'] == 1]['userId'].unique()\n",
    "non_churner_ids = features[~features['userId'].isin(churner_ids)]['userId'].unique()\n",
    "\n",
    "print(f\"Churners: {len(churner_ids)}\")\n",
    "print(f\"Non-churners: {len(non_churner_ids)}\")\n",
    "\n",
    "# For CHURNERS: Keep only their LAST session (the one closest to churn)\n",
    "churner_data = features[features['userId'].isin(churner_ids)].copy()\n",
    "churner_last_sessions = churner_data.sort_values('session_time').groupby('userId').last().reset_index()\n",
    "churner_last_sessions['will_churn_10days'] = 1  # They will churn\n",
    "\n",
    "# For NON-CHURNERS: Keep only their LAST session too (to match)\n",
    "non_churner_data = features[features['userId'].isin(non_churner_ids)].copy()\n",
    "non_churner_last_sessions = non_churner_data.sort_values('session_time').groupby('userId').last().reset_index()\n",
    "non_churner_last_sessions['will_churn_10days'] = 0  # They won't churn\n",
    "\n",
    "# Combine\n",
    "df_balanced = pd.concat([churner_last_sessions, non_churner_last_sessions], ignore_index=True)\n",
    "\n",
    "print(f\"\\nBalanced dataset:\")\n",
    "print(f\"Total: {len(df_balanced)} rows (one per user)\")\n",
    "print(f\"Churners: {(df_balanced['will_churn_10days'] == 1).sum()}\")\n",
    "print(f\"Non-churners: {(df_balanced['will_churn_10days'] == 0).sum()}\")\n",
    "print(f\"Churn rate: {df_balanced['will_churn_10days'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "57972813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 28\n",
      "X shape: (18048, 28)\n",
      "y distribution:\n",
      "will_churn_10days\n",
      "0    14613\n",
      "1     3435\n",
      "Name: count, dtype: int64\n",
      "\n",
      "scale_pos_weight: 4.25\n",
      "\n",
      "Balanced Accuracy: 0.7270\n",
      "ROC-AUC: 0.7956\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2327  596]\n",
      " [ 235  452]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.80      0.85      2923\n",
      "           1       0.43      0.66      0.52       687\n",
      "\n",
      "    accuracy                           0.77      3610\n",
      "   macro avg       0.67      0.73      0.68      3610\n",
      "weighted avg       0.82      0.77      0.79      3610\n",
      "\n",
      "\n",
      "Prediction distribution on validation:\n",
      "0    2562\n",
      "1    1048\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RETRAIN MODEL ON BALANCED DATA\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "\n",
    "# Prepare features\n",
    "feature_cols = [col for col in df_balanced.columns \n",
    "                if col not in ['userId', 'sessionId', 'session_time', 'will_churn_10days',\n",
    "                              'favorite_genre', 'favorite_artist']]\n",
    "\n",
    "X = df_balanced[feature_cols].fillna(0)\n",
    "y = df_balanced['will_churn_10days']\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Model\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"\\nscale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "model_balanced = xgb.XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.01,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_balanced.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model_balanced.predict(X_test)\n",
    "y_proba = model_balanced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"\\nBalanced Accuracy: {balanced_accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Check prediction distribution on test split\n",
    "print(f\"\\nPrediction distribution on validation:\")\n",
    "print(pd.Series(y_pred).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6a38fc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle test predictions:\n",
      "0    1913\n",
      "1     990\n",
      "Name: count, dtype: int64\n",
      "Churn rate: 34.1%\n",
      "\n",
      "✅ Submission saved!\n",
      "Target distribution:\n",
      "target\n",
      "0    1914\n",
      "1     990\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREDICT ON KAGGLE TEST DATA\n",
    "# ============================================================================\n",
    "\n",
    "# Use last sessions from test data\n",
    "X_kaggle = last_sessions[feature_cols].fillna(0)\n",
    "\n",
    "# Predict with new model\n",
    "y_kaggle_proba = model_balanced.predict_proba(X_kaggle)[:, 1]\n",
    "y_kaggle_pred = model_balanced.predict(X_kaggle)\n",
    "\n",
    "print(f\"Kaggle test predictions:\")\n",
    "print(pd.Series(y_kaggle_pred).value_counts())\n",
    "print(f\"Churn rate: {y_kaggle_pred.mean():.1%}\")\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': last_sessions['userId'],\n",
    "    'target': y_kaggle_pred\n",
    "})\n",
    "\n",
    "# Add user 1261737\n",
    "if '1261737' not in submission['id'].values:\n",
    "    submission = pd.concat([submission, pd.DataFrame({'id': ['1261737'], 'target': [0]})], ignore_index=True)\n",
    "\n",
    "submission = submission.sort_values('id').reset_index(drop=True)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\n✅ Submission saved!\")\n",
    "print(f\"Target distribution:\\n{submission['target'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "35b6abc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROBABILITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Probability distribution:\n",
      "count    2903.000000\n",
      "mean        0.437601\n",
      "std         0.137666\n",
      "min         0.130321\n",
      "25%         0.316511\n",
      "50%         0.443403\n",
      "75%         0.535543\n",
      "max         0.908271\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREDICTIONS AT DIFFERENT THRESHOLDS\n",
      "================================================================================\n",
      "Threshold 0.2: 2844 churners (98.0%)\n",
      "Threshold 0.3: 2278 churners (78.5%)\n",
      "Threshold 0.4: 1729 churners (59.6%)\n",
      "Threshold 0.5:  990 churners (34.1%)\n",
      "Threshold 0.6:  380 churners (13.1%)\n",
      "Threshold 0.7:   79 churners ( 2.7%)\n",
      "Threshold 0.8:    9 churners ( 0.3%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ANALYZE PROBABILITIES TO FIND OPTIMAL THRESHOLD\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PROBABILITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nProbability distribution:\")\n",
    "print(pd.Series(y_kaggle_proba).describe())\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y_kaggle_proba, bins=50, edgecolor='black')\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Threshold 0.5')\n",
    "plt.xlabel('Churn Probability')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Churn Probabilities')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_kaggle_proba, bins=50, edgecolor='black', cumulative=True, density=True)\n",
    "plt.xlabel('Churn Probability')\n",
    "plt.ylabel('Cumulative %')\n",
    "plt.title('Cumulative Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show predictions at different thresholds\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTIONS AT DIFFERENT THRESHOLDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for threshold in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "    preds = (y_kaggle_proba >= threshold).astype(int)\n",
    "    churn_rate = preds.mean()\n",
    "    n_churners = preds.sum()\n",
    "    print(f\"Threshold {threshold}: {n_churners:4d} churners ({churn_rate:5.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "27782024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.3: 2278 churners (78.5%) -> submission_t03.csv\n",
      "Threshold 0.4: 1729 churners (59.6%) -> submission_t04.csv\n",
      "Threshold 0.5: 990 churners (34.1%) -> submission_t05.csv\n",
      "Threshold 0.6: 380 churners (13.1%) -> submission_t06.csv\n",
      "Threshold 0.7: 79 churners (2.7%) -> submission_t07.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002283</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>1999455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>1999691</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>1999720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>1999908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>1999996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2904 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  target\n",
       "0     1000655       0\n",
       "1     1000963       0\n",
       "2     1001129       0\n",
       "3     1001963       0\n",
       "4     1002283       0\n",
       "...       ...     ...\n",
       "2899  1999455       0\n",
       "2900  1999691       1\n",
       "2901  1999720       0\n",
       "2902  1999908       0\n",
       "2903  1999996       0\n",
       "\n",
       "[2904 rows x 2 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE SUBMISSIONS WITH DIFFERENT THRESHOLDS\n",
    "# ============================================================================\n",
    "\n",
    "def create_submission(proba, threshold, filename):\n",
    "    preds = (proba >= threshold).astype(int)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': last_sessions['userId'],\n",
    "        'target': preds\n",
    "    })\n",
    "    \n",
    "    # Add user 1261737\n",
    "    if '1261737' not in submission['id'].values:\n",
    "        submission = pd.concat([submission, pd.DataFrame({'id': ['1261737'], 'target': [0]})], ignore_index=True)\n",
    "    \n",
    "    submission = submission.sort_values('id').reset_index(drop=True)\n",
    "    submission.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"Threshold {threshold}: {preds.sum()} churners ({preds.mean():.1%}) -> {filename}\")\n",
    "    return submission\n",
    "\n",
    "# Try multiple thresholds\n",
    "create_submission(y_kaggle_proba, 0.3, 'submission_t03.csv')\n",
    "create_submission(y_kaggle_proba, 0.4, 'submission_t04.csv')\n",
    "create_submission(y_kaggle_proba, 0.5, 'submission_t05.csv')\n",
    "create_submission(y_kaggle_proba, 0.6, 'submission_t06.csv')\n",
    "create_submission(y_kaggle_proba, 0.7, 'submission_t07.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "40c48090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 28\n",
      "With extra features: 32\n",
      "\n",
      "With Extra Features:\n",
      "Balanced Accuracy: 0.7439\n",
      "ROC-AUC: 0.8277\n",
      "Threshold 0.5: 708 churners (24.4%) -> submission_extra_features.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>1999455</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>1999691</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>1999720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>1999908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>1999996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2904 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  target\n",
       "0     1000655       0\n",
       "1     1000963       0\n",
       "2     1001129       0\n",
       "3     1001963       0\n",
       "4     1002283       1\n",
       "...       ...     ...\n",
       "2899  1999455       1\n",
       "2900  1999691       1\n",
       "2901  1999720       1\n",
       "2902  1999908       0\n",
       "2903  1999996       0\n",
       "\n",
       "[2904 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ADD SIMPLE ADDITIONAL FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "def add_extra_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Interaction features\n",
    "    if 'frustration_score' in df.columns and 'is_paid' in df.columns:\n",
    "        df['frustration_x_paid'] = df['frustration_score'] * df['is_paid']\n",
    "    \n",
    "    if 'thumbs_down_lifetime' in df.columns and 'thumbs_up_lifetime' in df.columns:\n",
    "        df['thumbs_ratio'] = df['thumbs_down_lifetime'] / (df['thumbs_up_lifetime'] + 1)\n",
    "    \n",
    "    if 'songs_listened_last_14days' in df.columns and 'total_songs_listened' in df.columns:\n",
    "        df['recent_activity_ratio'] = df['songs_listened_last_14days'] / (df['total_songs_listened'] + 1)\n",
    "    \n",
    "    if 'sessions_last7d_vs_avg' in df.columns and 'activity_trend_last_14days' in df.columns:\n",
    "        df['engagement_trend'] = df['sessions_last7d_vs_avg'] * df['activity_trend_last_14days']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add features to training data\n",
    "df_balanced_extra = add_extra_features(df_balanced)\n",
    "\n",
    "# Update feature columns\n",
    "feature_cols_extra = [col for col in df_balanced_extra.columns \n",
    "                      if col not in ['userId', 'sessionId', 'session_time', 'will_churn_10days',\n",
    "                                    'favorite_genre', 'favorite_artist']]\n",
    "\n",
    "print(f\"Original features: {len(feature_cols)}\")\n",
    "print(f\"With extra features: {len(feature_cols_extra)}\")\n",
    "\n",
    "# Retrain\n",
    "X_extra = df_balanced_extra[feature_cols_extra].fillna(0)\n",
    "y_extra = df_balanced_extra['will_churn_10days']\n",
    "\n",
    "X_train_e, X_test_e, y_train_e, y_test_e = train_test_split(\n",
    "    X_extra, y_extra, test_size=0.2, random_state=42, stratify=y_extra\n",
    ")\n",
    "\n",
    "model_extra = xgb.XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_extra.fit(X_train_e, y_train_e)\n",
    "\n",
    "y_pred_e = model_extra.predict(X_test_e)\n",
    "y_proba_e = model_extra.predict_proba(X_test_e)[:, 1]\n",
    "\n",
    "print(f\"\\nWith Extra Features:\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test_e, y_pred_e):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test_e, y_proba_e):.4f}\")\n",
    "\n",
    "# Add features to test data and predict\n",
    "last_sessions_extra = add_extra_features(last_sessions)\n",
    "X_kaggle_extra = last_sessions_extra[feature_cols_extra].fillna(0)\n",
    "\n",
    "y_kaggle_proba_extra = model_extra.predict_proba(X_kaggle_extra)[:, 1]\n",
    "create_submission(y_kaggle_proba_extra, 0.5, 'submission_extra_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4e99042a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "Best parameters: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 200}\n",
      "Best CV score: 0.8146\n",
      "\n",
      "Best Model Results:\n",
      "Balanced Accuracy: 0.7443\n",
      "ROC-AUC: 0.8215\n",
      "Threshold 0.5: 979 churners (33.7%) -> submission_best_model.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>1999455</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>1999691</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>1999720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>1999908</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>1999996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2904 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  target\n",
       "0     1000655       0\n",
       "1     1000963       1\n",
       "2     1001129       0\n",
       "3     1001963       0\n",
       "4     1002283       1\n",
       "...       ...     ...\n",
       "2899  1999455       1\n",
       "2900  1999691       1\n",
       "2901  1999720       1\n",
       "2902  1999908       1\n",
       "2903  1999996       1\n",
       "\n",
       "[2904 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RETRAIN WITH BETTER HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "}\n",
    "\n",
    "# Quick search with fewer combinations\n",
    "param_grid_quick = {\n",
    "    'max_depth': [4, 6],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "}\n",
    "\n",
    "model_cv = xgb.XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model_cv, \n",
    "    param_grid_quick, \n",
    "    cv=3, \n",
    "    scoring='roc_auc',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "y_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"\\nBest Model Results:\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba_best):.4f}\")\n",
    "\n",
    "# Predict on Kaggle test\n",
    "y_kaggle_proba_best = best_model.predict_proba(X_kaggle)[:, 1]\n",
    "create_submission(y_kaggle_proba_best, 0.5, 'submission_best_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8d180ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: ADD EXTRA FEATURES\n",
      "================================================================================\n",
      "Features: 33\n",
      "\n",
      "================================================================================\n",
      "STEP 2: FAST GRID SEARCH (only 36 combinations)\n",
      "================================================================================\n",
      "Starting grid search (36 combinations × 3 folds = 108 fits)...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "\n",
      "✅ Best parameters: {'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 150}\n",
      "✅ Best CV ROC-AUC: 0.8225\n",
      "\n",
      "================================================================================\n",
      "STEP 3: EVALUATE & CREATE SUBMISSIONS\n",
      "================================================================================\n",
      "Test ROC-AUC: 0.8275\n",
      "Test Balanced Accuracy: 0.7488\n",
      "Optimal threshold: 0.60\n",
      "\n",
      "Creating submissions:\n",
      "  sub_t03.csv: 1728 churners (59.5%)\n",
      "  sub_t04.csv: 1197 churners (41.2%)\n",
      "  sub_t05.csv: 752 churners (25.9%)\n",
      "  sub_optimal.csv: 425 churners (14.6%)\n",
      "\n",
      "✅ Done! Submit these files to Kaggle.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FAST GRID SEARCH (5-10 minutes max)\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, classification_report, confusion_matrix, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: ADD EXTRA FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def add_extra_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if 'frustration_score' in df.columns and 'is_paid' in df.columns:\n",
    "        df['frustration_x_paid'] = df['frustration_score'] * df['is_paid']\n",
    "    \n",
    "    if 'thumbs_down_lifetime' in df.columns and 'thumbs_up_lifetime' in df.columns:\n",
    "        df['thumbs_ratio'] = df['thumbs_down_lifetime'] / (df['thumbs_up_lifetime'] + 1)\n",
    "    \n",
    "    if 'songs_listened_last_14days' in df.columns and 'total_songs_listened' in df.columns:\n",
    "        df['recent_activity_ratio'] = df['songs_listened_last_14days'] / (df['total_songs_listened'] + 1)\n",
    "    \n",
    "    if 'sessions_last7d_vs_avg' in df.columns and 'activity_trend_last_14days' in df.columns:\n",
    "        df['engagement_trend'] = df['sessions_last7d_vs_avg'] * df['activity_trend_last_14days']\n",
    "    \n",
    "    if 'consecutive_days_inactive' in df.columns and 'days_without_thumbs_up' in df.columns:\n",
    "        df['inactivity_score'] = df['consecutive_days_inactive'] + df['days_without_thumbs_up'] * 0.5\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add features\n",
    "df_balanced_extra = add_extra_features(df_balanced)\n",
    "\n",
    "feature_cols_extra = [col for col in df_balanced_extra.columns \n",
    "                      if col not in ['userId', 'sessionId', 'session_time', 'will_churn_10days',\n",
    "                                    'favorite_genre', 'favorite_artist']]\n",
    "\n",
    "print(f\"Features: {len(feature_cols_extra)}\")\n",
    "\n",
    "# Prepare data\n",
    "X = df_balanced_extra[feature_cols_extra].fillna(0)\n",
    "y = df_balanced_extra['will_churn_10days']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: FAST GRID SEARCH (only 36 combinations)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# SMALL parameter grid - only 36 combinations\n",
    "param_grid = {\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [150, 200],\n",
    "    'min_child_weight': [1, 3],\n",
    "}\n",
    "\n",
    "base_model = xgb.XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # Only 3 folds (faster)\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting grid search (36 combinations × 3 folds = 108 fits)...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n✅ Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"✅ Best CV ROC-AUC: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: EVALUATE & CREATE SUBMISSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(f\"Test Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Find optimal threshold\n",
    "best_f1, best_threshold = 0, 0.5\n",
    "for t in np.arange(0.2, 0.8, 0.05):\n",
    "    f1 = f1_score(y_test, (y_proba >= t).astype(int))\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_threshold = f1, t\n",
    "\n",
    "print(f\"Optimal threshold: {best_threshold:.2f}\")\n",
    "\n",
    "# Predict on Kaggle test\n",
    "last_sessions_extra = add_extra_features(last_sessions)\n",
    "X_kaggle = last_sessions_extra[feature_cols_extra].fillna(0)\n",
    "y_kaggle_proba = best_model.predict_proba(X_kaggle)[:, 1]\n",
    "\n",
    "# Create submissions\n",
    "def create_submission(proba, threshold, filename):\n",
    "    preds = (proba >= threshold).astype(int)\n",
    "    submission = pd.DataFrame({'id': last_sessions['userId'], 'target': preds})\n",
    "    if '1261737' not in submission['id'].values:\n",
    "        submission = pd.concat([submission, pd.DataFrame({'id': ['1261737'], 'target': [0]})], ignore_index=True)\n",
    "    submission = submission.sort_values('id').reset_index(drop=True)\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"  {filename}: {preds.sum()} churners ({preds.mean():.1%})\")\n",
    "\n",
    "print(\"\\nCreating submissions:\")\n",
    "create_submission(y_kaggle_proba, 0.3, 'sub_t03.csv')\n",
    "create_submission(y_kaggle_proba, 0.4, 'sub_t04.csv')\n",
    "create_submission(y_kaggle_proba, 0.5, 'sub_t05.csv')\n",
    "create_submission(y_kaggle_proba, best_threshold, 'sub_optimal.csv')\n",
    "\n",
    "print(\"\\n✅ Done! Submit these files to Kaggle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fdbb9e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sub_t055.csv: 552 churners (19.0%)\n",
      "  sub_t06.csv: 425 churners (14.6%)\n"
     ]
    }
   ],
   "source": [
    "create_submission(y_kaggle_proba, 0.55, 'sub_t055.csv')\n",
    "create_submission(y_kaggle_proba, 0.6, 'sub_t06.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda5be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7e5a1101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_genre_encoded</th>\n",
       "      <th>favorite_artist_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10409</th>\n",
       "      <td>0.0</td>\n",
       "      <td>705.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8333</th>\n",
       "      <td>0.0</td>\n",
       "      <td>539.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17127</th>\n",
       "      <td>0.0</td>\n",
       "      <td>789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5871</th>\n",
       "      <td>0.0</td>\n",
       "      <td>526.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13193</th>\n",
       "      <td>0.0</td>\n",
       "      <td>924.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15348</th>\n",
       "      <td>0.0</td>\n",
       "      <td>551.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>0.0</td>\n",
       "      <td>601.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>0.0</td>\n",
       "      <td>601.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17588</th>\n",
       "      <td>0.0</td>\n",
       "      <td>570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13059</th>\n",
       "      <td>0.0</td>\n",
       "      <td>809.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14438 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       favorite_genre_encoded  favorite_artist_encoded\n",
       "10409                     0.0                    705.0\n",
       "8333                      0.0                    539.0\n",
       "17127                     0.0                    789.0\n",
       "5871                      0.0                    526.0\n",
       "13193                     0.0                    924.0\n",
       "...                       ...                      ...\n",
       "15348                     0.0                    551.0\n",
       "2896                      0.0                    601.0\n",
       "1108                      0.0                    601.0\n",
       "17588                     0.0                    570.0\n",
       "13059                     0.0                    809.0\n",
       "\n",
       "[14438 rows x 2 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns\n",
    "X_train[[\"favorite_genre_encoded\", \"favorite_artist_encoded\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dda367fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REMOVING BROKEN FEATURES AND RETRAINING\n",
      "================================================================================\n",
      "Original features: 33\n",
      "After removing broken: 32\n",
      "Removed: ['favorite_genre_encoded']\n",
      "\n",
      "Test ROC-AUC: 0.8258\n",
      "Test Balanced Accuracy: 0.7433\n",
      "\n",
      "Creating submissions:\n",
      "  sub_clean_t03.csv: 1437 churners (49.5%)\n",
      "  sub_clean_t04.csv: 999 churners (34.4%)\n",
      "  sub_clean_t05.csv: 651 churners (22.4%)\n",
      "\n",
      "✅ Done!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# QUICK FIX: Remove broken features and retrain\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"REMOVING BROKEN FEATURES AND RETRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Remove the broken encoded features\n",
    "broken_features = ['favorite_genre_encoded']\n",
    "\n",
    "feature_cols_clean = [col for col in feature_cols_extra if col not in broken_features]\n",
    "\n",
    "print(f\"Original features: {len(feature_cols_extra)}\")\n",
    "print(f\"After removing broken: {len(feature_cols_clean)}\")\n",
    "print(f\"Removed: {broken_features}\")\n",
    "\n",
    "# Prepare data\n",
    "X = df_balanced_extra[feature_cols_clean].fillna(0)\n",
    "y = df_balanced_extra['will_churn_10days']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "# Use best parameters from grid search (or defaults)\n",
    "best_model = xgb.XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"\\nTest ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(f\"Test Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Predict on Kaggle test\n",
    "X_kaggle = last_sessions_extra[feature_cols_clean].fillna(0)\n",
    "y_kaggle_proba = best_model.predict_proba(X_kaggle)[:, 1]\n",
    "\n",
    "# Create submissions\n",
    "def create_submission(proba, threshold, filename):\n",
    "    preds = (proba >= threshold).astype(int)\n",
    "    submission = pd.DataFrame({'id': last_sessions['userId'], 'target': preds})\n",
    "    if '1261737' not in submission['id'].values:\n",
    "        submission = pd.concat([submission, pd.DataFrame({'id': ['1261737'], 'target': [0]})], ignore_index=True)\n",
    "    submission = submission.sort_values('id').reset_index(drop=True)\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"  {filename}: {preds.sum()} churners ({preds.mean():.1%})\")\n",
    "\n",
    "print(\"\\nCreating submissions:\")\n",
    "create_submission(y_kaggle_proba, 0.3, 'sub_clean_t03.csv')\n",
    "create_submission(y_kaggle_proba, 0.4, 'sub_clean_t04.csv')\n",
    "create_submission(y_kaggle_proba, 0.5, 'sub_clean_t05.csv')\n",
    "\n",
    "print(\"\\n✅ Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "010b9deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINE-TUNING THRESHOLD AROUND 0.3\n",
      "================================================================================\n",
      "Threshold 0.15: 2283 churners (78.6%)\n",
      "Threshold 0.20: 1976 churners (68.1%)\n",
      "Threshold 0.25: 1698 churners (58.5%)\n",
      "Threshold 0.28: 1532 churners (52.8%)\n",
      "Threshold 0.30: 1437 churners (49.5%)\n",
      "Threshold 0.32: 1349 churners (46.5%)\n",
      "Threshold 0.35: 1196 churners (41.2%)\n",
      "Threshold 0.40:  999 churners (34.4%)\n",
      "\n",
      "Creating fine-tuned submissions:\n",
      "  sub_t020.csv: 1976 churners (68.1%)\n",
      "  sub_t025.csv: 1698 churners (58.5%)\n",
      "  sub_t028.csv: 1532 churners (52.8%)\n",
      "  sub_t032.csv: 1349 churners (46.5%)\n",
      "  sub_t035.csv: 1196 churners (41.2%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: FINE-TUNE THRESHOLD AROUND 0.3\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINE-TUNING THRESHOLD AROUND 0.3\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test thresholds around 0.3\n",
    "for threshold in [0.15, 0.20, 0.25, 0.28, 0.30, 0.32, 0.35, 0.40]:\n",
    "    preds = (y_kaggle_proba >= threshold).astype(int)\n",
    "    print(f\"Threshold {threshold:.2f}: {preds.sum():4d} churners ({preds.mean():.1%})\")\n",
    "\n",
    "# Create submissions at fine-tuned thresholds\n",
    "print(\"\\nCreating fine-tuned submissions:\")\n",
    "create_submission(y_kaggle_proba, 0.20, 'sub_t020.csv')\n",
    "create_submission(y_kaggle_proba, 0.25, 'sub_t025.csv')\n",
    "create_submission(y_kaggle_proba, 0.28, 'sub_t028.csv')\n",
    "create_submission(y_kaggle_proba, 0.32, 'sub_t032.csv')\n",
    "create_submission(y_kaggle_proba, 0.35, 'sub_t035.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc8ffac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "053e4b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ADDING MORE FEATURES\n",
      "================================================================================\n",
      "Previous features: 32\n",
      "New features: 45\n",
      "Added: 13 new features\n",
      "\n",
      "Model v2 Results:\n",
      "  ROC-AUC: 0.8285\n",
      "  Balanced Accuracy: 0.7413\n",
      "\n",
      "Top 20 features:\n",
      "                            feature  importance\n",
      "              recent_activity_ratio    0.094829\n",
      "                 activity_declining    0.059096\n",
      "                      has_ever_paid    0.042291\n",
      "                  consistency_score    0.039634\n",
      "               frustration_increase    0.038139\n",
      "               thumbs_down_lifetime    0.036816\n",
      "         frustration_score_lifetime    0.034379\n",
      "                  songs_outside_14d    0.029415\n",
      "listening_time_ratio_7d_vs_lifetime    0.025733\n",
      "                       thumbs_ratio    0.023910\n",
      "                            is_paid    0.023906\n",
      "                    downgrade_score    0.023688\n",
      "                 thumbs_up_lifetime    0.021921\n",
      "             sessions_last7d_vs_avg    0.021814\n",
      "            days_since_registration    0.021631\n",
      "                  frustration_score    0.021582\n",
      "           session_frequency_change    0.021059\n",
      "                     unique_artists    0.020417\n",
      "               total_songs_listened    0.019719\n",
      "         activity_trend_last_14days    0.019329\n",
      "\n",
      "Creating v2 submissions:\n",
      "  sub_v2_t025.csv: 1831 churners (63.1%)\n",
      "  sub_v2_t030.csv: 1623 churners (55.9%)\n",
      "  sub_v2_t035.csv: 1397 churners (48.1%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: ADD MORE POWERFUL FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADDING MORE FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def add_more_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # === ENGAGEMENT FEATURES ===\n",
    "    if 'thumbs_down_lifetime' in df.columns and 'thumbs_up_lifetime' in df.columns:\n",
    "        df['thumbs_ratio'] = df['thumbs_down_lifetime'] / (df['thumbs_up_lifetime'] + 1)\n",
    "        df['thumbs_diff'] = df['thumbs_down_lifetime'] - df['thumbs_up_lifetime']\n",
    "        df['total_thumbs'] = df['thumbs_down_lifetime'] + df['thumbs_up_lifetime']\n",
    "    \n",
    "    # === ACTIVITY TREND FEATURES ===\n",
    "    if 'songs_listened_last_14days' in df.columns and 'total_songs_listened' in df.columns:\n",
    "        df['recent_activity_ratio'] = df['songs_listened_last_14days'] / (df['total_songs_listened'] + 1)\n",
    "        df['songs_outside_14d'] = df['total_songs_listened'] - df['songs_listened_last_14days']\n",
    "    \n",
    "    if 'sessions_last7d_vs_avg' in df.columns:\n",
    "        df['session_decline'] = 1 - df['sessions_last7d_vs_avg']  # Higher = more decline\n",
    "        df['session_decline'] = df['session_decline'].clip(lower=0)  # Only positive decline\n",
    "    \n",
    "    if 'activity_trend_last_14days' in df.columns:\n",
    "        df['activity_declining'] = (df['activity_trend_last_14days'] < 1).astype(int)\n",
    "    \n",
    "    # === FRUSTRATION FEATURES ===\n",
    "    if 'frustration_score' in df.columns and 'is_paid' in df.columns:\n",
    "        df['frustration_x_paid'] = df['frustration_score'] * df['is_paid']\n",
    "    \n",
    "    if 'frustration_score' in df.columns and 'frustration_score_lifetime' in df.columns:\n",
    "        df['frustration_increase'] = df['frustration_score'] - (df['frustration_score_lifetime'] / 10)\n",
    "    \n",
    "    # === INACTIVITY FEATURES ===\n",
    "    if 'consecutive_days_inactive' in df.columns and 'days_without_thumbs_up' in df.columns:\n",
    "        df['inactivity_score'] = df['consecutive_days_inactive'] * 2 + df['days_without_thumbs_up']\n",
    "    \n",
    "    if 'consecutive_days_inactive' in df.columns:\n",
    "        df['very_inactive'] = (df['consecutive_days_inactive'] >= 7).astype(int)\n",
    "        df['moderately_inactive'] = (df['consecutive_days_inactive'] >= 3).astype(int)\n",
    "    \n",
    "    # === DOWNGRADE FEATURES ===\n",
    "    if 'has_downgraded' in df.columns and 'has_downgrade_last_15days' in df.columns:\n",
    "        df['downgrade_score'] = df['has_downgrade_last_15days'] * 3 + df['has_downgraded']\n",
    "    \n",
    "    if 'has_downgrade_last_15days' in df.columns and 'is_paid' in df.columns:\n",
    "        df['downgrade_from_paid'] = df['has_downgrade_last_15days'] * (1 - df['is_paid'])\n",
    "    \n",
    "    # === USAGE INTENSITY ===\n",
    "    if 'days_since_registration' in df.columns and 'total_songs_listened' in df.columns:\n",
    "        df['songs_per_day'] = df['total_songs_listened'] / (df['days_since_registration'] + 1)\n",
    "    \n",
    "    if 'days_since_registration' in df.columns and 'unique_artists' in df.columns:\n",
    "        df['artists_per_day'] = df['unique_artists'] / (df['days_since_registration'] + 1)\n",
    "    \n",
    "    # === COMBINED RISK SCORES ===\n",
    "    if all(col in df.columns for col in ['frustration_score', 'consecutive_days_inactive', 'has_downgrade_last_15days']):\n",
    "        df['churn_risk_score'] = (\n",
    "            df['frustration_score'] * 0.3 +\n",
    "            df['consecutive_days_inactive'] * 2 +\n",
    "            df['has_downgrade_last_15days'] * 10\n",
    "        )\n",
    "    \n",
    "    # === RATIOS ===\n",
    "    if 'help_visits_14d' in df.columns and 'songs_listened_last_14days' in df.columns:\n",
    "        df['help_per_song'] = df['help_visits_14d'] / (df['songs_listened_last_14days'] + 1)\n",
    "    \n",
    "    if 'error_rate_14d' in df.columns and 'frustration_score' in df.columns:\n",
    "        df['error_frustration'] = df['error_rate_14d'] * df['frustration_score']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to training data\n",
    "df_balanced_v2 = add_more_features(df_balanced)\n",
    "\n",
    "# Get new feature columns\n",
    "feature_cols_v2 = [col for col in df_balanced_v2.columns \n",
    "                   if col not in ['userId', 'sessionId', 'session_time', 'will_churn_10days',\n",
    "                                 'favorite_genre', 'favorite_artist', \n",
    "                                 'favorite_genre_encoded', 'favorite_artist_encoded']]\n",
    "\n",
    "print(f\"Previous features: {len(feature_cols_clean)}\")\n",
    "print(f\"New features: {len(feature_cols_v2)}\")\n",
    "print(f\"Added: {len(feature_cols_v2) - len(feature_cols_clean)} new features\")\n",
    "\n",
    "# Prepare data\n",
    "X = df_balanced_v2[feature_cols_v2].fillna(0)\n",
    "y = df_balanced_v2['will_churn_10days']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "# Train model\n",
    "model_v2 = xgb.XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_v2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model_v2.predict(X_test)\n",
    "y_proba = model_v2.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"\\nModel v2 Results:\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(f\"  Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Feature importance - show new features\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols_v2,\n",
    "    'importance': model_v2.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 20 features:\")\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# Predict on test\n",
    "last_sessions_v2 = add_more_features(last_sessions)\n",
    "X_kaggle_v2 = last_sessions_v2[feature_cols_v2].fillna(0)\n",
    "y_kaggle_proba_v2 = model_v2.predict_proba(X_kaggle_v2)[:, 1]\n",
    "\n",
    "# Create submissions\n",
    "print(\"\\nCreating v2 submissions:\")\n",
    "create_submission(y_kaggle_proba_v2, 0.25, 'sub_v2_t025.csv')\n",
    "create_submission(y_kaggle_proba_v2, 0.30, 'sub_v2_t030.csv')\n",
    "create_submission(y_kaggle_proba_v2, 0.35, 'sub_v2_t035.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8ead9893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sub_v2_t032.csv: 1537 churners (52.9%)\n",
      "  sub_v2_t031.csv: 1581 churners (54.5%)\n",
      "  sub_v2_t033.csv: 1492 churners (51.4%)\n"
     ]
    }
   ],
   "source": [
    "create_submission(y_kaggle_proba_v2, 0.32, 'sub_v2_t032.csv')\n",
    "create_submission(y_kaggle_proba_v2, 0.31, 'sub_v2_t031.csv')\n",
    "create_submission(y_kaggle_proba_v2, 0.33, 'sub_v2_t033.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "737eca29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRYING RANDOM FOREST\n",
      "================================================================================\n",
      "Random Forest ROC-AUC: 0.8073\n",
      "\n",
      "Creating RF submissions:\n",
      "  sub_rf_t025.csv: 2172 churners (74.8%)\n",
      "  sub_rf_t030.csv: 1812 churners (62.4%)\n",
      "  sub_rf_t035.csv: 1448 churners (49.9%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: TRY RANDOM FOREST (sometimes better than XGBoost)\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRYING RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "print(f\"Random Forest ROC-AUC: {roc_auc_score(y_test, y_proba_rf):.4f}\")\n",
    "\n",
    "# Predict on test\n",
    "y_kaggle_proba_rf = rf_model.predict_proba(X_kaggle_v2)[:, 1]\n",
    "\n",
    "print(\"\\nCreating RF submissions:\")\n",
    "create_submission(y_kaggle_proba_rf, 0.25, 'sub_rf_t025.csv')\n",
    "create_submission(y_kaggle_proba_rf, 0.30, 'sub_rf_t030.csv')\n",
    "create_submission(y_kaggle_proba_rf, 0.35, 'sub_rf_t035.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8186b73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENSEMBLE: XGBoost + Random Forest\n",
      "================================================================================\n",
      "Creating ensemble submissions:\n",
      "  sub_ensemble_t025.csv: 2024 churners (69.7%)\n",
      "  sub_ensemble_t030.csv: 1708 churners (58.8%)\n",
      "  sub_ensemble_t035.csv: 1419 churners (48.9%)\n",
      "\n",
      "================================================================================\n",
      "✅ DONE! Files to try on Kaggle:\n",
      "================================================================================\n",
      "\n",
      "1. Fine-tuned thresholds (current model):\n",
      "   - sub_t025.csv, sub_t028.csv, sub_t032.csv\n",
      "\n",
      "2. New features (v2 model):\n",
      "   - sub_v2_t025.csv, sub_v2_t030.csv, sub_v2_t035.csv\n",
      "\n",
      "3. Random Forest:\n",
      "   - sub_rf_t025.csv, sub_rf_t030.csv, sub_rf_t035.csv\n",
      "\n",
      "4. Ensemble (XGB + RF):\n",
      "   - sub_ensemble_t025.csv, sub_ensemble_t030.csv, sub_ensemble_t035.csv\n",
      "\n",
      "Start with the ensemble submissions - they often work best!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: ENSEMBLE - AVERAGE PREDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE: XGBoost + Random Forest\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Average probabilities\n",
    "y_kaggle_proba_ensemble = (y_kaggle_proba_v2 + y_kaggle_proba_rf) / 2\n",
    "\n",
    "print(\"Creating ensemble submissions:\")\n",
    "create_submission(y_kaggle_proba_ensemble, 0.25, 'sub_ensemble_t025.csv')\n",
    "create_submission(y_kaggle_proba_ensemble, 0.30, 'sub_ensemble_t030.csv')\n",
    "create_submission(y_kaggle_proba_ensemble, 0.35, 'sub_ensemble_t035.csv')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ DONE! Files to try on Kaggle:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. Fine-tuned thresholds (current model):\n",
    "   - sub_t025.csv, sub_t028.csv, sub_t032.csv\n",
    "\n",
    "2. New features (v2 model):\n",
    "   - sub_v2_t025.csv, sub_v2_t030.csv, sub_v2_t035.csv\n",
    "\n",
    "3. Random Forest:\n",
    "   - sub_rf_t025.csv, sub_rf_t030.csv, sub_rf_t035.csv\n",
    "\n",
    "4. Ensemble (XGB + RF):\n",
    "   - sub_ensemble_t025.csv, sub_ensemble_t030.csv, sub_ensemble_t035.csv\n",
    "\n",
    "Start with the ensemble submissions - they often work best!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c5cafdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NEW APPROACH: USER-LEVEL AGGREGATION\n",
      "================================================================================\n",
      "Creating user-level features for training...\n",
      "User features shape: (18048, 108)\n",
      "Creating user-level features for test...\n",
      "Test user features shape: (2903, 107)\n",
      "Common features: 106\n",
      "\n",
      "User-level model ROC-AUC: 0.8195\n",
      "\n",
      "Creating user-level submissions:\n",
      "  sub_user_t25.csv: 457 churners (15.7%)\n",
      "  sub_user_t30.csv: 323 churners (11.1%)\n",
      "  sub_user_t35.csv: 231 churners (8.0%)\n",
      "  sub_user_t40.csv: 141 churners (4.9%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# NEW APPROACH: AGGREGATE ALL SESSIONS PER USER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEW APPROACH: USER-LEVEL AGGREGATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_user_level_features(df):\n",
    "    \"\"\"Aggregate all sessions into user-level features\"\"\"\n",
    "    \n",
    "    # Columns to aggregate\n",
    "    numeric_cols = [col for col in df.columns if col not in \n",
    "                    ['userId', 'sessionId', 'session_time', 'will_churn_10days',\n",
    "                     'favorite_genre', 'favorite_artist', \n",
    "                     'favorite_genre_encoded', 'favorite_artist_encoded']]\n",
    "    \n",
    "    # Aggregations\n",
    "    agg_dict = {}\n",
    "    for col in numeric_cols:\n",
    "        agg_dict[col] = ['mean', 'max', 'min', 'last']\n",
    "    \n",
    "    # Group by user\n",
    "    user_features = df.groupby('userId').agg(agg_dict)\n",
    "    user_features.columns = ['_'.join(col) for col in user_features.columns]\n",
    "    user_features = user_features.reset_index()\n",
    "    \n",
    "    # Add count of sessions\n",
    "    session_counts = df.groupby('userId').size().reset_index(name='n_sessions')\n",
    "    user_features = user_features.merge(session_counts, on='userId')\n",
    "    \n",
    "    # Add time-based features\n",
    "    time_features = df.groupby('userId').agg({\n",
    "        'session_time': ['min', 'max']\n",
    "    })\n",
    "    time_features.columns = ['first_session', 'last_session']\n",
    "    time_features['active_days'] = (time_features['last_session'] - time_features['first_session']).dt.days\n",
    "    time_features = time_features.reset_index()[['userId', 'active_days']]\n",
    "    \n",
    "    user_features = user_features.merge(time_features, on='userId')\n",
    "    \n",
    "    # Add target if exists\n",
    "    if 'will_churn_10days' in df.columns:\n",
    "        target = df.groupby('userId')['will_churn_10days'].max().reset_index()\n",
    "        user_features = user_features.merge(target, on='userId')\n",
    "    \n",
    "    return user_features\n",
    "\n",
    "# Create user-level features for training\n",
    "print(\"Creating user-level features for training...\")\n",
    "user_features_train = create_user_level_features(df_balanced)\n",
    "print(f\"User features shape: {user_features_train.shape}\")\n",
    "\n",
    "# Create user-level features for test\n",
    "print(\"Creating user-level features for test...\")\n",
    "user_features_test = create_user_level_features(features_test)\n",
    "print(f\"Test user features shape: {user_features_test.shape}\")\n",
    "\n",
    "# Prepare for modeling\n",
    "feature_cols_user = [col for col in user_features_train.columns \n",
    "                     if col not in ['userId', 'will_churn_10days']]\n",
    "\n",
    "# Align columns\n",
    "common_cols = list(set(feature_cols_user) & set(user_features_test.columns))\n",
    "print(f\"Common features: {len(common_cols)}\")\n",
    "\n",
    "X_user = user_features_train[common_cols].fillna(0)\n",
    "y_user = user_features_train['will_churn_10days']\n",
    "\n",
    "X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(\n",
    "    X_user, y_user, test_size=0.2, random_state=42, stratify=y_user\n",
    ")\n",
    "\n",
    "# Train\n",
    "scale_pos_weight = (y_train_u == 0).sum() / (y_train_u == 1).sum()\n",
    "\n",
    "model_user = xgb.XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_user.fit(X_train_u, y_train_u)\n",
    "\n",
    "# Evaluate\n",
    "y_proba_u = model_user.predict_proba(X_test_u)[:, 1]\n",
    "print(f\"\\nUser-level model ROC-AUC: {roc_auc_score(y_test_u, y_proba_u):.4f}\")\n",
    "\n",
    "# Predict on test\n",
    "X_kaggle_user = user_features_test[common_cols].fillna(0)\n",
    "y_kaggle_proba_user = model_user.predict_proba(X_kaggle_user)[:, 1]\n",
    "\n",
    "# Create submissions\n",
    "print(\"\\nCreating user-level submissions:\")\n",
    "for t in [0.25, 0.30, 0.35, 0.40]:\n",
    "    preds = (y_kaggle_proba_user >= t).astype(int)\n",
    "    submission = pd.DataFrame({'id': user_features_test['userId'], 'target': preds})\n",
    "    if '1261737' not in submission['id'].values:\n",
    "        submission = pd.concat([submission, pd.DataFrame({'id': ['1261737'], 'target': [0]})], ignore_index=True)\n",
    "    submission = submission.sort_values('id').reset_index(drop=True)\n",
    "    filename = f'sub_user_t{int(t*100):02d}.csv'\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"  {filename}: {preds.sum()} churners ({preds.mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "416195c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRYING LOGISTIC REGRESSION\n",
      "================================================================================\n",
      "Logistic Regression ROC-AUC: 0.8159\n",
      "\n",
      "Creating LR submissions:\n",
      "  sub_lr_t25.csv: 1256 churners (43.3%)\n",
      "  sub_lr_t30.csv: 907 churners (31.2%)\n",
      "  sub_lr_t35.csv: 579 churners (19.9%)\n",
      "  sub_lr_t40.csv: 351 churners (12.1%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRY LOGISTIC REGRESSION\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRYING LOGISTIC REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_u)\n",
    "X_test_scaled = scaler.transform(X_test_u)\n",
    "\n",
    "# Train\n",
    "lr_model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train_scaled, y_train_u)\n",
    "\n",
    "y_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "print(f\"Logistic Regression ROC-AUC: {roc_auc_score(y_test_u, y_proba_lr):.4f}\")\n",
    "\n",
    "# Predict on test\n",
    "X_kaggle_scaled = scaler.transform(X_kaggle_user)\n",
    "y_kaggle_proba_lr = lr_model.predict_proba(X_kaggle_scaled)[:, 1]\n",
    "\n",
    "# Create submissions\n",
    "print(\"\\nCreating LR submissions:\")\n",
    "for t in [0.25, 0.30, 0.35, 0.40]:\n",
    "    preds = (y_kaggle_proba_lr >= t).astype(int)\n",
    "    submission = pd.DataFrame({'id': user_features_test['userId'], 'target': preds})\n",
    "    if '1261737' not in submission['id'].values:\n",
    "        submission = pd.concat([submission, pd.DataFrame({'id': ['1261737'], 'target': [0]})], ignore_index=True)\n",
    "    submission = submission.sort_values('id').reset_index(drop=True)\n",
    "    filename = f'sub_lr_t{int(t*100):02d}.csv'\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"  {filename}: {preds.sum()} churners ({preds.mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "71630891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINDING SINGLE BEST PREDICTOR\n",
      "================================================================================\n",
      "Top 10 individual features by AUC:\n",
      "                       feature       auc\n",
      "29       recent_activity_ratio  0.731926\n",
      "34        frustration_increase  0.693611\n",
      "13           frustration_score  0.689521\n",
      "5      thumbs_down_last_14days  0.687801\n",
      "9   songs_listened_last_14days  0.684373\n",
      "33          frustration_x_paid  0.651499\n",
      "12         settings_visits_14d  0.650116\n",
      "19        thumbs_down_lifetime  0.645787\n",
      "10             help_visits_14d  0.641773\n",
      "21  frustration_score_lifetime  0.637198\n",
      "\n",
      "Bottom 10 (might be inversely predictive):\n",
      "                    feature       auc\n",
      "16   mobile_usage_ratio_14d  0.475605\n",
      "37      moderately_inactive  0.474024\n",
      "32       activity_declining  0.473692\n",
      "30        songs_outside_14d  0.458458\n",
      "36            very_inactive  0.451509\n",
      "31          session_decline  0.451042\n",
      "17  days_since_registration  0.427443\n",
      "27              thumbs_diff  0.415132\n",
      "35         inactivity_score  0.358478\n",
      "6    days_without_thumbs_up  0.351032\n",
      "\n",
      "Best single feature: recent_activity_ratio\n",
      "Created sub_single_feature.csv: 1452 churners\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 4: FIND THE SINGLE BEST FEATURE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINDING SINGLE BEST PREDICTOR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Test each feature individually\n",
    "feature_aucs = []\n",
    "\n",
    "for col in feature_cols_v2:\n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, X_test[col])\n",
    "        feature_aucs.append({'feature': col, 'auc': auc})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "feature_aucs_df = pd.DataFrame(feature_aucs).sort_values('auc', ascending=False)\n",
    "print(\"Top 10 individual features by AUC:\")\n",
    "print(feature_aucs_df.head(10))\n",
    "\n",
    "print(\"\\nBottom 10 (might be inversely predictive):\")\n",
    "print(feature_aucs_df.tail(10))\n",
    "\n",
    "# Try using just the best feature\n",
    "best_feature = feature_aucs_df.iloc[0]['feature']\n",
    "print(f\"\\nBest single feature: {best_feature}\")\n",
    "\n",
    "# Create submission using just this feature\n",
    "best_feat_values = last_sessions_v2[best_feature].fillna(0)\n",
    "threshold_bf = best_feat_values.median()\n",
    "\n",
    "preds_bf = (best_feat_values >= threshold_bf).astype(int)\n",
    "submission = pd.DataFrame({'id': last_sessions['userId'], 'target': preds_bf})\n",
    "if '1261737' not in submission['id'].values:\n",
    "    submission = pd.concat([submission, pd.DataFrame({'id': ['1261737'], 'target': [0]})], ignore_index=True)\n",
    "submission = submission.sort_values('id').reset_index(drop=True)\n",
    "submission.to_csv('sub_single_feature.csv', index=False)\n",
    "print(f\"Created sub_single_feature.csv: {preds_bf.sum()} churners\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c9d9be52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRYING DIFFERENT TARGET CHURN RATES\n",
      "================================================================================\n",
      "  sub_rate_55.csv: 1597 churners (55.0%)\n",
      "  sub_rate_57.csv: 1684 churners (58.0%)\n",
      "  sub_rate_64.csv: 1858 churners (64.0%)\n",
      "  sub_rate_68.csv: 1974 churners (68.0%)\n",
      "  sub_rate_72.csv: 2090 churners (72.0%)\n",
      "\n",
      "✅ Submit sub_rate_10.csv, sub_rate_20.csv, sub_rate_30.csv to Kaggle\n",
      "This will help us understand what churn rate Kaggle expects!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 5: TRY DIFFERENT CHURN RATES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRYING DIFFERENT TARGET CHURN RATES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# The true churn rate in test might be different from what we think\n",
    "# Let's create submissions with different churn rates\n",
    "\n",
    "target_rates = [0.55, 0.58, 0.64, 0.68, 0.72]\n",
    "\n",
    "for target_rate in target_rates:\n",
    "    # Find threshold that gives this churn rate\n",
    "    threshold = np.percentile(y_kaggle_proba_v2, 100 * (1 - target_rate))\n",
    "    preds = (y_kaggle_proba_v2 >= threshold).astype(int)\n",
    "    actual_rate = preds.mean()\n",
    "    \n",
    "    submission = pd.DataFrame({'id': last_sessions['userId'], 'target': preds})\n",
    "    if '1261737' not in submission['id'].values:\n",
    "        submission = pd.concat([submission, pd.DataFrame({'id': ['1261737'], 'target': [0]})], ignore_index=True)\n",
    "    submission = submission.sort_values('id').reset_index(drop=True)\n",
    "    \n",
    "    filename = f'sub_rate_{int(target_rate*100):02d}.csv'\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"  {filename}: {preds.sum()} churners ({actual_rate:.1%})\")\n",
    "\n",
    "print(\"\\n✅ Submit sub_rate_10.csv, sub_rate_20.csv, sub_rate_30.csv to Kaggle\")\n",
    "print(\"This will help us understand what churn rate Kaggle expects!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b99637f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINE-TUNING AROUND 57% CHURN RATE\n",
      "================================================================================\n",
      "  sub_rate_50.csv: 1452 churners (50.0%), threshold=0.339\n",
      "  sub_rate_52.csv: 1510 churners (52.0%), threshold=0.325\n",
      "  sub_rate_54.csv: 1568 churners (54.0%), threshold=0.313\n",
      "  sub_rate_55.csv: 1597 churners (55.0%), threshold=0.306\n",
      "  sub_rate_56.csv: 1626 churners (56.0%), threshold=0.299\n",
      "  sub_rate_56.csv: 1655 churners (57.0%), threshold=0.293\n",
      "  sub_rate_57.csv: 1684 churners (58.0%), threshold=0.285\n",
      "  sub_rate_59.csv: 1713 churners (59.0%), threshold=0.279\n",
      "  sub_rate_60.csv: 1742 churners (60.0%), threshold=0.270\n",
      "  sub_rate_62.csv: 1800 churners (62.0%), threshold=0.257\n",
      "  sub_rate_65.csv: 1887 churners (65.0%), threshold=0.237\n",
      "\n",
      "✅ Submit sub_rate_55.csv, sub_rate_56.csv, sub_rate_58.csv, sub_rate_59.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINE-TUNE AROUND 57% CHURN RATE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINE-TUNING AROUND 57% CHURN RATE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find thresholds that give churn rates around 57%\n",
    "target_rates = [0.50, 0.52, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.62, 0.65]\n",
    "\n",
    "for target_rate in target_rates:\n",
    "    threshold = np.percentile(y_kaggle_proba_v2, 100 * (1 - target_rate))\n",
    "    preds = (y_kaggle_proba_v2 >= threshold).astype(int)\n",
    "    actual_rate = preds.mean()\n",
    "    \n",
    "    submission = pd.DataFrame({'id': last_sessions['userId'], 'target': preds})\n",
    "    if '1261737' not in submission['id'].values:\n",
    "        submission = pd.concat([submission, pd.DataFrame({'id': ['1261737'], 'target': [0]})], ignore_index=True)\n",
    "    submission = submission.sort_values('id').reset_index(drop=True)\n",
    "    \n",
    "    filename = f'sub_rate_{int(target_rate*100):02d}.csv'\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"  {filename}: {preds.sum()} churners ({actual_rate:.1%}), threshold={threshold:.3f}\")\n",
    "\n",
    "print(\"\\n✅ Submit sub_rate_55.csv, sub_rate_56.csv, sub_rate_58.csv, sub_rate_59.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "44d20bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZING FOR BALANCED ACCURACY\n",
      "================================================================================\n",
      "  scale_pos_weight=0.5: balanced_accuracy=0.6564\n",
      "  scale_pos_weight=1.0: balanced_accuracy=0.6763\n",
      "  scale_pos_weight=1.5: balanced_accuracy=0.6935\n",
      "  scale_pos_weight=2.0: balanced_accuracy=0.7063\n",
      "  scale_pos_weight=3.0: balanced_accuracy=0.7189\n",
      "  scale_pos_weight=4.0: balanced_accuracy=0.7293\n",
      "  scale_pos_weight=5.0: balanced_accuracy=0.7323\n",
      "  scale_pos_weight=7.0: balanced_accuracy=0.7335\n",
      "  scale_pos_weight=10.0: balanced_accuracy=0.7335\n",
      "\n",
      "✅ Best weight: 7.0 (balanced_accuracy=0.7335)\n",
      "\n",
      "Creating submissions with balanced model:\n",
      "  sub_bal_55.csv: 1597 churners (55.0%)\n",
      "  sub_bal_56.csv: 1655 churners (57.0%)\n",
      "  sub_bal_59.csv: 1713 churners (59.0%)\n",
      "  sub_bal_60.csv: 1742 churners (60.0%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RETRAIN MODEL OPTIMIZING FOR BALANCED ACCURACY\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZING FOR BALANCED ACCURACY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Custom scorer\n",
    "bal_acc_scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "# Try different scale_pos_weight values\n",
    "# This affects how much the model cares about each class\n",
    "\n",
    "best_bal_acc = 0\n",
    "best_weight = 1\n",
    "\n",
    "for weight in [0.5, 1.0, 1.5, 2.0, 3.0, 4.0, 5.0, 7.0, 10.0]:\n",
    "    model_test = xgb.XGBClassifier(\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=200,\n",
    "        scale_pos_weight=weight,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Cross-validation score\n",
    "    scores = cross_val_score(model_test, X_train, y_train, cv=3, scoring=bal_acc_scorer)\n",
    "    mean_score = scores.mean()\n",
    "    \n",
    "    if mean_score > best_bal_acc:\n",
    "        best_bal_acc = mean_score\n",
    "        best_weight = weight\n",
    "    \n",
    "    print(f\"  scale_pos_weight={weight}: balanced_accuracy={mean_score:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Best weight: {best_weight} (balanced_accuracy={best_bal_acc:.4f})\")\n",
    "\n",
    "# Retrain with best weight\n",
    "model_balanced = xgb.XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    scale_pos_weight=best_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_balanced.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_kaggle_proba_bal = model_balanced.predict_proba(X_kaggle_v2)[:, 1]\n",
    "\n",
    "# Create submissions around 57% rate\n",
    "print(\"\\nCreating submissions with balanced model:\")\n",
    "for target_rate in [0.55, 0.57, 0.59, 0.60]:\n",
    "    threshold = np.percentile(y_kaggle_proba_bal, 100 * (1 - target_rate))\n",
    "    preds = (y_kaggle_proba_bal >= threshold).astype(int)\n",
    "    \n",
    "    submission = pd.DataFrame({'id': last_sessions['userId'], 'target': preds})\n",
    "    if '1261737' not in submission['id'].values:\n",
    "        submission = pd.concat([submission, pd.DataFrame({'id': ['1261737'], 'target': [0]})], ignore_index=True)\n",
    "    submission = submission.sort_values('id').reset_index(drop=True)\n",
    "    \n",
    "    filename = f'sub_bal_{int(target_rate*100):02d}.csv'\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"  {filename}: {preds.sum()} churners ({preds.mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6533c2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CALIBRATING PROBABILITIES\n",
      "================================================================================\n",
      "Calibrated probability distribution:\n",
      "count    2903.000000\n",
      "mean        0.183805\n",
      "std         0.133782\n",
      "min         0.005769\n",
      "25%         0.088131\n",
      "50%         0.148590\n",
      "75%         0.232853\n",
      "max         0.955740\n",
      "dtype: float64\n",
      "\n",
      "Creating calibrated submissions:\n",
      "  sub_cal_55.csv: 1597 churners (55.0%)\n",
      "  sub_cal_56.csv: 1655 churners (57.0%)\n",
      "  sub_cal_59.csv: 1714 churners (59.0%)\n",
      "  sub_cal_60.csv: 1743 churners (60.0%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CALIBRATE PROBABILITIES\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALIBRATING PROBABILITIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calibrate the model\n",
    "model_calibrated = CalibratedClassifierCV(model_balanced, method='isotonic', cv=3)\n",
    "model_calibrated.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_kaggle_proba_cal = model_calibrated.predict_proba(X_kaggle_v2)[:, 1]\n",
    "\n",
    "print(\"Calibrated probability distribution:\")\n",
    "print(pd.Series(y_kaggle_proba_cal).describe())\n",
    "\n",
    "# Create submissions\n",
    "print(\"\\nCreating calibrated submissions:\")\n",
    "for target_rate in [0.55, 0.57, 0.59, 0.60]:\n",
    "    threshold = np.percentile(y_kaggle_proba_cal, 100 * (1 - target_rate))\n",
    "    preds = (y_kaggle_proba_cal >= threshold).astype(int)\n",
    "    \n",
    "    submission = pd.DataFrame({'id': last_sessions['userId'], 'target': preds})\n",
    "    if '1261737' not in submission['id'].values:\n",
    "        submission = pd.concat([submission, pd.DataFrame({'id': ['1261737'], 'target': [0]})], ignore_index=True)\n",
    "    submission = submission.sort_values('id').reset_index(drop=True)\n",
    "    \n",
    "    filename = f'sub_cal_{int(target_rate*100):02d}.csv'\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"  {filename}: {preds.sum()} churners ({preds.mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "69afefba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARING TRAIN VS TEST USERS\n",
      "================================================================================\n",
      "\n",
      "Feature comparison (mean values):\n",
      "Feature                                  Train       Test       Diff\n",
      "-----------------------------------------------------------------\n",
      "frustration_score                        12.32      14.84      +2.53\n",
      "consecutive_days_inactive                 3.43       3.61      +0.17\n",
      "has_downgrade_last_15days                 0.45       0.54      +0.09\n",
      "thumbs_down_lifetime                      6.75      11.32      +4.58\n",
      "days_since_registration                  67.98      78.36     +10.37\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHECK: Are test users different from train users?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARING TRAIN VS TEST USERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare key features\n",
    "key_features = ['frustration_score', 'consecutive_days_inactive', 'has_downgrade_last_15days', \n",
    "                'thumbs_down_lifetime', 'days_since_registration']\n",
    "\n",
    "print(\"\\nFeature comparison (mean values):\")\n",
    "print(f\"{'Feature':<35} {'Train':>10} {'Test':>10} {'Diff':>10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for feat in key_features:\n",
    "    if feat in df_balanced.columns and feat in last_sessions_v2.columns:\n",
    "        train_mean = df_balanced[feat].mean()\n",
    "        test_mean = last_sessions_v2[feat].mean()\n",
    "        diff = test_mean - train_mean\n",
    "        print(f\"{feat:<35} {train_mean:>10.2f} {test_mean:>10.2f} {diff:>+10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8fa289d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADJUSTING FOR DISTRIBUTION SHIFT\n",
      "================================================================================\n",
      "\n",
      "Churner vs Non-Churner profiles (Training):\n",
      "Feature                                Non-Churn        Churn         Test\n",
      "---------------------------------------------------------------------------\n",
      "frustration_score                          10.29        20.94        14.84  -> NON-CHURN\n",
      "consecutive_days_inactive                   3.55         2.95         3.61  -> NON-CHURN\n",
      "has_downgrade_last_15days                   0.40         0.64         0.54  -> CHURN\n",
      "thumbs_down_lifetime                        5.95        10.14        11.32  -> CHURN\n",
      "days_since_registration                    68.99        63.69        78.36  -> NON-CHURN\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY: ACCOUNT FOR DISTRIBUTION SHIFT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ADJUSTING FOR DISTRIBUTION SHIFT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# The test users are more \"churn-like\" on average\n",
    "# Let's see what % of TEST users look like CHURNERS from training\n",
    "\n",
    "# Get churner vs non-churner profiles from training\n",
    "train_churners = df_balanced[df_balanced['will_churn_10days'] == 1]\n",
    "train_non_churners = df_balanced[df_balanced['will_churn_10days'] == 0]\n",
    "\n",
    "key_features = ['frustration_score', 'consecutive_days_inactive', 'has_downgrade_last_15days', \n",
    "                'thumbs_down_lifetime', 'days_since_registration']\n",
    "\n",
    "print(\"\\nChurner vs Non-Churner profiles (Training):\")\n",
    "print(f\"{'Feature':<35} {'Non-Churn':>12} {'Churn':>12} {'Test':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for feat in key_features:\n",
    "    nc_mean = train_non_churners[feat].mean()\n",
    "    c_mean = train_churners[feat].mean()\n",
    "    test_mean = last_sessions_v2[feat].mean()\n",
    "    \n",
    "    # Is test closer to churners or non-churners?\n",
    "    closer = \"CHURN\" if abs(test_mean - c_mean) < abs(test_mean - nc_mean) else \"NON-CHURN\"\n",
    "    \n",
    "    print(f\"{feat:<35} {nc_mean:>12.2f} {c_mean:>12.2f} {test_mean:>12.2f}  -> {closer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8c6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f007b9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RULE-BASED PREDICTION\n",
      "================================================================================\n",
      "Rule score distribution:\n",
      "rule_score\n",
      "0    175\n",
      "1    154\n",
      "2    163\n",
      "3    371\n",
      "4    228\n",
      "5    163\n",
      "6    250\n",
      "7    127\n",
      "8    647\n",
      "9    625\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Predictions at different rule thresholds:\n",
      "  Score >= 0: 2903 churners (100.0%)\n",
      "  Score >= 1: 2728 churners (94.0%)\n",
      "  Score >= 2: 2574 churners (88.7%)\n",
      "  Score >= 3: 2411 churners (83.1%)\n",
      "  Score >= 4: 2040 churners (70.3%)\n",
      "  Score >= 5: 1812 churners (62.4%)\n",
      "  Score >= 6: 1649 churners (56.8%)\n",
      "  Score >= 7: 1399 churners (48.2%)\n",
      "\n",
      "Best rule threshold for ~57% churn: 6\n",
      "\n",
      "Created sub_rules.csv: 1649 churners (56.8%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SIMPLE RULE-BASED APPROACH\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RULE-BASED PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Based on training data, find thresholds that separate churners\n",
    "# Then apply to test\n",
    "\n",
    "test_data = last_sessions_v2.copy()\n",
    "\n",
    "# Calculate churn score based on rules\n",
    "test_data['rule_score'] = 0\n",
    "\n",
    "# Rule 1: High frustration\n",
    "frustration_threshold = train_churners['frustration_score'].quantile(0.25)\n",
    "test_data['rule_score'] += (test_data['frustration_score'] >= frustration_threshold).astype(int) * 2\n",
    "\n",
    "# Rule 2: Inactive days\n",
    "inactive_threshold = train_churners['consecutive_days_inactive'].quantile(0.25)\n",
    "test_data['rule_score'] += (test_data['consecutive_days_inactive'] >= inactive_threshold).astype(int) * 2\n",
    "\n",
    "# Rule 3: Has downgraded recently\n",
    "test_data['rule_score'] += test_data['has_downgrade_last_15days'] * 3\n",
    "\n",
    "# Rule 4: High thumbs down\n",
    "thumbs_threshold = train_churners['thumbs_down_lifetime'].quantile(0.25)\n",
    "test_data['rule_score'] += (test_data['thumbs_down_lifetime'] >= thumbs_threshold).astype(int)\n",
    "\n",
    "# Rule 5: Low activity trend\n",
    "if 'activity_trend_last_14days' in test_data.columns:\n",
    "    test_data['rule_score'] += (test_data['activity_trend_last_14days'] < 1).astype(int)\n",
    "\n",
    "print(f\"Rule score distribution:\")\n",
    "print(test_data['rule_score'].value_counts().sort_index())\n",
    "\n",
    "# Try different rule thresholds\n",
    "print(\"\\nPredictions at different rule thresholds:\")\n",
    "for rule_thresh in range(0, 8):\n",
    "    preds = (test_data['rule_score'] >= rule_thresh).astype(int)\n",
    "    print(f\"  Score >= {rule_thresh}: {preds.sum()} churners ({preds.mean():.1%})\")\n",
    "\n",
    "# Find threshold closest to 57%\n",
    "best_thresh = 0\n",
    "best_diff = 1.0\n",
    "for rule_thresh in range(0, 8):\n",
    "    preds = (test_data['rule_score'] >= rule_thresh).astype(int)\n",
    "    diff = abs(preds.mean() - 0.57)\n",
    "    if diff < best_diff:\n",
    "        best_diff = diff\n",
    "        best_thresh = rule_thresh\n",
    "\n",
    "print(f\"\\nBest rule threshold for ~57% churn: {best_thresh}\")\n",
    "\n",
    "# Create submission\n",
    "preds_rule = (test_data['rule_score'] >= best_thresh).astype(int)\n",
    "submission = pd.DataFrame({'id': last_sessions['userId'], 'target': preds_rule})\n",
    "if '1261737' not in submission['id'].values:\n",
    "    submission = pd.concat([submission, pd.DataFrame({'id': ['1261737'], 'target': [0]})], ignore_index=True)\n",
    "submission = submission.sort_values('id').reset_index(drop=True)\n",
    "submission.to_csv('sub_rules.csv', index=False)\n",
    "print(f\"\\nCreated sub_rules.csv: {preds_rule.sum()} churners ({preds_rule.mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0bdc6275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMBINING MODEL + RULES\n",
      "================================================================================\n",
      "Combined score distribution:\n",
      "count    2903.000000\n",
      "mean        0.500478\n",
      "std         0.230838\n",
      "min         0.009014\n",
      "25%         0.304322\n",
      "50%         0.532007\n",
      "75%         0.683763\n",
      "max         0.974898\n",
      "Name: rule_score, dtype: float64\n",
      "  sub_combined_55.csv: 1597 churners (55.0%)\n",
      "  sub_combined_56.csv: 1655 churners (57.0%)\n",
      "  sub_combined_59.csv: 1713 churners (59.0%)\n",
      "  sub_combined_60.csv: 1742 churners (60.0%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENSEMBLE: MODEL + RULES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMBINING MODEL + RULES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Normalize both scores to 0-1\n",
    "model_scores = y_kaggle_proba_v2\n",
    "rule_scores = test_data['rule_score'] / test_data['rule_score'].max()\n",
    "\n",
    "# Combine\n",
    "combined_scores = (model_scores + rule_scores) / 2\n",
    "\n",
    "print(\"Combined score distribution:\")\n",
    "print(pd.Series(combined_scores).describe())\n",
    "\n",
    "# Create submissions at different rates\n",
    "for target_rate in [0.55, 0.57, 0.59, 0.60]:\n",
    "    threshold = np.percentile(combined_scores, 100 * (1 - target_rate))\n",
    "    preds = (combined_scores >= threshold).astype(int)\n",
    "    \n",
    "    submission = pd.DataFrame({'id': last_sessions['userId'], 'target': preds})\n",
    "    if '1261737' not in submission['id'].values:\n",
    "        submission = pd.concat([submission, pd.DataFrame({'id': ['1261737'], 'target': [0]})], ignore_index=True)\n",
    "    submission = submission.sort_values('id').reset_index(drop=True)\n",
    "    \n",
    "    filename = f'sub_combined_{int(target_rate*100):02d}.csv'\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"  {filename}: {preds.sum()} churners ({preds.mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "726b369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING WITH HIGHER CHURN WEIGHT\n",
      "================================================================================\n",
      "  weight=0.3: default churn rate = 2.0%\n",
      "  weight=0.5: default churn rate = 3.0%\n",
      "  weight=0.7: default churn rate = 4.7%\n",
      "  weight=1.0: default churn rate = 7.0%\n",
      "\n",
      "✅ Try: sub_combined_57.csv, sub_rules.csv, sub_weight_05.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RETRAIN WITH ADJUSTED CLASS WEIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING WITH HIGHER CHURN WEIGHT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Since test has ~57% churners but training has ~19%, \n",
    "# we should weight churners MORE heavily\n",
    "\n",
    "# Try very high weights for churners\n",
    "for weight in [0.3, 0.5, 0.7, 1.0]:\n",
    "    model_adj = xgb.XGBClassifier(\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=200,\n",
    "        scale_pos_weight=weight,  # LOWER weight = predict MORE churns\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model_adj.fit(X_train, y_train)\n",
    "    y_proba_adj = model_adj.predict_proba(X_kaggle_v2)[:, 1]\n",
    "    \n",
    "    # What churn rate does default threshold give?\n",
    "    default_preds = (y_proba_adj >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"  weight={weight}: default churn rate = {default_preds.mean():.1%}\")\n",
    "    \n",
    "    # Create submission at 57% rate\n",
    "    threshold = np.percentile(y_proba_adj, 100 * (1 - 0.57))\n",
    "    preds = (y_proba_adj >= threshold).astype(int)\n",
    "    \n",
    "    submission = pd.DataFrame({'id': last_sessions['userId'], 'target': preds})\n",
    "    if '1261737' not in submission['id'].values:\n",
    "        submission = pd.concat([submission, pd.DataFrame({'id': ['1261737'], 'target': [0]})], ignore_index=True)\n",
    "    submission = submission.sort_values('id').reset_index(drop=True)\n",
    "    \n",
    "    filename = f'sub_weight_{str(weight).replace(\".\", \"\")}.csv'\n",
    "    submission.to_csv(filename, index=False)\n",
    "\n",
    "print(\"\\n✅ Try: sub_combined_57.csv, sub_rules.csv, sub_weight_05.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147c1e2",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c1660",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfa238d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_parquet(\"data/test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b626b49",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38549e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is an outlier, so we remove it and will put arbitrarily a value to it later\n",
    "data_clean = df_test[df_test['userId'] != '1261737']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7d71659",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'submission' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m new_row = pd.DataFrame({\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33muserId\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33m1261737\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mwill_churn_prediction\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0\u001b[39m]\n\u001b[32m      5\u001b[39m })\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Ajouter à ton submission\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m submission_complete = pd.concat([\u001b[43msubmission\u001b[49m, new_row], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSubmission before: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(submission)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSubmission after: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(submission_complete)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'submission' is not defined"
     ]
    }
   ],
   "source": [
    "# Créer une ligne pour user 1261737\n",
    "new_row = pd.DataFrame({\n",
    "    'userId': ['1261737'],\n",
    "    'will_churn_prediction': [0]\n",
    "})\n",
    "\n",
    "# Ajouter à ton submission\n",
    "submission_complete = pd.concat([submission, new_row], ignore_index=True)\n",
    "\n",
    "print(f\"Submission before: {len(submission)} rows\")\n",
    "print(f\"Submission after: {len(submission_complete)} rows\")\n",
    "\n",
    "# Vérifier\n",
    "print(f\"\\nUser 1261737 in submission: {'1261737' in submission_complete['userId'].values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfeb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PIPELINE COMPLÈTE : TRAIN → TEST → PRÉDICTIONS\n",
      "================================================================================\n",
      "\n",
      "1. Chargement du modèle entraîné...\n",
      "✅ Modèle chargé\n",
      "\n",
      "2. Chargement du test set...\n",
      "Test set shape: (3739498, 19)\n",
      "Users test: 2903\n",
      "\n",
      "3. Génération des features par session (test set)...\n",
      "================================================================================\n",
      "CALCUL DES 24 FEATURES PAR SESSION (VERSION ULTRA-OPTIMISÉE)\n",
      "================================================================================\n",
      "\n",
      "Création des flags...\n",
      "Dataset: (3739498, 18)\n",
      "Users: 2903\n",
      "Sessions: 41384\n",
      "\n",
      "Identification des sessions...\n",
      "Total sessions: 43,608\n",
      "Batch size: 10,000\n",
      "Nombre de batches: 5\n",
      "\n",
      "Pré-calcul des groupes par user...\n",
      "Batch 1/5... ✓ (10000 sessions)\n",
      "Batch 2/5... ✓ (10000 sessions)\n",
      "Batch 3/5... ✓ (10000 sessions)\n",
      "Batch 4/5... ✓ (10000 sessions)\n",
      "Batch 5/5... ✓ (3608 sessions)\n",
      "\n",
      "Combinaison finale...\n",
      "\n",
      "================================================================================\n",
      "✅ 24 FEATURES PAR SESSION CRÉÉES (ULTRA-OPTIMISÉ)\n",
      "================================================================================\n",
      "Shape: (43608, 26)\n",
      "Sessions par user (moyenne): 15.0\n",
      "\n",
      "OPTIMISATIONS:\n",
      "  ✅ consistency_score simplifié (pas de date_range)\n",
      "  ✅ consecutive_days_inactive vectorisé\n",
      "  ✅ has_downgraded vectorisé\n",
      "  ✅ listening_time_ratio_7d_vs_lifetime simplifié (ratio totaux)\n",
      "  ❌ avg_time_in_session_without_music_14d enlevée (trop lente)\n",
      "\n",
      "    userId  sessionId        session_time  negative_actions_last7d_vs_avg  \\\n",
      "0  1000655      19711 2018-10-03 09:00:59                             1.0   \n",
      "1  1000655      36911 2018-10-05 22:06:38                             1.0   \n",
      "2  1000655      49298 2018-10-07 19:32:55                             1.0   \n",
      "3  1000655      54275 2018-10-18 04:38:45                             0.0   \n",
      "4  1000655      96058 2018-10-21 14:25:04                             0.0   \n",
      "\n",
      "   consistency_score  consecutive_days_inactive  session_frequency_change  \\\n",
      "0                0.0                          0                       1.0   \n",
      "1                NaN                          0                       1.0   \n",
      "2                0.0                          1                       2.0   \n",
      "3                0.0                          1                       0.0   \n",
      "4                0.0                         10                       1.0   \n",
      "\n",
      "   sessions_last7d_vs_avg  thumbs_down_last_14days  days_without_thumbs_up  \\\n",
      "0                1.000000                        0                      18   \n",
      "1                0.428571                        1                       2   \n",
      "2                0.714286                        1                       1   \n",
      "3                0.000000                        0                      12   \n",
      "4                0.678571                        0                      15   \n",
      "\n",
      "   ...  frustration_score  is_paid  has_downgraded  mobile_usage_ratio_14d  \\\n",
      "0  ...                0.0        0               0                     0.5   \n",
      "1  ...                2.0        0               0                     0.0   \n",
      "2  ...                3.5        0               0                     0.0   \n",
      "3  ...                1.5        0               0                     0.0   \n",
      "4  ...                0.0        0               0                     0.0   \n",
      "\n",
      "   days_since_registration  has_ever_paid  thumbs_down_lifetime  \\\n",
      "0                       18              0                     0   \n",
      "1                       21              0                     1   \n",
      "2                       23              0                     1   \n",
      "3                       33              0                     1   \n",
      "4                       37              0                     1   \n",
      "\n",
      "   thumbs_up_lifetime  frustration_score_lifetime  \\\n",
      "0                   0                         0.0   \n",
      "1                   2                         2.0   \n",
      "2                   3                         3.5   \n",
      "3                   3                         3.5   \n",
      "4                   3                         3.5   \n",
      "\n",
      "   listening_time_ratio_7d_vs_lifetime  \n",
      "0                             1.000000  \n",
      "1                             0.428571  \n",
      "2                             0.714286  \n",
      "3                             0.000000  \n",
      "4                             0.362822  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "✅ Features test générées: (43608, 26)\n",
      "\n",
      "4. Calcul des préférences musicales (test set)...\n",
      "================================================================================\n",
      "CALCUL DES PRÉFÉRENCES MUSICALES PAR USER\n",
      "================================================================================\n",
      "\n",
      "Dataset initial: (3739498, 19)\n",
      "Événements NextSong: 3,061,811\n",
      "Après nettoyage NaN: 3,061,811 (supprimé: 0)\n",
      "\n",
      "Distribution des genres:\n",
      "genre\n",
      "Autre               2800375\n",
      "Rock Alternatif       62907\n",
      "Pop Rock              56711\n",
      "Pop                   25842\n",
      "Hip-Hop               23565\n",
      "Metal Alternatif      21091\n",
      "Indie Pop             16959\n",
      "Country               15747\n",
      "Blues Rock            14999\n",
      "Folk Rock             13839\n",
      "Name: count, dtype: int64\n",
      "Artistes 'Autre' (non mappés): 2,800,375\n",
      "\n",
      "Calcul des préférences par user...\n",
      "Users avec préférences: 2,902\n",
      "\n",
      "Statistiques:\n",
      "  Moyenne songs écoutées: 1055.1\n",
      "  Moyenne artistes uniques: 722.4\n",
      "  Moyenne genres uniques: 9.7\n",
      "\n",
      "Encodage des variables catégorielles...\n",
      "  favorite_artist: 203 classes\n",
      "  favorite_song: 422 classes\n",
      "  favorite_genre: 1 classes\n",
      "\n",
      "================================================================================\n",
      "✅ PRÉFÉRENCES CALCULÉES\n",
      "================================================================================\n",
      "\n",
      "    userId         favorite_artist                   favorite_song  \\\n",
      "0  1000655           Kings Of Leon                        Tive Sim   \n",
      "1  1000963           Kings Of Leon                            Undo   \n",
      "2  1001129  Florence + The Machine                  You're The One   \n",
      "3  1001963               Daft Punk                     Cosmic Love   \n",
      "4  1002283                Coldplay                            Undo   \n",
      "5  1002397            3 Doors Down            3 Rounds and a Sound   \n",
      "6  1002533           Kings Of Leon                            Undo   \n",
      "7  1002712         Black Eyed Peas              Heartbreak Warfare   \n",
      "8  1002879                    Muse  Dog Days Are Over (Radio Edit)   \n",
      "9  1003703  Florence + The Machine                   ReprÃÂ©sente   \n",
      "\n",
      "  favorite_genre  total_songs_listened  unique_artists  unique_genres  \\\n",
      "0          Autre                   262             239              9   \n",
      "1          Autre                  2112            1365             11   \n",
      "2          Autre                   547             452             11   \n",
      "3          Autre                   539             459             11   \n",
      "4          Autre                  3160            1880             11   \n",
      "5          Autre                    31              31              5   \n",
      "6          Autre                   954             720             10   \n",
      "7          Autre                   315             281             10   \n",
      "8          Autre                  1065             833             11   \n",
      "9          Autre                   411             365             11   \n",
      "\n",
      "   favorite_artist_encoded  favorite_song_encoded  favorite_genre_encoded  \n",
      "0                      155                    405                       0  \n",
      "1                      155                    409                       0  \n",
      "2                      132                    421                       0  \n",
      "3                      111                    239                       0  \n",
      "4                      106                    409                       0  \n",
      "5                        9                     57                       0  \n",
      "6                      155                    409                       0  \n",
      "7                       82                    293                       0  \n",
      "8                      170                    256                       0  \n",
      "9                      132                    371                       0  \n",
      "\n",
      "✅ Features test avec musique: (43608, 31)\n",
      "\n",
      "5. Préparation de X_test...\n",
      "✅ X_test shape: (30164, 31)\n",
      "Colonnes: ['sessionId', 'negative_actions_last7d_vs_avg', 'consistency_score', 'consecutive_days_inactive', 'session_frequency_change', 'sessions_last7d_vs_avg', 'thumbs_down_last_14days', 'days_without_thumbs_up', 'has_downgrade_last_15days', 'activity_trend_last_14days', 'songs_listened_last_14days', 'help_visits_14d', 'error_rate_14d', 'settings_visits_14d', 'frustration_score', 'is_paid', 'has_downgraded', 'mobile_usage_ratio_14d', 'days_since_registration', 'has_ever_paid', 'thumbs_down_lifetime', 'thumbs_up_lifetime', 'frustration_score_lifetime', 'listening_time_ratio_7d_vs_lifetime', 'favorite_genre_encoded', 'favorite_artist_encoded', 'total_songs_listened', 'unique_artists', 'unique_genres', 'favorite_genre', 'favorite_artist']\n",
      "\n",
      "6. Vérification des colonnes...\n",
      "✅ Colonnes alignées: (30164, 31)\n",
      "\n",
      "7. Génération des prédictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bk/qpt56ww928z8p47c40hrn6j00000gn/T/ipykernel_35979/3304050670.py:87: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  features_test['favorite_genre_encoded'].fillna(-1, inplace=True)\n",
      "/var/folders/bk/qpt56ww928z8p47c40hrn6j00000gn/T/ipykernel_35979/3304050670.py:88: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  features_test['favorite_artist_encoded'].fillna(-1, inplace=True)\n",
      "/var/folders/bk/qpt56ww928z8p47c40hrn6j00000gn/T/ipykernel_35979/3304050670.py:89: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  features_test['total_songs_listened'].fillna(0, inplace=True)\n",
      "/var/folders/bk/qpt56ww928z8p47c40hrn6j00000gn/T/ipykernel_35979/3304050670.py:90: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  features_test['unique_artists'].fillna(0, inplace=True)\n",
      "/var/folders/bk/qpt56ww928z8p47c40hrn6j00000gn/T/ipykernel_35979/3304050670.py:91: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  features_test['unique_genres'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (30164) does not match length of index (43608)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 167\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Ajouter au DataFrame\u001b[39;00m\n\u001b[32m    166\u001b[39m predictions = features_test[[\u001b[33m'\u001b[39m\u001b[33muserId\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msession_time\u001b[39m\u001b[33m'\u001b[39m]].copy()\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m \u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchurn_probability\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m = y_pred_proba\n\u001b[32m    168\u001b[39m predictions[\u001b[33m'\u001b[39m\u001b[33mwill_churn_prediction\u001b[39m\u001b[33m'\u001b[39m] = y_pred\n\u001b[32m    170\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Prédictions générées: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictions.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:4322\u001b[39m, in \u001b[36mDataFrame.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4319\u001b[39m     \u001b[38;5;28mself\u001b[39m._setitem_array([key], value)\n\u001b[32m   4320\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4321\u001b[39m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4322\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:4535\u001b[39m, in \u001b[36mDataFrame._set_item\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4525\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4526\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4527\u001b[39m \u001b[33;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[32m   4528\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4533\u001b[39m \u001b[33;03m    ensure homogeneity.\u001b[39;00m\n\u001b[32m   4534\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4535\u001b[39m     value, refs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4537\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4538\u001b[39m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns\n\u001b[32m   4539\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m value.ndim == \u001b[32m1\u001b[39m\n\u001b[32m   4540\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value.dtype, ExtensionDtype)\n\u001b[32m   4541\u001b[39m     ):\n\u001b[32m   4542\u001b[39m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[32m   4543\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.is_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.columns, MultiIndex):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:5288\u001b[39m, in \u001b[36mDataFrame._sanitize_column\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m   5285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m.index)\n\u001b[32m   5287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[32m-> \u001b[39m\u001b[32m5288\u001b[39m     \u001b[43mcom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5289\u001b[39m arr = sanitize_array(value, \u001b[38;5;28mself\u001b[39m.index, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   5290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   5291\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[32m   5292\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m value.dtype == \u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5295\u001b[39m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[32m   5296\u001b[39m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/common.py:573\u001b[39m, in \u001b[36mrequire_length_match\u001b[39m\u001b[34m(data, index)\u001b[39m\n\u001b[32m    569\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    570\u001b[39m \u001b[33;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) != \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    574\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLength of values \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    575\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdoes not match length of index \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    578\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Length of values (30164) does not match length of index (43608)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PIPELINE COMPLÈTE : FEATURES TEST SET + PRÉDICTIONS\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PIPELINE COMPLÈTE : TRAIN → TEST → PRÉDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 1 : CHARGER LE MODÈLE ENTRAÎNÉ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"1. Chargement du modèle entraîné...\")\n",
    "\n",
    "# Option B : Si tu as le modèle en mémoire\n",
    "model = model_XGB  # (celui retourné par XGB_training)\n",
    "\n",
    "print(\"✅ Modèle chargé\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 2 : CHARGER LE TEST SET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"2. Chargement du test set...\")\n",
    "\n",
    "df_test = data_clean.copy()  # Ton fichier test\n",
    "\n",
    "print(f\"Test set shape: {df_test.shape}\")\n",
    "print(f\"Users test: {df_test['userId'].nunique()}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 3 : CRÉER LES FEATURES PAR SESSION (TEST SET)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"3. Génération des features par session (test set)...\")\n",
    "\n",
    "# Colonnes nécessaires\n",
    "required_cols = ['userId', 'sessionId', 'time', 'page', 'registration', 'level', 'userAgent', 'length', 'artist']\n",
    "\n",
    "# IMPORTANT: Le test set N'A PAS de churn events\n",
    "# Donc on ne filtre PAS \"Cancellation Confirmation\"\n",
    "df_test_clean = df_test[required_cols].copy()\n",
    "\n",
    "# Générer les features\n",
    "features_test = create_features_per_session_optimized(\n",
    "    df_test_clean,\n",
    "    batch_size=10000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Features test générées: {features_test.shape}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 4 : AJOUTER LES PRÉFÉRENCES MUSICALES (TEST SET)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"4. Calcul des préférences musicales (test set)...\")\n",
    "\n",
    "music_prefs_test = create_user_preferences_improved(\n",
    "    df_test,\n",
    "    encode=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Merger\n",
    "features_test = features_test.merge(\n",
    "    music_prefs_test[[\n",
    "        'userId',\n",
    "        'favorite_genre_encoded',\n",
    "        'favorite_artist_encoded',\n",
    "        'total_songs_listened',\n",
    "        'unique_artists',\n",
    "        'unique_genres'\n",
    "    ]],\n",
    "    on='userId',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remplir les NaN\n",
    "features_test['favorite_genre_encoded'].fillna(-1, inplace=True)\n",
    "features_test['favorite_artist_encoded'].fillna(-1, inplace=True)\n",
    "features_test['total_songs_listened'].fillna(0, inplace=True)\n",
    "features_test['unique_artists'].fillna(0, inplace=True)\n",
    "features_test['unique_genres'].fillna(0, inplace=True)\n",
    "\n",
    "print(f\"✅ Features test avec musique: {features_test.shape}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 5 : PRÉPARER X_test (MÊMES COLONNES QUE TRAIN)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"5. Préparation de X_test...\")\n",
    "\n",
    "# Colonnes à exclure\n",
    "exclude_cols = [\n",
    "    'userId',\n",
    "    'sessionId',\n",
    "    'session_time',\n",
    "    'favorite_genre',      # Texte\n",
    "    'favorite_artist'      # Texte\n",
    "]\n",
    "\n",
    "# Enlever les colonnes d'exclusion\n",
    "X_test = features_test.drop(columns=exclude_cols, errors='ignore')\n",
    "\n",
    "\n",
    "print(f\"✅ X_test shape: {X_test.shape}\")\n",
    "print(f\"Colonnes: {X_test.columns.tolist()}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 6 : VÉRIFIER QUE LES COLONNES MATCHENT AVEC LE TRAIN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"6. Vérification des colonnes...\")\n",
    "\n",
    "# Récupérer les colonnes utilisées pendant l'entraînement\n",
    "# Option A : Si tu as sauvegardé les colonnes\n",
    "# train_cols = pd.read_csv('train_columns.txt', header=None)[0].tolist()\n",
    "\n",
    "# Option B : Utiliser les feature_names du modèle XGBoost\n",
    "train_cols = model.get_booster().feature_names\n",
    "\n",
    "# Vérifier les colonnes manquantes/en trop\n",
    "missing_cols = set(train_cols) - set(X_test.columns)\n",
    "extra_cols = set(X_test.columns) - set(train_cols)\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"⚠️ Colonnes manquantes dans test: {missing_cols}\")\n",
    "    # Ajouter les colonnes manquantes avec des 0\n",
    "    for col in missing_cols:\n",
    "        X_test[col] = 0\n",
    "\n",
    "if extra_cols:\n",
    "    print(f\"⚠️ Colonnes en trop dans test: {extra_cols}\")\n",
    "    # Enlever les colonnes en trop\n",
    "    X_test = X_test.drop(columns=list(extra_cols))\n",
    "\n",
    "# Réordonner les colonnes dans le même ordre que train\n",
    "X_test = X_test[train_cols]\n",
    "\n",
    "print(f\"✅ Colonnes alignées: {X_test.shape}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 7 : PRÉDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"7. Génération des prédictions...\")\n",
    "\n",
    "# Prédire les probabilités\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Prédire les classes (avec seuil par défaut 0.5)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Ajouter au DataFrame\n",
    "predictions = features_test[['userId', 'sessionId', 'session_time']].copy()\n",
    "predictions['churn_probability'] = y_pred_proba\n",
    "predictions['will_churn_prediction'] = y_pred\n",
    "\n",
    "print(f\"✅ Prédictions générées: {predictions.shape}\")\n",
    "print()\n",
    "print(\"Aperçu des prédictions:\")\n",
    "print(predictions.head(20))\n",
    "print()\n",
    "print(\"Distribution des prédictions:\")\n",
    "print(predictions['will_churn_prediction'].value_counts())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "26261cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Préparation de X_test...\n",
      "✅ X_test shape: (43608, 28)\n",
      "Colonnes: ['negative_actions_last7d_vs_avg', 'consistency_score', 'consecutive_days_inactive', 'session_frequency_change', 'sessions_last7d_vs_avg', 'thumbs_down_last_14days', 'days_without_thumbs_up', 'has_downgrade_last_15days', 'activity_trend_last_14days', 'songs_listened_last_14days', 'help_visits_14d', 'error_rate_14d', 'settings_visits_14d', 'frustration_score', 'is_paid', 'has_downgraded', 'mobile_usage_ratio_14d', 'days_since_registration', 'has_ever_paid', 'thumbs_down_lifetime', 'thumbs_up_lifetime', 'frustration_score_lifetime', 'listening_time_ratio_7d_vs_lifetime', 'favorite_genre_encoded', 'favorite_artist_encoded', 'total_songs_listened', 'unique_artists', 'unique_genres']\n",
      "\n",
      "6. Vérification des colonnes...\n",
      "⚠️ Colonnes manquantes dans test: {'sessionId'}\n",
      "✅ Colonnes alignées: (43608, 29)\n",
      "\n",
      "7. Génération des prédictions...\n",
      "✅ Prédictions générées: (43608, 5)\n",
      "\n",
      "Aperçu des prédictions:\n",
      "     userId  sessionId        session_time  churn_probability  \\\n",
      "0   1000655      19711 2018-10-03 09:00:59           0.473175   \n",
      "1   1000655      36911 2018-10-05 22:06:38           0.743302   \n",
      "2   1000655      49298 2018-10-07 19:32:55           0.799956   \n",
      "3   1000655      54275 2018-10-18 04:38:45           0.722955   \n",
      "4   1000655      96058 2018-10-21 14:25:04           0.733780   \n",
      "5   1000655     107838 2018-10-28 15:30:29           0.718473   \n",
      "6   1000655     136737 2018-11-04 20:26:57           0.742593   \n",
      "7   1000655     158124 2018-11-12 11:33:58           0.715430   \n",
      "8   1000655     182881 2018-11-14 10:05:13           0.723879   \n",
      "9   1000655     190141 2018-11-15 16:46:22           0.781142   \n",
      "10  1000963       1563 2018-10-04 23:29:51           0.077419   \n",
      "11  1000963      46146 2018-10-08 23:21:31           0.077791   \n",
      "12  1000963      59367 2018-10-12 13:37:53           0.084901   \n",
      "13  1000963      76228 2018-10-15 08:06:48           0.097345   \n",
      "14  1000963      85025 2018-10-16 21:16:38           0.120891   \n",
      "15  1000963      91621 2018-10-22 15:37:59           0.126957   \n",
      "16  1000963     114044 2018-10-24 20:38:20           0.164432   \n",
      "17  1000963     120608 2018-10-25 15:51:05           0.161516   \n",
      "18  1000963     124928 2018-10-28 00:06:27           0.175106   \n",
      "19  1000963     132138 2018-10-29 08:43:53           0.190294   \n",
      "\n",
      "    will_churn_prediction  \n",
      "0                       0  \n",
      "1                       1  \n",
      "2                       1  \n",
      "3                       1  \n",
      "4                       1  \n",
      "5                       1  \n",
      "6                       1  \n",
      "7                       1  \n",
      "8                       1  \n",
      "9                       1  \n",
      "10                      0  \n",
      "11                      0  \n",
      "12                      0  \n",
      "13                      0  \n",
      "14                      0  \n",
      "15                      0  \n",
      "16                      0  \n",
      "17                      0  \n",
      "18                      0  \n",
      "19                      0  \n",
      "\n",
      "Distribution des prédictions:\n",
      "will_churn_prediction\n",
      "0    23686\n",
      "1    19922\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"5. Préparation de X_test...\")\n",
    "\n",
    "# Colonnes à exclure\n",
    "exclude_cols = [\n",
    "    'userId',\n",
    "    'sessionId',\n",
    "    'session_time',\n",
    "    'favorite_genre',\n",
    "    'favorite_artist'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "X_test = features_test.drop(columns=exclude_cols, errors='ignore').copy()\n",
    "\n",
    "\n",
    "print(f\"✅ X_test shape: {X_test.shape}\")\n",
    "print(f\"Colonnes: {X_test.columns.tolist()}\")\n",
    "print()\n",
    "numeric_cols = X_test.select_dtypes(include=['number']).columns\n",
    "X_test[numeric_cols] = X_test[numeric_cols].fillna(0)\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 6 : VÉRIFIER QUE LES COLONNES MATCHENT AVEC LE TRAIN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"6. Vérification des colonnes...\")\n",
    "\n",
    "# Récupérer les colonnes utilisées pendant l'entraînement\n",
    "# Option A : Si tu as sauvegardé les colonnes\n",
    "# train_cols = pd.read_csv('train_columns.txt', header=None)[0].tolist()\n",
    "\n",
    "# Option B : Utiliser les feature_names du modèle XGBoost\n",
    "train_cols = model.get_booster().feature_names\n",
    "\n",
    "# Vérifier les colonnes manquantes/en trop\n",
    "missing_cols = set(train_cols) - set(X_test.columns)\n",
    "extra_cols = set(X_test.columns) - set(train_cols)\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"⚠️ Colonnes manquantes dans test: {missing_cols}\")\n",
    "    # Ajouter les colonnes manquantes avec des 0\n",
    "    for col in missing_cols:\n",
    "        X_test[col] = 0\n",
    "\n",
    "if extra_cols:\n",
    "    print(f\"⚠️ Colonnes en trop dans test: {extra_cols}\")\n",
    "    # Enlever les colonnes en trop\n",
    "    X_test = X_test.drop(columns=list(extra_cols))\n",
    "\n",
    "# Réordonner les colonnes dans le même ordre que train\n",
    "X_test = X_test[train_cols]\n",
    "\n",
    "print(f\"✅ Colonnes alignées: {X_test.shape}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 7 : PRÉDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"7. Génération des prédictions...\")\n",
    "\n",
    "# Prédire les probabilités\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Prédire les classes (avec seuil par défaut 0.5)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Ajouter au DataFrame\n",
    "predictions = features_test[['userId', 'sessionId', 'session_time']].copy()\n",
    "predictions['churn_probability'] = y_pred_proba\n",
    "predictions['will_churn_prediction'] = y_pred\n",
    "\n",
    "print(f\"✅ Prédictions générées: {predictions.shape}\")\n",
    "print()\n",
    "print(\"Aperçu des prédictions:\")\n",
    "print(predictions.head(20))\n",
    "print()\n",
    "print(\"Distribution des prédictions:\")\n",
    "print(predictions['will_churn_prediction'].value_counts())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "53627004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.7\n",
      "Predictions distribution:\n",
      "will_churn_prediction\n",
      "0    34627\n",
      "1     8981\n",
      "Name: count, dtype: int64\n",
      "Churn rate: 20.6%\n",
      "\n",
      "✅ New submission saved!\n",
      "Total: 2904 users\n",
      "Target distribution:\n",
      "target\n",
      "1    2061\n",
      "0     843\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# NEW SUBMISSION WITH ADJUSTED THRESHOLD\n",
    "# ============================================================================\n",
    "\n",
    "# Option A: Use a fixed higher threshold\n",
    "NEW_THRESHOLD = 0.7  # Much stricter\n",
    "\n",
    "# Option B: Use top X% as churners (uncomment to use)\n",
    "# TARGET_CHURN_RATE = 0.25\n",
    "# sorted_probs = predictions['churn_probability'].sort_values(ascending=False)\n",
    "# NEW_THRESHOLD = sorted_probs.iloc[int(len(sorted_probs) * TARGET_CHURN_RATE)]\n",
    "\n",
    "predictions['will_churn_prediction'] = (predictions['churn_probability'] >= NEW_THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"Threshold: {NEW_THRESHOLD}\")\n",
    "print(f\"Predictions distribution:\\n{predictions['will_churn_prediction'].value_counts()}\")\n",
    "print(f\"Churn rate: {predictions['will_churn_prediction'].mean():.1%}\")\n",
    "\n",
    "# Now create submission with the same code as before...\n",
    "submission = predictions[['userId', 'will_churn_prediction']].copy()\n",
    "\n",
    "# Add back user 1261737\n",
    "new_row = pd.DataFrame({'userId': ['1261737'], 'will_churn_prediction': [0]})\n",
    "submission = pd.concat([submission, new_row], ignore_index=True)\n",
    "\n",
    "# Rename and deduplicate\n",
    "submission = submission.rename(columns={'userId': 'id', 'will_churn_prediction': 'target'})\n",
    "submission = submission.groupby('id').agg({'target': 'max'}).reset_index()\n",
    "\n",
    "# Add missing users\n",
    "missing_users = set(df_test['userId'].unique()) - set(submission['id'].unique())\n",
    "if missing_users:\n",
    "    missing_rows = pd.DataFrame({'id': list(missing_users), 'target': 0})\n",
    "    submission = pd.concat([submission, missing_rows], ignore_index=True)\n",
    "\n",
    "submission = submission.sort_values('id').reset_index(drop=True)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\n✅ New submission saved!\")\n",
    "print(f\"Total: {len(submission)} users\")\n",
    "print(f\"Target distribution:\\n{submission['target'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "718b8f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>session_time</th>\n",
       "      <th>churn_probability</th>\n",
       "      <th>will_churn_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23714</th>\n",
       "      <td>1556487</td>\n",
       "      <td>12210</td>\n",
       "      <td>2018-10-12 21:40:19</td>\n",
       "      <td>0.548697</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15072</th>\n",
       "      <td>1359870</td>\n",
       "      <td>55053</td>\n",
       "      <td>2018-10-09 08:01:35</td>\n",
       "      <td>0.075741</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21473</th>\n",
       "      <td>1502101</td>\n",
       "      <td>15177</td>\n",
       "      <td>2018-11-01 20:39:51</td>\n",
       "      <td>0.804972</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17642</th>\n",
       "      <td>1417835</td>\n",
       "      <td>79646</td>\n",
       "      <td>2018-10-14 01:48:12</td>\n",
       "      <td>0.078170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15368</th>\n",
       "      <td>1367676</td>\n",
       "      <td>155456</td>\n",
       "      <td>2018-11-05 13:37:16</td>\n",
       "      <td>0.272234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5046</th>\n",
       "      <td>1122562</td>\n",
       "      <td>149616</td>\n",
       "      <td>2018-11-04 16:31:39</td>\n",
       "      <td>0.595181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26946</th>\n",
       "      <td>1632728</td>\n",
       "      <td>657</td>\n",
       "      <td>2018-10-05 18:24:16</td>\n",
       "      <td>0.385794</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14601</th>\n",
       "      <td>1350539</td>\n",
       "      <td>151130</td>\n",
       "      <td>2018-11-02 11:29:31</td>\n",
       "      <td>0.620243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20713</th>\n",
       "      <td>1482187</td>\n",
       "      <td>46993</td>\n",
       "      <td>2018-10-24 07:20:44</td>\n",
       "      <td>0.562501</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4854</th>\n",
       "      <td>1117946</td>\n",
       "      <td>20760</td>\n",
       "      <td>2018-10-24 20:55:41</td>\n",
       "      <td>0.437341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  sessionId        session_time  churn_probability  \\\n",
       "23714  1556487      12210 2018-10-12 21:40:19           0.548697   \n",
       "15072  1359870      55053 2018-10-09 08:01:35           0.075741   \n",
       "21473  1502101      15177 2018-11-01 20:39:51           0.804972   \n",
       "17642  1417835      79646 2018-10-14 01:48:12           0.078170   \n",
       "15368  1367676     155456 2018-11-05 13:37:16           0.272234   \n",
       "5046   1122562     149616 2018-11-04 16:31:39           0.595181   \n",
       "26946  1632728        657 2018-10-05 18:24:16           0.385794   \n",
       "14601  1350539     151130 2018-11-02 11:29:31           0.620243   \n",
       "20713  1482187      46993 2018-10-24 07:20:44           0.562501   \n",
       "4854   1117946      20760 2018-10-24 20:55:41           0.437341   \n",
       "\n",
       "       will_churn_prediction  \n",
       "23714                      0  \n",
       "15072                      0  \n",
       "21473                      1  \n",
       "17642                      0  \n",
       "15368                      0  \n",
       "5046                       0  \n",
       "26946                      0  \n",
       "14601                      0  \n",
       "20713                      0  \n",
       "4854                       0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68520b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sessionId', 'negative_actions_last7d_vs_avg', 'consistency_score',\n",
       "       'consecutive_days_inactive', 'session_frequency_change',\n",
       "       'sessions_last7d_vs_avg', 'thumbs_down_last_14days',\n",
       "       'days_without_thumbs_up', 'has_downgrade_last_15days',\n",
       "       'activity_trend_last_14days', 'songs_listened_last_14days',\n",
       "       'help_visits_14d', 'error_rate_14d', 'settings_visits_14d',\n",
       "       'frustration_score', 'is_paid', 'has_downgraded',\n",
       "       'mobile_usage_ratio_14d', 'days_since_registration', 'has_ever_paid',\n",
       "       'thumbs_down_lifetime', 'thumbs_up_lifetime',\n",
       "       'frustration_score_lifetime', 'listening_time_ratio_7d_vs_lifetime',\n",
       "       'favorite_genre_encoded', 'favorite_artist_encoded',\n",
       "       'total_songs_listened', 'unique_artists', 'unique_genres',\n",
       "       'favorite_genre', 'favorite_artist'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#X_train_check = X_test.copy()\n",
    "X_train_check.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b28fd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sessionId', 'negative_actions_last7d_vs_avg', 'consistency_score',\n",
       "       'consecutive_days_inactive', 'session_frequency_change',\n",
       "       'sessions_last7d_vs_avg', 'thumbs_down_last_14days',\n",
       "       'days_without_thumbs_up', 'has_downgrade_last_15days',\n",
       "       'activity_trend_last_14days', 'songs_listened_last_14days',\n",
       "       'help_visits_14d', 'error_rate_14d', 'settings_visits_14d',\n",
       "       'frustration_score', 'is_paid', 'has_downgraded',\n",
       "       'mobile_usage_ratio_14d', 'days_since_registration', 'has_ever_paid',\n",
       "       'thumbs_down_lifetime', 'thumbs_up_lifetime',\n",
       "       'frustration_score_lifetime', 'listening_time_ratio_7d_vs_lifetime',\n",
       "       'favorite_genre_encoded', 'favorite_artist_encoded',\n",
       "       'total_songs_listened', 'unique_artists', 'unique_genres',\n",
       "       'favorite_genre', 'favorite_artist'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f96dbd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_clean.to_csv('features_test_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7638f100",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'submission_complete' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Renommer les colonnes pour Kaggle\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m submission_final = \u001b[43msubmission_complete\u001b[49m.rename(columns={\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33muserId\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mwill_churn_prediction\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m })\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Vérifier\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSubmission structure:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'submission_complete' is not defined"
     ]
    }
   ],
   "source": [
    "# Renommer les colonnes pour Kaggle\n",
    "submission_final = submission_complete.rename(columns={\n",
    "    'userId': 'id',\n",
    "    'will_churn_prediction': 'target'\n",
    "})\n",
    "\n",
    "# Vérifier\n",
    "print(\"Submission structure:\")\n",
    "print(submission_final.head())\n",
    "print(f\"\\nColumns: {submission_final.columns.tolist()}\")\n",
    "print(f\"Shape: {submission_final.shape}\")\n",
    "\n",
    "# Vérifier pas de null\n",
    "print(f\"\\nNull values: {submission_final.isnull().sum()}\")\n",
    "\n",
    "print(\"\\n✓ Submission ready with columns: id, target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "44b9558b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deduplication: 2904 unique users\n",
      "\n",
      "Target distribution:\n",
      "target\n",
      "1    2825\n",
      "0      79\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Grouper par userId : 1 si au moins un 1, sinon 0\n",
    "submission_final = submission_final.groupby('id').agg({\n",
    "    'target': 'max'  # max = 1 si au moins un 1, sinon 0\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"After deduplication: {len(submission_final)} unique users\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(submission_final['target'].value_counts())\n",
    "\n",
    "# Vérifier plus de duplicates\n",
    "print(f\"\\nDuplicates: {submission_final['id'].duplicated().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "85a90a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected users: 2903\n",
      "Submitted users: 2904\n",
      "\n",
      "Missing users: 0\n",
      "Missing user IDs: []\n",
      "\n",
      "Before adding missing: 2904 rows\n",
      "After adding missing: 2904 rows\n",
      "\n",
      "✓ Complete submission with all 2904 users saved!\n"
     ]
    }
   ],
   "source": [
    "# 1. Identifier les userId manquants\n",
    "expected_users = df_test['userId'].unique()\n",
    "submitted_users = submission_final['id'].unique()\n",
    "\n",
    "print(f\"Expected users: {len(expected_users)}\")\n",
    "print(f\"Submitted users: {len(submitted_users)}\")\n",
    "\n",
    "# Trouver les manquants\n",
    "missing_users = set(expected_users) - set(submitted_users)\n",
    "print(f\"\\nMissing users: {len(missing_users)}\")\n",
    "print(f\"Missing user IDs: {list(missing_users)[:10]}\")  # Afficher les 10 premiers\n",
    "\n",
    "# 2. Créer des lignes pour les utilisateurs manquants (avec target=0)\n",
    "missing_rows = pd.DataFrame({\n",
    "    'id': list(missing_users),\n",
    "    'target': 0\n",
    "})\n",
    "\n",
    "# 3. Ajouter à la submission\n",
    "submission_complete = pd.concat([submission_final, missing_rows], ignore_index=True)\n",
    "\n",
    "print(f\"\\nBefore adding missing: {len(submission_final)} rows\")\n",
    "print(f\"After adding missing: {len(submission_complete)} rows\")\n",
    "\n",
    "# 4. Vérifier qu'on a bien 2904\n",
    "assert len(submission_complete) == 2904, f\"Expected 2904, got {len(submission_complete)}\"\n",
    "\n",
    "# 5. Trier par id (optionnel mais propre)\n",
    "submission_complete = submission_complete.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "# 6. Sauvegarder\n",
    "submission_complete.to_csv('submission.csv', index=False)\n",
    "print(\"\\n✓ Complete submission with all 2904 users saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e86268",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b756af5-4802-489c-9110-9712ef5d4057",
   "metadata": {},
   "source": [
    "The competition is to be performed in groups of two. You'll have a report of 4 pages to submit by december 14th, presenting the methods you tested and used. For the defense you'll get 8 minutes of presentations + 7 minutes of questions, including on question on the labs, that may involve writing a code snippet.\n",
    "\n",
    "\n",
    "Churn prediction 25/26\n",
    "**Predict churn prediction from streaming service logs**\n",
    "\n",
    "The goal of the competition is to predict whether or not some users (whose user ids are in the test file) will **churn in the window of 10 days that follows the given observations (ie after \"2018-11-20\")**. We consider that a user churns when they visit the page **'Cancellation Confirmation'** (edited) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f8b63795-e760-4f0a-bae5-cf2df708a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train_test = pd.read_parquet(\"data/test.parquet\")\n",
    "df_train = pd.read_parquet(\"data/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4f5aaa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2904"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_test[\"userId\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701237a",
   "metadata": {},
   "source": [
    "### Creation of target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51d866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant: 18304, Après: 18304, Supprimé: 836\n"
     ]
    }
   ],
   "source": [
    "# Supprimer users qui churnent dans les 7 premiers jours\n",
    "churners = df_train[df_train['page'] == 'Cancellation Confirmation'].copy()\n",
    "churners['days_since_start'] = (churners['time'] - df_train['time'].min()).dt.days\n",
    "early_churners = churners[churners['days_since_start'] <= 7]['userId'].unique()\n",
    "\n",
    "df_train = df_train[~df_train['userId'].isin(early_churners)]\n",
    "\n",
    "print(f\"Avant: {df_train['userId'].nunique()}, Après: {df_train['userId'].nunique()}, Supprimé: {len(early_churners)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97204208-75c0-4e5e-bd4d-1ee12e661e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating cancellation in following ten days column\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cancellation_events = df_train[df_train['page'] == 'Cancellation Confirmation'].copy()\n",
    "cancellation_events = cancellation_events[['userId', 'time']].rename(columns={'time': 'churn_time'})\n",
    "\n",
    "df_train = df_train.merge(cancellation_events, on='userId', how='left')\n",
    "\n",
    "df_train['days_until_churn'] = (df_train['churn_time'] - df_train['time']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "df_train['will_churn_10days'] = ((df_train['days_until_churn'] >= 0) & \n",
    "                                   (df_train['days_until_churn'] <= 10)).astype(int)\n",
    "\n",
    "df_train = df_train.drop(['churn_time', 'days_until_churn'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1962a199-b9c6-4f5e-a34a-892fdf8e4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe() #max time is 2018-11-20 so we are going to keep only the rows that are at least 10 days old OR that have churn True\n",
    "\n",
    "df_train = df_train[(df_train[\"time\"] < \"2018-11-10\" )| (df_train[\"will_churn_10days\"] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2884f3",
   "metadata": {},
   "source": [
    "## Preliminary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fab7aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'utilisateurs qui ont churné : 3435\n",
      "Taux de churn global : 19.03%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>churn_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>2018-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000083</td>\n",
       "      <td>2018-10-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000280</td>\n",
       "      <td>2018-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000353</td>\n",
       "      <td>2018-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000503</td>\n",
       "      <td>2018-10-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>1998845</td>\n",
       "      <td>2018-10-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>1998879</td>\n",
       "      <td>2018-10-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>1999022</td>\n",
       "      <td>2018-11-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>1999847</td>\n",
       "      <td>2018-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>1999892</td>\n",
       "      <td>2018-10-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3435 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  churn_date\n",
       "0     1000025  2018-10-18\n",
       "1     1000083  2018-10-12\n",
       "2     1000280  2018-11-13\n",
       "3     1000353  2018-10-22\n",
       "4     1000503  2018-10-13\n",
       "...       ...         ...\n",
       "3430  1998845  2018-10-24\n",
       "3431  1998879  2018-10-21\n",
       "3432  1999022  2018-11-04\n",
       "3433  1999847  2018-10-18\n",
       "3434  1999892  2018-10-26\n",
       "\n",
       "[3435 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Identifier les churners et leur date de churn\n",
    "churners = df_train[df_train['page'] == 'Cancellation Confirmation'].copy()\n",
    "churn_dates = churners.groupby('userId')['time'].min().reset_index()\n",
    "churn_dates.columns = ['userId', 'churn_date']\n",
    "\n",
    "print(f\"Nombre d'utilisateurs qui ont churné : {len(churn_dates)}\")\n",
    "print(f\"Taux de churn global : {len(churn_dates) / df_train['userId'].nunique() * 100:.2f}%\")\n",
    "churn_dates['churn_date'] = churn_dates['churn_date'].dt.date\n",
    "churn_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2db3b87",
   "metadata": {},
   "source": [
    "## Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5525fd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape avant nettoyage: (14478407, 20)\n",
      "Mémoire avant: 12.33 GB\n",
      "Shape après nettoyage: (14478407, 8)\n",
      "Mémoire après: 5.21 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape avant nettoyage: {df_train.shape}\")\n",
    "print(f\"Mémoire avant: {df_train.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "# Garder uniquement les colonnes nécessaires\n",
    "colonnes_necessaires = ['userId', 'sessionId', 'level', 'userAgent', 'time', 'page', 'length', 'registration']\n",
    "df_train = df_train[colonnes_necessaires].copy()\n",
    "\n",
    "print(f\"Shape après nettoyage: {df_train.shape}\")\n",
    "print(f\"Mémoire après: {df_train.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "# Supprimer le leakage\n",
    "df_train = df_train[~df_train['page'].isin(['Cancel', 'Cancellation Confirmation'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc63e97",
   "metadata": {},
   "source": [
    "## Sous le capot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7620df",
   "metadata": {},
   "source": [
    "### Current function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c57b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction ULTRA-OPTIMISÉE par SESSION avec 24 features\n",
    "Version RAPIDE : ~2 minutes pour 1M sessions\n",
    "\n",
    "OPTIMISATIONS:\n",
    "- Feature 25 (avg_time_in_session_without_music_14d) ENLEVÉE (trop lente)\n",
    "- Features 23, 24 SIMPLIFIÉES (ratios totaux au lieu de moyennes par session)\n",
    "- consistency_score SIMPLIFIÉ\n",
    "- consecutive_days_inactive VECTORISÉ\n",
    "- has_downgraded VECTORISÉ\n",
    "- Pré-calculs maximisés\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import gc\n",
    "\n",
    "\n",
    "def create_features_per_session_optimized(\n",
    "    df: pd.DataFrame,\n",
    "    batch_size: int = 10000,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule 24 features ULTRA-RAPIDE à chaque nouvelle session\n",
    "    \n",
    "    Colonnes requises: userId, sessionId, time, page, registration, level, userAgent, length\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame avec (userId, sessionId, session_time, 24 features)\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*80)\n",
    "        print(\"CALCUL DES 24 FEATURES PAR SESSION (VERSION ULTRA-OPTIMISÉE)\")\n",
    "        print(\"=\"*80)\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PRÉPARATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['date'] = df['time'].dt.date\n",
    "    df['registration'] = pd.to_datetime(df['registration'])\n",
    "    df = df.sort_values(['userId', 'sessionId', 'time']).reset_index(drop=True)\n",
    "    \n",
    "    # Convertir length en float (en secondes)\n",
    "    df['length'] = pd.to_numeric(df['length'], errors='coerce').fillna(0)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Création des flags...\")\n",
    "    \n",
    "    # Flags (vectorisé)\n",
    "    df['is_nextsong'] = (df['page'] == 'NextSong').astype(np.int8)\n",
    "    df['is_thumbs_up'] = (df['page'] == 'Thumbs Up').astype(np.int8)\n",
    "    df['is_thumbs_down'] = (df['page'] == 'Thumbs Down').astype(np.int8)\n",
    "    df['is_error'] = (df['page'] == 'Error').astype(np.int8)\n",
    "    df['is_help'] = (df['page'] == 'Help').astype(np.int8)\n",
    "    df['is_settings'] = (df['page'] == 'Settings').astype(np.int8)\n",
    "    df['is_downgrade'] = (df['page'] == 'Downgrade').astype(np.int8)\n",
    "    \n",
    "    # Device type (simplifié)\n",
    "    df['is_mobile_action'] = df['userAgent'].str.contains('iPhone|iPad|Android', case=False, na=False).astype(np.int8)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Dataset: {df.shape}\")\n",
    "        print(f\"Users: {df['userId'].nunique()}\")\n",
    "        print(f\"Sessions: {df['sessionId'].nunique()}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # IDENTIFIER LES SESSIONS\n",
    "    # ========================================================================\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Identification des sessions...\")\n",
    "    \n",
    "    session_starts = df.groupby(['userId', 'sessionId']).agg({\n",
    "        'time': 'min',\n",
    "        'registration': 'first'\n",
    "    }).reset_index()\n",
    "    session_starts.columns = ['userId', 'sessionId', 'session_time', 'registration']\n",
    "    \n",
    "    total_sessions = len(session_starts)\n",
    "    num_batches = (total_sessions + batch_size - 1) // batch_size\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Total sessions: {total_sessions:,}\")\n",
    "        print(f\"Batch size: {batch_size:,}\")\n",
    "        print(f\"Nombre de batches: {num_batches}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PRÉ-CALCUL : GROUPBY PAR USER\n",
    "    # ========================================================================\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Pré-calcul des groupes par user...\")\n",
    "    \n",
    "    user_groups = {user_id: group for user_id, group in df.groupby('userId', sort=False)}\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TRAITEMENT PAR BATCH DE SESSIONS\n",
    "    # ========================================================================\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        if verbose:\n",
    "            print(f\"Batch {batch_idx + 1}/{num_batches}...\", end=' ')\n",
    "        \n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, total_sessions)\n",
    "        batch_sessions = session_starts.iloc[start_idx:end_idx]\n",
    "        \n",
    "        batch_features = []\n",
    "        \n",
    "        for idx, row in batch_sessions.iterrows():\n",
    "            user_id = row['userId']\n",
    "            session_id = row['sessionId']\n",
    "            session_time = row['session_time']\n",
    "            registration = row['registration']\n",
    "            \n",
    "            user_data = user_groups[user_id]\n",
    "            user_data_before = user_data[user_data['time'] < session_time]\n",
    "            \n",
    "            if len(user_data_before) == 0:\n",
    "                # Première session : features par défaut\n",
    "                features = {\n",
    "                    'userId': user_id,\n",
    "                    'sessionId': session_id,\n",
    "                    'session_time': session_time,\n",
    "                    'negative_actions_last7d_vs_avg': 1.0,\n",
    "                    'consistency_score': 0.0,\n",
    "                    'consecutive_days_inactive': 0,\n",
    "                    'session_frequency_change': 1.0,\n",
    "                    'sessions_last7d_vs_avg': 1.0,\n",
    "                    'thumbs_down_last_14days': 0,\n",
    "                    'days_without_thumbs_up': (session_time - registration).days,\n",
    "                    'has_downgrade_last_15days': 0,\n",
    "                    'activity_trend_last_14days': 1.0,\n",
    "                    'songs_listened_last_14days': 0,\n",
    "                    'help_visits_14d': 0,\n",
    "                    'error_rate_14d': 0.0,\n",
    "                    'settings_visits_14d': 0,\n",
    "                    'frustration_score': 0.0,\n",
    "                    'is_paid': 0,\n",
    "                    'has_downgraded': 0,\n",
    "                    'mobile_usage_ratio_14d': 0.5,\n",
    "                    'days_since_registration': (session_time - registration).days,\n",
    "                    'has_ever_paid': 0,\n",
    "                    'thumbs_down_lifetime': 0,\n",
    "                    'thumbs_up_lifetime': 0,\n",
    "                    'frustration_score_lifetime': 0.0,\n",
    "                    'listening_time_ratio_7d_vs_lifetime': 1.0\n",
    "                }\n",
    "                batch_features.append(features)\n",
    "                continue\n",
    "            \n",
    "            # Fenêtres temporelles\n",
    "            window_14d_start = session_time - timedelta(days=14)\n",
    "            window_7d_start = session_time - timedelta(days=7)\n",
    "            window_15d_start = session_time - timedelta(days=15)\n",
    "            \n",
    "            data_14d = user_data_before[(user_data_before['time'] >= window_14d_start)]\n",
    "            data_7d = user_data_before[(user_data_before['time'] >= window_7d_start)]\n",
    "            data_week2 = user_data_before[(user_data_before['time'] >= window_14d_start) & \n",
    "                                          (user_data_before['time'] < window_7d_start)]\n",
    "            data_15d = user_data_before[(user_data_before['time'] >= window_15d_start)]\n",
    "            data_lifetime = user_data_before\n",
    "            \n",
    "            first_activity = data_lifetime['time'].min()\n",
    "            lifetime_days = (session_time - first_activity).days + 1\n",
    "            \n",
    "            # ============================================================\n",
    "            # FEATURES ORIGINALES (OPTIMISÉES)\n",
    "            # ============================================================\n",
    "            \n",
    "            # 1. negative_actions_last7d_vs_avg\n",
    "            thumbs_down_7d = data_7d['is_thumbs_down'].sum()\n",
    "            negative_7d = thumbs_down_7d + data_7d['is_error'].sum() + data_7d['is_help'].sum() + data_7d['is_settings'].sum()\n",
    "            negative_lifetime = (data_lifetime['is_thumbs_down'].sum() + \n",
    "                                data_lifetime['is_error'].sum() + \n",
    "                                data_lifetime['is_help'].sum() + \n",
    "                                data_lifetime['is_settings'].sum())\n",
    "            actions_7d = len(data_7d)\n",
    "            actions_lifetime = len(data_lifetime)\n",
    "            negative_7d_norm = negative_7d / max(actions_7d, 1)\n",
    "            negative_lifetime_norm = negative_lifetime / max(actions_lifetime, 1)\n",
    "            negative_actions_last7d_vs_avg = negative_7d_norm / max(negative_lifetime_norm, 0.001)\n",
    "            \n",
    "            # 2. consistency_score (SIMPLIFIÉ - pas de date_range)\n",
    "            if len(data_14d) > 0:\n",
    "                daily_sessions = data_14d.groupby('date')['sessionId'].nunique()\n",
    "                mean_sessions = daily_sessions.mean()\n",
    "                std_sessions = daily_sessions.std()\n",
    "                consistency_score = std_sessions / mean_sessions if mean_sessions > 0 else 0.0\n",
    "            else:\n",
    "                consistency_score = 0.0\n",
    "            \n",
    "            # 3. consecutive_days_inactive (VECTORISÉ)\n",
    "            if len(data_14d) > 0:\n",
    "                active_dates_14d = sorted(data_14d['date'].unique())\n",
    "                if len(active_dates_14d) > 1:\n",
    "                    # Calculer les gaps entre dates actives\n",
    "                    active_dates_series = pd.Series(active_dates_14d)\n",
    "                    date_diffs = active_dates_series.diff().dt.days\n",
    "                    consecutive_days_inactive = int(date_diffs.max() - 1) if len(date_diffs) > 0 else 0\n",
    "                    consecutive_days_inactive = max(0, consecutive_days_inactive)\n",
    "                else:\n",
    "                    consecutive_days_inactive = 0\n",
    "            else:\n",
    "                consecutive_days_inactive = 14\n",
    "            \n",
    "            # 4. session_frequency_change\n",
    "            sessions_week1 = data_7d['sessionId'].nunique()\n",
    "            sessions_week2 = data_week2['sessionId'].nunique()\n",
    "            session_frequency_change = sessions_week1 / max(sessions_week2, 1)\n",
    "            \n",
    "            # 5. sessions_last7d_vs_avg\n",
    "            sessions_7d = data_7d['sessionId'].nunique()\n",
    "            sessions_lifetime = data_lifetime['sessionId'].nunique()\n",
    "            sessions_lifetime_avg = sessions_lifetime / max(lifetime_days, 1)\n",
    "            sessions_7d_avg = sessions_7d / 7\n",
    "            sessions_last7d_vs_avg = sessions_7d_avg / max(sessions_lifetime_avg, 0.01)\n",
    "            \n",
    "            # 6. thumbs_down_last_14days\n",
    "            thumbs_down_last_14days = data_14d['is_thumbs_down'].sum()\n",
    "            \n",
    "            # 7. days_without_thumbs_up\n",
    "            thumbs_up_data = data_lifetime[data_lifetime['is_thumbs_up'] == 1]\n",
    "            if len(thumbs_up_data) > 0:\n",
    "                last_thumbs_up = thumbs_up_data['time'].max()\n",
    "                days_without_thumbs_up = (session_time - last_thumbs_up).days\n",
    "            else:\n",
    "                days_without_thumbs_up = (session_time - registration).days\n",
    "            \n",
    "            # 8. has_downgrade_last_15days\n",
    "            has_downgrade_last_15days = int(data_15d['is_downgrade'].sum() > 0)\n",
    "            \n",
    "            # 9. activity_trend_last_14days\n",
    "            songs_week1 = data_7d['is_nextsong'].sum()\n",
    "            songs_week2 = data_week2['is_nextsong'].sum()\n",
    "            activity_trend_last_14days = songs_week1 / max(songs_week2, 1)\n",
    "            \n",
    "            # 10. songs_listened_last_14days\n",
    "            songs_listened_last_14days = data_14d['is_nextsong'].sum()\n",
    "            \n",
    "            # 11. help_visits_14d\n",
    "            help_visits_14d = data_14d['is_help'].sum()\n",
    "            \n",
    "            # 12. error_rate_14d\n",
    "            errors_14d = data_14d['is_error'].sum()\n",
    "            total_actions_14d = len(data_14d)\n",
    "            error_rate_14d = errors_14d / max(total_actions_14d, 1)\n",
    "            \n",
    "            # 13. settings_visits_14d\n",
    "            settings_visits_14d = data_14d['is_settings'].sum()\n",
    "            \n",
    "            # 14. frustration_score\n",
    "            frustration_score = (\n",
    "                thumbs_down_last_14days * 2.0 +\n",
    "                help_visits_14d * 1.5 +\n",
    "                settings_visits_14d * 1.5 +\n",
    "                has_downgrade_last_15days * 3.0\n",
    "            )\n",
    "            \n",
    "            # 15. is_paid\n",
    "            current_level = data_lifetime.iloc[-1]['level'] if len(data_lifetime) > 0 else 'free'\n",
    "            is_paid = 1 if current_level == 'paid' else 0\n",
    "            \n",
    "            # 16. has_downgraded (VECTORISÉ)\n",
    "            levels_series = data_lifetime.sort_values('time')['level']\n",
    "            level_changes = levels_series != levels_series.shift()\n",
    "            transitions = levels_series[level_changes]\n",
    "            has_downgraded = 0\n",
    "            if len(transitions) > 1:\n",
    "                for i in range(len(transitions) - 1):\n",
    "                    if transitions.iloc[i] == 'paid' and transitions.iloc[i+1] == 'free':\n",
    "                        has_downgraded = 1\n",
    "                        break\n",
    "            \n",
    "            # 17. mobile_usage_ratio_14d\n",
    "            mobile_actions_14d = data_14d['is_mobile_action'].sum()\n",
    "            mobile_usage_ratio_14d = mobile_actions_14d / max(total_actions_14d, 1)\n",
    "            \n",
    "            # 18. days_since_registration\n",
    "            days_since_registration = (session_time - registration).days\n",
    "            \n",
    "            # 19. has_ever_paid\n",
    "            has_ever_paid = 1 if (data_lifetime['level'] == 'paid').any() else 0\n",
    "            \n",
    "            # ============================================================\n",
    "            # NOUVELLES FEATURES (5 au lieu de 6)\n",
    "            # ============================================================\n",
    "            \n",
    "            # 20. thumbs_down_lifetime\n",
    "            thumbs_down_lifetime = data_lifetime['is_thumbs_down'].sum()\n",
    "            \n",
    "            # 21. thumbs_up_lifetime\n",
    "            thumbs_up_lifetime = data_lifetime['is_thumbs_up'].sum()\n",
    "            \n",
    "            # 22. frustration_score_lifetime\n",
    "            help_visits_lifetime = data_lifetime['is_help'].sum()\n",
    "            settings_visits_lifetime = data_lifetime['is_settings'].sum()\n",
    "            has_ever_downgraded = 1 if data_lifetime['is_downgrade'].sum() > 0 else 0\n",
    "            \n",
    "            frustration_score_lifetime = (\n",
    "                thumbs_down_lifetime * 2.0 +\n",
    "                help_visits_lifetime * 1.5 +\n",
    "                settings_visits_lifetime * 1.5 +\n",
    "                has_ever_downgraded * 3.0\n",
    "            )\n",
    "            \n",
    "            # 23. listening_time_ratio_7d_vs_lifetime (SIMPLIFIÉ - ratio des totaux)\n",
    "            # Au lieu de moyennes par session, ratio des temps totaux\n",
    "            total_listening_7d = data_7d[data_7d['is_nextsong'] == 1]['length'].sum()\n",
    "            total_listening_lifetime = data_lifetime[data_lifetime['is_nextsong'] == 1]['length'].sum()\n",
    "            \n",
    "            # Normaliser par nombre de jours\n",
    "            listening_per_day_7d = total_listening_7d / 7\n",
    "            listening_per_day_lifetime = total_listening_lifetime / max(lifetime_days, 1)\n",
    "            \n",
    "            listening_time_ratio_7d_vs_lifetime = listening_per_day_7d / max(listening_per_day_lifetime, 1.0)\n",
    "            \n",
    "            # Feature 24 (avg_listening_time_per_session_last7d) fusionnée dans 23\n",
    "            # Feature 25 (avg_time_in_session_without_music_14d) ENLEVÉE (trop lente)\n",
    "            \n",
    "            # ============================================================\n",
    "            # ASSEMBLER (24 features)\n",
    "            # ============================================================\n",
    "            \n",
    "            features = {\n",
    "                'userId': user_id,\n",
    "                'sessionId': session_id,\n",
    "                'session_time': session_time,\n",
    "                'negative_actions_last7d_vs_avg': negative_actions_last7d_vs_avg,\n",
    "                'consistency_score': consistency_score,\n",
    "                'consecutive_days_inactive': consecutive_days_inactive,\n",
    "                'session_frequency_change': session_frequency_change,\n",
    "                'sessions_last7d_vs_avg': sessions_last7d_vs_avg,\n",
    "                'thumbs_down_last_14days': int(thumbs_down_last_14days),\n",
    "                'days_without_thumbs_up': days_without_thumbs_up,\n",
    "                'has_downgrade_last_15days': has_downgrade_last_15days,\n",
    "                'activity_trend_last_14days': activity_trend_last_14days,\n",
    "                'songs_listened_last_14days': int(songs_listened_last_14days),\n",
    "                'help_visits_14d': int(help_visits_14d),\n",
    "                'error_rate_14d': error_rate_14d,\n",
    "                'settings_visits_14d': int(settings_visits_14d),\n",
    "                'frustration_score': frustration_score,\n",
    "                'is_paid': is_paid,\n",
    "                'has_downgraded': has_downgraded,\n",
    "                'mobile_usage_ratio_14d': mobile_usage_ratio_14d,\n",
    "                'days_since_registration': days_since_registration,\n",
    "                'has_ever_paid': has_ever_paid,\n",
    "                'thumbs_down_lifetime': int(thumbs_down_lifetime),\n",
    "                'thumbs_up_lifetime': int(thumbs_up_lifetime),\n",
    "                'frustration_score_lifetime': frustration_score_lifetime,\n",
    "                'listening_time_ratio_7d_vs_lifetime': listening_time_ratio_7d_vs_lifetime\n",
    "            }\n",
    "            \n",
    "            batch_features.append(features)\n",
    "        \n",
    "        if batch_features:\n",
    "            batch_df = pd.DataFrame(batch_features)\n",
    "            all_features.append(batch_df)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ ({len(batch_features)} sessions)\")\n",
    "        \n",
    "        del batch_features\n",
    "        gc.collect()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # COMBINER\n",
    "    # ========================================================================\n",
    "    \n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"Combinaison finale...\")\n",
    "    \n",
    "    final_df = pd.concat(all_features, ignore_index=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"=\"*80)\n",
    "        print(\"✅ 24 FEATURES PAR SESSION CRÉÉES (ULTRA-OPTIMISÉ)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Shape: {final_df.shape}\")\n",
    "        print(f\"Sessions par user (moyenne): {len(final_df) / final_df['userId'].nunique():.1f}\")\n",
    "        print()\n",
    "        print(\"OPTIMISATIONS:\")\n",
    "        print(\"  ✅ consistency_score simplifié (pas de date_range)\")\n",
    "        print(\"  ✅ consecutive_days_inactive vectorisé\")\n",
    "        print(\"  ✅ has_downgraded vectorisé\")\n",
    "        print(\"  ✅ listening_time_ratio_7d_vs_lifetime simplifié (ratio totaux)\")\n",
    "        print(\"  ❌ avg_time_in_session_without_music_14d enlevée (trop lente)\")\n",
    "        print()\n",
    "        print(final_df.head())\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b192a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CALCUL DES 24 FEATURES PAR SESSION (VERSION ULTRA-OPTIMISÉE)\n",
      "================================================================================\n",
      "\n",
      "Création des flags...\n",
      "Dataset: (14471537, 17)\n",
      "Users: 18048\n",
      "Sessions: 137078\n",
      "\n",
      "Identification des sessions...\n",
      "Total sessions: 175,059\n",
      "Batch size: 10,000\n",
      "Nombre de batches: 18\n",
      "\n",
      "Pré-calcul des groupes par user...\n",
      "Batch 1/18... ✓ (10000 sessions)\n",
      "Batch 2/18... ✓ (10000 sessions)\n",
      "Batch 3/18... ✓ (10000 sessions)\n",
      "Batch 4/18... ✓ (10000 sessions)\n",
      "Batch 5/18... ✓ (10000 sessions)\n",
      "Batch 6/18... ✓ (10000 sessions)\n",
      "Batch 7/18... ✓ (10000 sessions)\n",
      "Batch 8/18... ✓ (10000 sessions)\n",
      "Batch 9/18... ✓ (10000 sessions)\n",
      "Batch 10/18... ✓ (10000 sessions)\n",
      "Batch 11/18... ✓ (10000 sessions)\n",
      "Batch 12/18... ✓ (10000 sessions)\n",
      "Batch 13/18... ✓ (10000 sessions)\n",
      "Batch 14/18... ✓ (10000 sessions)\n",
      "Batch 15/18... ✓ (10000 sessions)\n",
      "Batch 16/18... ✓ (10000 sessions)\n",
      "Batch 17/18... ✓ (10000 sessions)\n",
      "Batch 18/18... ✓ (5059 sessions)\n",
      "\n",
      "Combinaison finale...\n",
      "\n",
      "================================================================================\n",
      "✅ 24 FEATURES PAR SESSION CRÉÉES (ULTRA-OPTIMISÉ)\n",
      "================================================================================\n",
      "Shape: (175059, 26)\n",
      "Sessions par user (moyenne): 9.7\n",
      "\n",
      "OPTIMISATIONS:\n",
      "  ✅ consistency_score simplifié (pas de date_range)\n",
      "  ✅ consecutive_days_inactive vectorisé\n",
      "  ✅ has_downgraded vectorisé\n",
      "  ✅ listening_time_ratio_7d_vs_lifetime simplifié (ratio totaux)\n",
      "  ❌ avg_time_in_session_without_music_14d enlevée (trop lente)\n",
      "\n",
      "    userId  sessionId        session_time  negative_actions_last7d_vs_avg  \\\n",
      "0  1000025      23706 2018-10-02 08:59:29                             1.0   \n",
      "1  1000025      31688 2018-10-02 18:12:22                             1.0   \n",
      "2  1000025      39243 2018-10-04 01:04:35                             1.0   \n",
      "3  1000025      42490 2018-10-05 01:36:46                             1.0   \n",
      "4  1000025      45191 2018-10-06 22:09:33                             1.0   \n",
      "\n",
      "   consistency_score  consecutive_days_inactive  session_frequency_change  \\\n",
      "0           0.000000                          0                       1.0   \n",
      "1                NaN                          0                       1.0   \n",
      "2           0.471405                          0                       2.0   \n",
      "3           0.433013                          0                       3.0   \n",
      "4           0.400000                          0                       4.0   \n",
      "\n",
      "   sessions_last7d_vs_avg  thumbs_down_last_14days  days_without_thumbs_up  \\\n",
      "0                1.000000                        0                      83   \n",
      "1                0.142857                        0                       0   \n",
      "2                0.285714                        5                       0   \n",
      "3                0.428571                        8                       0   \n",
      "4                0.714286                        8                       2   \n",
      "\n",
      "   ...  frustration_score  is_paid  has_downgraded  mobile_usage_ratio_14d  \\\n",
      "0  ...                0.0        0               0                     0.5   \n",
      "1  ...                1.5        0               0                     0.0   \n",
      "2  ...               19.0        1               0                     0.0   \n",
      "3  ...               26.5        1               0                     0.0   \n",
      "4  ...               26.5        1               0                     0.0   \n",
      "\n",
      "   days_since_registration  has_ever_paid  thumbs_down_lifetime  \\\n",
      "0                       83              0                     0   \n",
      "1                       84              0                     0   \n",
      "2                       85              1                     5   \n",
      "3                       86              1                     8   \n",
      "4                       88              1                     8   \n",
      "\n",
      "   thumbs_up_lifetime  frustration_score_lifetime  \\\n",
      "0                   0                         0.0   \n",
      "1                   1                         1.5   \n",
      "2                  26                        19.0   \n",
      "3                  38                        26.5   \n",
      "4                  38                        26.5   \n",
      "\n",
      "   listening_time_ratio_7d_vs_lifetime  \n",
      "0                             1.000000  \n",
      "1                             0.142857  \n",
      "2                             0.285714  \n",
      "3                             0.428571  \n",
      "4                             0.714286  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "best_features = create_features_per_session_optimized(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd307c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features.to_csv('best_features.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb49f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction améliorée pour calculer les préférences musicales par user\n",
    "+ Encodage pour le ML\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAPPING DES GENRES\n",
    "# ============================================================================\n",
    "\n",
    "GENRE_MAPPING = {\n",
    "    'Kings Of Leon': 'Rock Alternatif',\n",
    "    'Coldplay': 'Pop Rock',\n",
    "    'Florence + The Machine': 'Indie Pop',\n",
    "    'The Black Keys': 'Blues Rock',\n",
    "    'Björk': 'Electronic',\n",
    "    'BjÃƒÂ¶rk': 'Electronic',\n",
    "    'Muse': 'Rock Alternatif',\n",
    "    'Jack Johnson': 'Folk Rock',\n",
    "    'Dwight Yoakam': 'Country',\n",
    "    'Justin Bieber': 'Pop',\n",
    "    'Train': 'Pop Rock',\n",
    "    'Eminem': 'Hip-Hop',\n",
    "    'Radiohead': 'Rock Alternatif',\n",
    "    'Taylor Swift': 'Pop',\n",
    "    'Alliance Ethnik': 'Hip-Hop',\n",
    "    'The Killers': 'Rock Alternatif',\n",
    "    'Linkin Park': 'Metal Alternatif',\n",
    "    'OneRepublic': 'Pop Rock',\n",
    "    'Metallica': 'Heavy Metal',\n",
    "    'John Mayer': 'Pop Rock',\n",
    "    'Evanescence': 'Metal Alternatif'\n",
    "}\n",
    "\n",
    "\n",
    "def create_user_preferences_improved(\n",
    "    df: pd.DataFrame, \n",
    "    genre_map: dict = GENRE_MAPPING,\n",
    "    encode: bool = True,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule les préférences musicales par user de manière robuste\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame avec colonnes userId, page, artist, song\n",
    "        genre_map: Dictionnaire {artiste: genre}\n",
    "        encode: Si True, encode les variables catégorielles pour ML\n",
    "        verbose: Afficher les statistiques\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame avec userId + préférences (encodées si encode=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*80)\n",
    "        print(\"CALCUL DES PRÉFÉRENCES MUSICALES PAR USER\")\n",
    "        print(\"=\"*80)\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # VALIDATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    required_cols = ['userId', 'page', 'artist', 'song']\n",
    "    missing = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}. Disponibles: {df.columns.tolist()}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FILTRAGE ET NETTOYAGE\n",
    "    # ========================================================================\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Dataset initial: {df.shape}\")\n",
    "    \n",
    "    # Filtrer NextSong uniquement\n",
    "    df_songs = df[df['page'] == 'NextSong'].copy()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Événements NextSong: {len(df_songs):,}\")\n",
    "    \n",
    "    # Nettoyer les NaN\n",
    "    before_clean = len(df_songs)\n",
    "    df_songs = df_songs.dropna(subset=['artist', 'song', 'userId'])\n",
    "    after_clean = len(df_songs)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Après nettoyage NaN: {after_clean:,} (supprimé: {before_clean - after_clean:,})\")\n",
    "    \n",
    "    # Convertir userId en string\n",
    "    df_songs['userId'] = df_songs['userId'].astype(str)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MAPPING DES GENRES\n",
    "    # ========================================================================\n",
    "    \n",
    "    df_songs['genre'] = df_songs['artist'].map(genre_map).fillna('Autre')\n",
    "    \n",
    "    if verbose:\n",
    "        genre_dist = df_songs['genre'].value_counts()\n",
    "        print(f\"\\nDistribution des genres:\")\n",
    "        print(genre_dist.head(10))\n",
    "        print(f\"Artistes 'Autre' (non mappés): {(df_songs['genre'] == 'Autre').sum():,}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CALCUL DES PRÉFÉRENCES PAR USER\n",
    "    # ========================================================================\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Calcul des préférences par user...\")\n",
    "    \n",
    "    # Fonction robuste pour trouver le mode\n",
    "    def safe_mode(series):\n",
    "        \"\"\"Retourne le mode, ou None si vide\"\"\"\n",
    "        if series.empty:\n",
    "            return None\n",
    "        modes = series.mode()\n",
    "        return modes.iloc[0] if len(modes) > 0 else None\n",
    "    \n",
    "    # Groupby et agrégation\n",
    "    favorites = df_songs.groupby('userId').agg(\n",
    "        favorite_artist=('artist', safe_mode),\n",
    "        favorite_song=('song', safe_mode),\n",
    "        favorite_genre=('genre', safe_mode),\n",
    "        total_songs_listened=('song', 'count'),  # Bonus: volume d'écoute\n",
    "        unique_artists=('artist', 'nunique'),     # Bonus: diversité artistes\n",
    "        unique_genres=('genre', 'nunique')        # Bonus: diversité genres\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Gérer les None (users sans données valides)\n",
    "    favorites['favorite_artist'] = favorites['favorite_artist'].fillna('Unknown')\n",
    "    favorites['favorite_song'] = favorites['favorite_song'].fillna('Unknown')\n",
    "    favorites['favorite_genre'] = favorites['favorite_genre'].fillna('Unknown')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Users avec préférences: {len(favorites):,}\")\n",
    "        print()\n",
    "        print(\"Statistiques:\")\n",
    "        print(f\"  Moyenne songs écoutées: {favorites['total_songs_listened'].mean():.1f}\")\n",
    "        print(f\"  Moyenne artistes uniques: {favorites['unique_artists'].mean():.1f}\")\n",
    "        print(f\"  Moyenne genres uniques: {favorites['unique_genres'].mean():.1f}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # ENCODAGE POUR ML (si demandé)\n",
    "    # ========================================================================\n",
    "    \n",
    "    if encode:\n",
    "        if verbose:\n",
    "            print(\"Encodage des variables catégorielles...\")\n",
    "        \n",
    "        # Option 1: Label Encoding (compact, bon pour tree-based)\n",
    "        le_artist = LabelEncoder()\n",
    "        le_song = LabelEncoder()\n",
    "        le_genre = LabelEncoder()\n",
    "        \n",
    "        favorites['favorite_artist_encoded'] = le_artist.fit_transform(favorites['favorite_artist'])\n",
    "        favorites['favorite_song_encoded'] = le_song.fit_transform(favorites['favorite_song'])\n",
    "        favorites['favorite_genre_encoded'] = le_genre.fit_transform(favorites['favorite_genre'])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  favorite_artist: {len(le_artist.classes_)} classes\")\n",
    "            print(f\"  favorite_song: {len(le_song.classes_)} classes\")\n",
    "            print(f\"  favorite_genre: {len(le_genre.classes_)} classes\")\n",
    "            print()\n",
    "        \n",
    "        # Garder aussi les versions non-encodées pour référence\n",
    "        # (utile pour l'interprétation)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*80)\n",
    "        print(\"✅ PRÉFÉRENCES CALCULÉES\")\n",
    "        print(\"=\"*80)\n",
    "        print()\n",
    "        print(favorites.head(10))\n",
    "        print()\n",
    "    \n",
    "    return favorites\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FONCTION ALTERNATIVE : ONE-HOT ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "def create_user_preferences_onehot(\n",
    "    df: pd.DataFrame,\n",
    "    genre_map: dict = GENRE_MAPPING,\n",
    "    top_n_artists: int = 50,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Version avec One-Hot Encoding (meilleur pour genres, plus lourd pour artistes)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame source\n",
    "        genre_map: Mapping artistes → genres\n",
    "        top_n_artists: Garder seulement les N artistes les plus populaires\n",
    "        verbose: Afficher stats\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame avec userId + préférences one-hot encodées\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculer préférences de base\n",
    "    favorites = create_user_preferences_improved(df, genre_map, encode=False, verbose=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"One-Hot Encoding...\")\n",
    "    \n",
    "    # One-hot pour genres (peu de classes)\n",
    "    genre_dummies = pd.get_dummies(favorites['favorite_genre'], prefix='genre')\n",
    "    \n",
    "    # Pour artistes : garder seulement top N (sinon trop de colonnes)\n",
    "    top_artists = favorites['favorite_artist'].value_counts().head(top_n_artists).index\n",
    "    favorites['favorite_artist_grouped'] = favorites['favorite_artist'].apply(\n",
    "        lambda x: x if x in top_artists else 'Other'\n",
    "    )\n",
    "    artist_dummies = pd.get_dummies(favorites['favorite_artist_grouped'], prefix='artist')\n",
    "    \n",
    "    # Combiner\n",
    "    result = pd.concat([\n",
    "        favorites[['userId', 'total_songs_listened', 'unique_artists', 'unique_genres']],\n",
    "        genre_dummies,\n",
    "        artist_dummies\n",
    "    ], axis=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nShape finale: {result.shape}\")\n",
    "        print(f\"Colonnes one-hot créées: {result.shape[1] - 4}\")\n",
    "        print()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41f743ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"data/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc4aebfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CALCUL DES PRÉFÉRENCES MUSICALES PAR USER\n",
      "================================================================================\n",
      "\n",
      "Dataset initial: (17499636, 19)\n",
      "Événements NextSong: 14,291,433\n",
      "Après nettoyage NaN: 14,291,433 (supprimé: 0)\n",
      "\n",
      "Distribution des genres:\n",
      "genre\n",
      "Autre               13070062\n",
      "Rock Alternatif       293851\n",
      "Pop Rock              264277\n",
      "Pop                   120492\n",
      "Hip-Hop               111583\n",
      "Metal Alternatif       97803\n",
      "Indie Pop              79062\n",
      "Country                73391\n",
      "Blues Rock             70511\n",
      "Folk Rock              64713\n",
      "Name: count, dtype: int64\n",
      "Artistes 'Autre' (non mappés): 13,070,062\n",
      "\n",
      "Calcul des préférences par user...\n",
      "Users avec préférences: 19,117\n",
      "\n",
      "Statistiques:\n",
      "  Moyenne songs écoutées: 747.6\n",
      "  Moyenne artistes uniques: 535.8\n",
      "  Moyenne genres uniques: 8.9\n",
      "\n",
      "Encodage des variables catégorielles...\n",
      "  favorite_artist: 944 classes\n",
      "  favorite_song: 2358 classes\n",
      "  favorite_genre: 4 classes\n",
      "\n",
      "================================================================================\n",
      "✅ PRÉFÉRENCES CALCULÉES\n",
      "================================================================================\n",
      "\n",
      "    userId               favorite_artist                   favorite_song  \\\n",
      "0  1000025                      BjÃÂ¶rk                            Undo   \n",
      "1  1000035                 Kings Of Leon                         Revelry   \n",
      "2  1000083  Creedence Clearwater Revival                         Invalid   \n",
      "3  1000103                      Coldplay   (Kom SÃÂ¥ Ska Vi) Leva Livet   \n",
      "4  1000164               Alliance Ethnik                            Undo   \n",
      "5  1000168               Alliance Ethnik  Dog Days Are Over (Radio Edit)   \n",
      "6  1000182                      Coldplay                            Undo   \n",
      "7  1000194               Alliance Ethnik                         Revelry   \n",
      "8  1000214                      Coldplay                   ReprÃÂ©sente   \n",
      "9  1000233                    John Mayer                Ain't Misbehavin   \n",
      "\n",
      "  favorite_genre  total_songs_listened  unique_artists  unique_genres  \\\n",
      "0          Autre                  1662            1162             11   \n",
      "1          Autre                  1266             916             11   \n",
      "2          Autre                   501             427             10   \n",
      "3          Autre                    57              56              5   \n",
      "4          Autre                   847             660             11   \n",
      "5          Autre                   545             451             11   \n",
      "6          Autre                   357             317              9   \n",
      "7          Autre                    75              73              5   \n",
      "8          Autre                  1364             993             11   \n",
      "9          Autre                    51              49              5   \n",
      "\n",
      "   favorite_artist_encoded  favorite_song_encoded  favorite_genre_encoded  \n",
      "0                      405                   2299                       0  \n",
      "1                      750                   2164                       0  \n",
      "2                      539                   1925                       0  \n",
      "3                      526                     73                       0  \n",
      "4                      220                   2299                       0  \n",
      "5                      220                   1566                       0  \n",
      "6                      526                   2299                       0  \n",
      "7                      220                   2164                       0  \n",
      "8                      526                   2160                       0  \n",
      "9                      721                    748                       0  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bk/qpt56ww928z8p47c40hrn6j00000gn/T/ipykernel_35979/1052498402.py:43: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  features_final['favorite_genre_encoded'].fillna(-1, inplace=True)\n",
      "/var/folders/bk/qpt56ww928z8p47c40hrn6j00000gn/T/ipykernel_35979/1052498402.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  features_final['favorite_artist_encoded'].fillna(-1, inplace=True)\n",
      "/var/folders/bk/qpt56ww928z8p47c40hrn6j00000gn/T/ipykernel_35979/1052498402.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  features_final['total_songs_listened'].fillna(0, inplace=True)\n",
      "/var/folders/bk/qpt56ww928z8p47c40hrn6j00000gn/T/ipykernel_35979/1052498402.py:46: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  features_final['unique_artists'].fillna(0, inplace=True)\n",
      "/var/folders/bk/qpt56ww928z8p47c40hrn6j00000gn/T/ipykernel_35979/1052498402.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  features_final['unique_genres'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape finale: (175059, 33)\n",
      "    userId  sessionId        session_time  negative_actions_last7d_vs_avg  \\\n",
      "0  1000025      23706 2018-10-02 08:59:29                             1.0   \n",
      "1  1000025      31688 2018-10-02 18:12:22                             1.0   \n",
      "2  1000025      39243 2018-10-04 01:04:35                             1.0   \n",
      "3  1000025      42490 2018-10-05 01:36:46                             1.0   \n",
      "4  1000025      45191 2018-10-06 22:09:33                             1.0   \n",
      "\n",
      "   consistency_score  consecutive_days_inactive  session_frequency_change  \\\n",
      "0           0.000000                          0                       1.0   \n",
      "1                NaN                          0                       1.0   \n",
      "2           0.471405                          0                       2.0   \n",
      "3           0.433013                          0                       3.0   \n",
      "4           0.400000                          0                       4.0   \n",
      "\n",
      "   sessions_last7d_vs_avg  thumbs_down_last_14days  days_without_thumbs_up  \\\n",
      "0                1.000000                        0                      83   \n",
      "1                0.142857                        0                       0   \n",
      "2                0.285714                        5                       0   \n",
      "3                0.428571                        8                       0   \n",
      "4                0.714286                        8                       2   \n",
      "\n",
      "   ...  thumbs_up_lifetime  frustration_score_lifetime  \\\n",
      "0  ...                   0                         0.0   \n",
      "1  ...                   1                         1.5   \n",
      "2  ...                  26                        19.0   \n",
      "3  ...                  38                        26.5   \n",
      "4  ...                  38                        26.5   \n",
      "\n",
      "   listening_time_ratio_7d_vs_lifetime  favorite_genre_encoded  \\\n",
      "0                             1.000000                     0.0   \n",
      "1                             0.142857                     0.0   \n",
      "2                             0.285714                     0.0   \n",
      "3                             0.428571                     0.0   \n",
      "4                             0.714286                     0.0   \n",
      "\n",
      "   favorite_artist_encoded  total_songs_listened  unique_artists  \\\n",
      "0                    405.0                1662.0          1162.0   \n",
      "1                    405.0                1662.0          1162.0   \n",
      "2                    405.0                1662.0          1162.0   \n",
      "3                    405.0                1662.0          1162.0   \n",
      "4                    405.0                1662.0          1162.0   \n",
      "\n",
      "   unique_genres  favorite_genre  favorite_artist  \n",
      "0           11.0           Autre         BjÃÂ¶rk  \n",
      "1           11.0           Autre         BjÃÂ¶rk  \n",
      "2           11.0           Autre         BjÃÂ¶rk  \n",
      "3           11.0           Autre         BjÃÂ¶rk  \n",
      "4           11.0           Autre         BjÃÂ¶rk  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "✅ Sauvegardé avec préférences musicales encodées\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour aggréger les deux dataset de features\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CHARGER DONNÉES\n",
    "# ============================================================================\n",
    "\n",
    "df_full = train.copy()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CALCULER PRÉFÉRENCES MUSICALES\n",
    "# ============================================================================\n",
    "\n",
    "music_prefs = create_user_preferences_improved(\n",
    "    df_full,\n",
    "    encode=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. MERGER AVEC LES FEATURES DE SESSIONS\n",
    "# ============================================================================\n",
    "\n",
    "features_sessions = best_features\n",
    "\n",
    "features_final = features_sessions.merge(\n",
    "    music_prefs[[\n",
    "        'userId', \n",
    "        'favorite_genre_encoded',      # Pour ML\n",
    "        'favorite_artist_encoded',     # Pour ML\n",
    "        'total_songs_listened',        # Volume\n",
    "        'unique_artists',              # Diversité\n",
    "        'unique_genres',               # Diversité\n",
    "        'favorite_genre',              # Pour interprétation (garder aussi le texte)\n",
    "        'favorite_artist'              # Pour interprétation\n",
    "    ]], \n",
    "    on='userId', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remplir les NaN (users sans préférences)\n",
    "features_final['favorite_genre_encoded'].fillna(-1, inplace=True)\n",
    "features_final['favorite_artist_encoded'].fillna(-1, inplace=True)\n",
    "features_final['total_songs_listened'].fillna(0, inplace=True)\n",
    "features_final['unique_artists'].fillna(0, inplace=True)\n",
    "features_final['unique_genres'].fillna(0, inplace=True)\n",
    "\n",
    "print(f\"Shape finale: {features_final.shape}\")\n",
    "print(features_final.head())\n",
    "\n",
    "# ============================================================================\n",
    "# 4. SAUVEGARDER\n",
    "# ============================================================================\n",
    "\n",
    "features_final.to_csv('features_with_music_encoded.csv', index=False)\n",
    "print(\"✅ Sauvegardé avec préférences musicales encodées\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d9470ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>session_time</th>\n",
       "      <th>negative_actions_last7d_vs_avg</th>\n",
       "      <th>consistency_score</th>\n",
       "      <th>consecutive_days_inactive</th>\n",
       "      <th>session_frequency_change</th>\n",
       "      <th>sessions_last7d_vs_avg</th>\n",
       "      <th>thumbs_down_last_14days</th>\n",
       "      <th>days_without_thumbs_up</th>\n",
       "      <th>...</th>\n",
       "      <th>has_ever_paid</th>\n",
       "      <th>thumbs_down_lifetime</th>\n",
       "      <th>thumbs_up_lifetime</th>\n",
       "      <th>frustration_score_lifetime</th>\n",
       "      <th>listening_time_ratio_7d_vs_lifetime</th>\n",
       "      <th>favorite_genre_encoded</th>\n",
       "      <th>favorite_artist_encoded</th>\n",
       "      <th>total_songs_listened</th>\n",
       "      <th>unique_artists</th>\n",
       "      <th>unique_genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>23706</td>\n",
       "      <td>2018-10-02 08:59:29</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000025</td>\n",
       "      <td>31688</td>\n",
       "      <td>2018-10-02 18:12:22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000025</td>\n",
       "      <td>39243</td>\n",
       "      <td>2018-10-04 01:04:35</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000025</td>\n",
       "      <td>42490</td>\n",
       "      <td>2018-10-05 01:36:46</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000025</td>\n",
       "      <td>45191</td>\n",
       "      <td>2018-10-06 22:09:33</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175054</th>\n",
       "      <td>1999905</td>\n",
       "      <td>94383</td>\n",
       "      <td>2018-10-20 15:00:08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.250745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175055</th>\n",
       "      <td>1999905</td>\n",
       "      <td>105570</td>\n",
       "      <td>2018-10-23 13:25:14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.353185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175056</th>\n",
       "      <td>1999905</td>\n",
       "      <td>115721</td>\n",
       "      <td>2018-10-26 06:51:51</td>\n",
       "      <td>0.622857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.935355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175057</th>\n",
       "      <td>1999905</td>\n",
       "      <td>126577</td>\n",
       "      <td>2018-10-26 15:25:08</td>\n",
       "      <td>0.498990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.178571</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.231033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175058</th>\n",
       "      <td>1999905</td>\n",
       "      <td>127830</td>\n",
       "      <td>2018-10-31 15:37:27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.601064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175059 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userId  sessionId        session_time  \\\n",
       "0       1000025      23706 2018-10-02 08:59:29   \n",
       "1       1000025      31688 2018-10-02 18:12:22   \n",
       "2       1000025      39243 2018-10-04 01:04:35   \n",
       "3       1000025      42490 2018-10-05 01:36:46   \n",
       "4       1000025      45191 2018-10-06 22:09:33   \n",
       "...         ...        ...                 ...   \n",
       "175054  1999905      94383 2018-10-20 15:00:08   \n",
       "175055  1999905     105570 2018-10-23 13:25:14   \n",
       "175056  1999905     115721 2018-10-26 06:51:51   \n",
       "175057  1999905     126577 2018-10-26 15:25:08   \n",
       "175058  1999905     127830 2018-10-31 15:37:27   \n",
       "\n",
       "        negative_actions_last7d_vs_avg  consistency_score  \\\n",
       "0                             1.000000           0.000000   \n",
       "1                             1.000000                NaN   \n",
       "2                             1.000000           0.471405   \n",
       "3                             1.000000           0.433013   \n",
       "4                             1.000000           0.400000   \n",
       "...                                ...                ...   \n",
       "175054                        0.000000           0.433013   \n",
       "175055                        0.000000           0.000000   \n",
       "175056                        0.622857           0.000000   \n",
       "175057                        0.498990           0.000000   \n",
       "175058                        0.000000           0.400000   \n",
       "\n",
       "        consecutive_days_inactive  session_frequency_change  \\\n",
       "0                               0                  1.000000   \n",
       "1                               0                  1.000000   \n",
       "2                               0                  2.000000   \n",
       "3                               0                  3.000000   \n",
       "4                               0                  4.000000   \n",
       "...                           ...                       ...   \n",
       "175054                          4                  0.333333   \n",
       "175055                          4                  2.000000   \n",
       "175056                          4                  1.000000   \n",
       "175057                          2                  3.000000   \n",
       "175058                          2                  0.666667   \n",
       "\n",
       "        sessions_last7d_vs_avg  thumbs_down_last_14days  \\\n",
       "0                     1.000000                        0   \n",
       "1                     0.142857                        0   \n",
       "2                     0.285714                        5   \n",
       "3                     0.428571                        8   \n",
       "4                     0.714286                        8   \n",
       "...                        ...                      ...   \n",
       "175054                0.457143                        1   \n",
       "175055                0.904762                        1   \n",
       "175056                0.857143                        2   \n",
       "175057                1.178571                        1   \n",
       "175058                0.857143                        1   \n",
       "\n",
       "        days_without_thumbs_up  ...  has_ever_paid  thumbs_down_lifetime  \\\n",
       "0                           83  ...              0                     0   \n",
       "1                            0  ...              0                     0   \n",
       "2                            0  ...              1                     5   \n",
       "3                            0  ...              1                     8   \n",
       "4                            2  ...              1                     8   \n",
       "...                        ...  ...            ...                   ...   \n",
       "175054                      12  ...              0                     2   \n",
       "175055                       2  ...              0                     2   \n",
       "175056                       2  ...              0                     3   \n",
       "175057                       3  ...              0                     3   \n",
       "175058                       4  ...              0                     3   \n",
       "\n",
       "        thumbs_up_lifetime  frustration_score_lifetime  \\\n",
       "0                        0                         0.0   \n",
       "1                        1                         1.5   \n",
       "2                       26                        19.0   \n",
       "3                       38                        26.5   \n",
       "4                       38                        26.5   \n",
       "...                    ...                         ...   \n",
       "175054                   5                         7.0   \n",
       "175055                   6                         7.0   \n",
       "175056                   8                         9.0   \n",
       "175057                   8                         9.0   \n",
       "175058                   9                         9.0   \n",
       "\n",
       "        listening_time_ratio_7d_vs_lifetime  favorite_genre_encoded  \\\n",
       "0                                  1.000000                     0.0   \n",
       "1                                  0.142857                     0.0   \n",
       "2                                  0.285714                     0.0   \n",
       "3                                  0.428571                     0.0   \n",
       "4                                  0.714286                     0.0   \n",
       "...                                     ...                     ...   \n",
       "175054                             0.250745                     0.0   \n",
       "175055                             0.353185                     0.0   \n",
       "175056                             0.935355                     0.0   \n",
       "175057                             1.231033                     0.0   \n",
       "175058                             0.601064                     0.0   \n",
       "\n",
       "        favorite_artist_encoded  total_songs_listened  unique_artists  \\\n",
       "0                         405.0                1662.0          1162.0   \n",
       "1                         405.0                1662.0          1162.0   \n",
       "2                         405.0                1662.0          1162.0   \n",
       "3                         405.0                1662.0          1162.0   \n",
       "4                         405.0                1662.0          1162.0   \n",
       "...                         ...                   ...             ...   \n",
       "175054                     35.0                 259.0           230.0   \n",
       "175055                     35.0                 259.0           230.0   \n",
       "175056                     35.0                 259.0           230.0   \n",
       "175057                     35.0                 259.0           230.0   \n",
       "175058                     35.0                 259.0           230.0   \n",
       "\n",
       "        unique_genres  \n",
       "0                11.0  \n",
       "1                11.0  \n",
       "2                11.0  \n",
       "3                11.0  \n",
       "4                11.0  \n",
       "...               ...  \n",
       "175054           10.0  \n",
       "175055           10.0  \n",
       "175056           10.0  \n",
       "175057           10.0  \n",
       "175058           10.0  \n",
       "\n",
       "[175059 rows x 31 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76e47acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_time = churners[['userId', 'time']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b470f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_clean  = features_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3398cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_clean.to_csv('features_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffa59116",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_time.to_csv('churn_time.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2c149e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>1749042</td>\n",
       "      <td>2018-10-21 01:16:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5923</th>\n",
       "      <td>1222580</td>\n",
       "      <td>2018-10-30 23:17:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17878</th>\n",
       "      <td>1385500</td>\n",
       "      <td>2018-11-17 04:54:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46340</th>\n",
       "      <td>1144647</td>\n",
       "      <td>2018-10-28 02:43:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58317</th>\n",
       "      <td>1240184</td>\n",
       "      <td>2018-11-14 21:43:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17123383</th>\n",
       "      <td>1835694</td>\n",
       "      <td>2018-10-09 14:49:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17167280</th>\n",
       "      <td>1895668</td>\n",
       "      <td>2018-11-15 14:21:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17233170</th>\n",
       "      <td>1353786</td>\n",
       "      <td>2018-11-12 23:15:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17269806</th>\n",
       "      <td>1652329</td>\n",
       "      <td>2018-11-02 17:32:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17299902</th>\n",
       "      <td>1030763</td>\n",
       "      <td>2018-11-18 18:51:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3435 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           userId                time\n",
       "1222      1749042 2018-10-21 01:16:24\n",
       "5923      1222580 2018-10-30 23:17:30\n",
       "17878     1385500 2018-11-17 04:54:09\n",
       "46340     1144647 2018-10-28 02:43:58\n",
       "58317     1240184 2018-11-14 21:43:36\n",
       "...           ...                 ...\n",
       "17123383  1835694 2018-10-09 14:49:14\n",
       "17167280  1895668 2018-11-15 14:21:59\n",
       "17233170  1353786 2018-11-12 23:15:05\n",
       "17269806  1652329 2018-11-02 17:32:12\n",
       "17299902  1030763 2018-11-18 18:51:15\n",
       "\n",
       "[3435 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90a1efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop des colonnes de textes si besoin\n",
    "features_final.drop(columns = ['favorite_genre', 'favorite_artist'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3438812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will_churn_10days\n",
      "0    156402\n",
      "1     18657\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# On rajoute la colonne target\n",
    "import pandas as pd\n",
    "\n",
    "# Charger\n",
    "features = pd.read_csv('features_clean.csv')\n",
    "churn_time = pd.read_csv('churn_time.csv')\n",
    "\n",
    "# Convertir\n",
    "features['session_time'] = pd.to_datetime(features['session_time'])\n",
    "churn_time['time'] = pd.to_datetime(churn_time['time'])\n",
    "churn_time = churn_time.rename(columns={'time': 'churn_date'})\n",
    "\n",
    "# Merger\n",
    "features = features.merge(churn_time[['userId', 'churn_date']], on='userId', how='left')\n",
    "\n",
    "# Target\n",
    "features['days_until_churn'] = (features['churn_date'] - features['session_time']).dt.days\n",
    "features['will_churn_10days'] = ((features['days_until_churn'] >= 0) & \n",
    "                                  (features['days_until_churn'] <= 10)).astype(int)\n",
    "\n",
    "# Drop\n",
    "features = features.drop(['churn_date', 'days_until_churn'], axis=1)\n",
    "\n",
    "print(features['will_churn_10days'].value_counts())\n",
    "features.to_csv('features_with_target.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33114570",
   "metadata": {},
   "source": [
    "### Application of the function to the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656a563",
   "metadata": {},
   "source": [
    "### Remove all previous sessions from churners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd85b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def keep_only_consistent_users(features):\n",
    "    \"\"\"\n",
    "    Conserve uniquement :\n",
    "    - Les utilisateurs qui n'ont JAMAIS churné (tous les will_churn_10days=0).\n",
    "    - Les sessions avec will_churn_10days=1 (provenant des utilisateurs qui churnent).\n",
    "    \n",
    "    Retire : les sessions avec will_churn_10days=0 provenant d'utilisateurs qui churnent ultérieurement.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Calculer le statut de churn maximal pour chaque utilisateur (1 si l'utilisateur a churné au moins une fois)\n",
    "    max_churn_per_user = features.groupby('userId')['will_churn_10days'].transform('max')\n",
    "    \n",
    "    # 2. Créer le masque de filtrage\n",
    "    # On garde les lignes si :\n",
    "    # A) L'utilisateur est un \"non-churner\" (max_churn_per_user == 0)\n",
    "    # OU\n",
    "    # B) La session elle-même est une session de churn (features['will_churn_10days'] == 1)\n",
    "    mask = (max_churn_per_user == 0) | (features['will_churn_10days'] == 1)\n",
    "    \n",
    "    # 3. Appliquer le filtre\n",
    "    features_filtered = features[mask].copy()\n",
    "    \n",
    "    return features_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dab557b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('features_with_target.csv')  # Déjà nettoyé des post-churn\n",
    "df_consistant = keep_only_consistent_users(features)\n",
    "df_consistant.to_csv('df_consistant.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1a49f",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7139dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def XGB_training(df_features):\n",
    "\n",
    "    # Prepare data\n",
    "    feature_cols = [col for col in df_features.columns \n",
    "                    if col not in ['favorite_genre', 'favorite_artist','userId', 'prediction_date','session_start','sessionId', 'session_time','time','registration', 'will_churn_10days']]\n",
    "\n",
    "    X = df_features[feature_cols].copy()\n",
    "    \n",
    "    # Handle missing values differently by column type\n",
    "    # For numeric columns: fill with 0\n",
    "    numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "    X[numeric_cols] = X[numeric_cols].fillna(0)\n",
    "    \n",
    "    # For object/string columns: fill with 'Unknown' then convert to category\n",
    "    object_cols = X.select_dtypes(include=['object']).columns\n",
    "    X[object_cols] = X[object_cols].fillna('Unknown')\n",
    "    for col in object_cols:\n",
    "        X[col] = X[col].astype('category')\n",
    "    \n",
    "    y = df_features['will_churn_10days']\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Calculate scale_pos_weight: ratio of negative to positive class\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "    print(f\"Class distribution in train:\")\n",
    "    print(y_train.value_counts())\n",
    "    print(f\"\\nCalculated scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "    # XGBoost with optimal parameters for churn prediction\n",
    "    model = xgb.XGBClassifier(\n",
    "        # Handle imbalanced classes\n",
    "        scale_pos_weight=scale_pos_weight,  # CRITICAL for imbalanced data\n",
    "        enable_categorical = True,\n",
    "        # Model complexity\n",
    "        n_estimators=200,                   # Number of boosting rounds\n",
    "        max_depth=6,                        # Tree depth (prevent overfitting)\n",
    "        learning_rate=0.01,                 # Lower = more robust but slower\n",
    "        \n",
    "        # Optimization\n",
    "        objective='binary:logistic',        # Binary classification,                 \n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Train (simplified - no need to pass eval_metric again)\n",
    "    print(\"\\nTraining XGBoost model...\")\n",
    "    model.fit(\n",
    "        X_train, y_train\n",
    "    )\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"balanced accuracy: {balanced_accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "    # Feature importance\n",
    "    import pandas as pd\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    print(\"\\nTop 30 Most Important Features:\")\n",
    "    print(feature_importance.head(30).to_string(index=False))\n",
    "    return X_test, y_test, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf6b2ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in train:\n",
      "will_churn_10days\n",
      "0    105729\n",
      "1     14926\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Calculated scale_pos_weight: 7.08\n",
      "\n",
      "Training XGBoost model...\n",
      "balanced accuracy: 0.8045054171390154\n",
      "ROC-AUC: 0.8896\n",
      "\n",
      "Confusion Matrix:\n",
      "[[20951  5482]\n",
      " [  685  3046]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Churn       0.97      0.79      0.87     26433\n",
      "       Churn       0.36      0.82      0.50      3731\n",
      "\n",
      "    accuracy                           0.80     30164\n",
      "   macro avg       0.66      0.80      0.68     30164\n",
      "weighted avg       0.89      0.80      0.83     30164\n",
      "\n",
      "\n",
      "Top 30 Most Important Features:\n",
      "                            feature  importance\n",
      "                  frustration_score    0.186231\n",
      "               thumbs_down_lifetime    0.175407\n",
      "         frustration_score_lifetime    0.092902\n",
      "                     unique_artists    0.054654\n",
      "               total_songs_listened    0.054346\n",
      "         songs_listened_last_14days    0.045495\n",
      "             mobile_usage_ratio_14d    0.038870\n",
      "                  consistency_score    0.038488\n",
      "          has_downgrade_last_15days    0.035252\n",
      "                          sessionId    0.034902\n",
      "                     has_downgraded    0.030838\n",
      "                      unique_genres    0.029894\n",
      "                 thumbs_up_lifetime    0.028253\n",
      "          consecutive_days_inactive    0.022665\n",
      "           session_frequency_change    0.021772\n",
      "             days_without_thumbs_up    0.018059\n",
      "                            is_paid    0.016663\n",
      "         activity_trend_last_14days    0.016485\n",
      "                      has_ever_paid    0.011087\n",
      "             sessions_last7d_vs_avg    0.010118\n",
      "                    help_visits_14d    0.006277\n",
      "            favorite_artist_encoded    0.005909\n",
      "listening_time_ratio_7d_vs_lifetime    0.005423\n",
      "            days_since_registration    0.005264\n",
      "                settings_visits_14d    0.005093\n",
      "            thumbs_down_last_14days    0.004158\n",
      "     negative_actions_last7d_vs_avg    0.002914\n",
      "                     error_rate_14d    0.002579\n",
      "             favorite_genre_encoded    0.000000\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test, model_XGB = XGB_training(df_consistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7931119d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['userId', 'sessionId', 'session_time', 'negative_actions_last7d_vs_avg',\n",
       "       'consistency_score', 'consecutive_days_inactive',\n",
       "       'session_frequency_change', 'sessions_last7d_vs_avg',\n",
       "       'thumbs_down_last_14days', 'days_without_thumbs_up',\n",
       "       'has_downgrade_last_15days', 'activity_trend_last_14days',\n",
       "       'songs_listened_last_14days', 'help_visits_14d', 'error_rate_14d',\n",
       "       'settings_visits_14d', 'frustration_score', 'is_paid', 'has_downgraded',\n",
       "       'mobile_usage_ratio_14d', 'days_since_registration', 'has_ever_paid',\n",
       "       'thumbs_down_lifetime', 'thumbs_up_lifetime',\n",
       "       'frustration_score_lifetime', 'listening_time_ratio_7d_vs_lifetime',\n",
       "       'favorite_genre_encoded', 'favorite_artist_encoded',\n",
       "       'total_songs_listened', 'unique_artists', 'unique_genres',\n",
       "       'favorite_genre', 'favorite_artist', 'will_churn_10days'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_consistant.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
