{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e147c1e2",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bfa238d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_parquet(\"data/test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b626b49",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38549e19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data_clean = \u001b[43mdf_test\u001b[49m[df_test[\u001b[33m'\u001b[39m\u001b[33muserId\u001b[39m\u001b[33m'\u001b[39m] != \u001b[33m'\u001b[39m\u001b[33m1261737\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# It is an outlier\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "# It is an outlier, so we remove it and will put arbitrarily a value to it later\n",
    "data_clean = df_test[df_test['userId'] != '1261737']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b7d71659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission before: 43608 rows\n",
      "Submission after: 43609 rows\n",
      "\n",
      "User 1261737 in submission: True\n"
     ]
    }
   ],
   "source": [
    "# Créer une ligne pour user 1261737\n",
    "new_row = pd.DataFrame({\n",
    "    'userId': ['1261737'],\n",
    "    'will_churn_prediction': [0]\n",
    "})\n",
    "\n",
    "# Ajouter à ton submission\n",
    "submission_complete = pd.concat([submission, new_row], ignore_index=True)\n",
    "\n",
    "print(f\"Submission before: {len(submission)} rows\")\n",
    "print(f\"Submission after: {len(submission_complete)} rows\")\n",
    "\n",
    "# Vérifier\n",
    "print(f\"\\nUser 1261737 in submission: {'1261737' in submission_complete['userId'].values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfeb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>session_time</th>\n",
       "      <th>churn_probability</th>\n",
       "      <th>will_churn_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000655</td>\n",
       "      <td>19711.0</td>\n",
       "      <td>2018-10-03 09:00:59</td>\n",
       "      <td>0.473175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000655</td>\n",
       "      <td>36911.0</td>\n",
       "      <td>2018-10-05 22:06:38</td>\n",
       "      <td>0.758766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000655</td>\n",
       "      <td>49298.0</td>\n",
       "      <td>2018-10-07 19:32:55</td>\n",
       "      <td>0.799956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000655</td>\n",
       "      <td>54275.0</td>\n",
       "      <td>2018-10-18 04:38:45</td>\n",
       "      <td>0.722955</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000655</td>\n",
       "      <td>96058.0</td>\n",
       "      <td>2018-10-21 14:25:04</td>\n",
       "      <td>0.733780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43604</th>\n",
       "      <td>1999996</td>\n",
       "      <td>136422.0</td>\n",
       "      <td>2018-10-29 19:16:14</td>\n",
       "      <td>0.807168</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43605</th>\n",
       "      <td>1999996</td>\n",
       "      <td>137240.0</td>\n",
       "      <td>2018-10-30 15:24:07</td>\n",
       "      <td>0.822897</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43606</th>\n",
       "      <td>1999996</td>\n",
       "      <td>140582.0</td>\n",
       "      <td>2018-11-08 00:34:07</td>\n",
       "      <td>0.774387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43607</th>\n",
       "      <td>1999996</td>\n",
       "      <td>170036.0</td>\n",
       "      <td>2018-11-13 09:01:28</td>\n",
       "      <td>0.729282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43608</th>\n",
       "      <td>1261737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43609 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  sessionId        session_time  churn_probability  \\\n",
       "0      1000655    19711.0 2018-10-03 09:00:59           0.473175   \n",
       "1      1000655    36911.0 2018-10-05 22:06:38           0.758766   \n",
       "2      1000655    49298.0 2018-10-07 19:32:55           0.799956   \n",
       "3      1000655    54275.0 2018-10-18 04:38:45           0.722955   \n",
       "4      1000655    96058.0 2018-10-21 14:25:04           0.733780   \n",
       "...        ...        ...                 ...                ...   \n",
       "43604  1999996   136422.0 2018-10-29 19:16:14           0.807168   \n",
       "43605  1999996   137240.0 2018-10-30 15:24:07           0.822897   \n",
       "43606  1999996   140582.0 2018-11-08 00:34:07           0.774387   \n",
       "43607  1999996   170036.0 2018-11-13 09:01:28           0.729282   \n",
       "43608  1261737        NaN                 NaT                NaN   \n",
       "\n",
       "       will_churn_prediction  \n",
       "0                          0  \n",
       "1                          1  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          1  \n",
       "...                      ...  \n",
       "43604                      1  \n",
       "43605                      1  \n",
       "43606                      1  \n",
       "43607                      1  \n",
       "43608                      0  \n",
       "\n",
       "[43609 rows x 5 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "PIPELINE COMPLÈTE : FEATURES TEST SET + PRÉDICTIONS\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PIPELINE COMPLÈTE : TRAIN → TEST → PRÉDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 1 : CHARGER LE MODÈLE ENTRAÎNÉ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"1. Chargement du modèle entraîné...\")\n",
    "\n",
    "# Option B : Si tu as le modèle en mémoire\n",
    "model = model_XGB  # (celui retourné par XGB_training)\n",
    "\n",
    "print(\"✅ Modèle chargé\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 2 : CHARGER LE TEST SET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"2. Chargement du test set...\")\n",
    "\n",
    "df_test = data_clean.copy()  # Ton fichier test\n",
    "\n",
    "print(f\"Test set shape: {df_test.shape}\")\n",
    "print(f\"Users test: {df_test['userId'].nunique()}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 3 : CRÉER LES FEATURES PAR SESSION (TEST SET)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"3. Génération des features par session (test set)...\")\n",
    "\n",
    "# Colonnes nécessaires\n",
    "required_cols = ['userId', 'sessionId', 'time', 'page', 'registration', 'level', 'userAgent', 'length', 'artist']\n",
    "\n",
    "# IMPORTANT: Le test set N'A PAS de churn events\n",
    "# Donc on ne filtre PAS \"Cancellation Confirmation\"\n",
    "df_test_clean = df_test[required_cols].copy()\n",
    "\n",
    "# Générer les features\n",
    "features_test = create_features_per_session_optimized(\n",
    "    df_test_clean,\n",
    "    batch_size=10000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Features test générées: {features_test.shape}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 4 : AJOUTER LES PRÉFÉRENCES MUSICALES (TEST SET)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"4. Calcul des préférences musicales (test set)...\")\n",
    "\n",
    "music_prefs_test = create_user_preferences_improved(\n",
    "    df_test,\n",
    "    encode=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Merger\n",
    "features_test = features_test.merge(\n",
    "    music_prefs_test[[\n",
    "        'userId',\n",
    "        'favorite_genre_encoded',\n",
    "        'favorite_artist_encoded',\n",
    "        'total_songs_listened',\n",
    "        'unique_artists',\n",
    "        'unique_genres'\n",
    "    ]],\n",
    "    on='userId',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remplir les NaN\n",
    "features_test['favorite_genre_encoded'].fillna(-1, inplace=True)\n",
    "features_test['favorite_artist_encoded'].fillna(-1, inplace=True)\n",
    "features_test['total_songs_listened'].fillna(0, inplace=True)\n",
    "features_test['unique_artists'].fillna(0, inplace=True)\n",
    "features_test['unique_genres'].fillna(0, inplace=True)\n",
    "\n",
    "print(f\"✅ Features test avec musique: {features_test.shape}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 5 : PRÉPARER X_test (MÊMES COLONNES QUE TRAIN)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"5. Préparation de X_test...\")\n",
    "\n",
    "# Colonnes à exclure\n",
    "exclude_cols = [\n",
    "    'userId',\n",
    "    'sessionId',\n",
    "    'session_time',\n",
    "    'favorite_genre',      # Texte\n",
    "    'favorite_artist'      # Texte\n",
    "]\n",
    "\n",
    "# Enlever les colonnes d'exclusion\n",
    "features_test_clean = features_test.drop(columns=exclude_cols, errors='ignore')\n",
    "\n",
    "\n",
    "print(f\"✅ X_test shape: {X_test.shape}\")\n",
    "print(f\"Colonnes: {X_test.columns.tolist()}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 6 : VÉRIFIER QUE LES COLONNES MATCHENT AVEC LE TRAIN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"6. Vérification des colonnes...\")\n",
    "\n",
    "# Récupérer les colonnes utilisées pendant l'entraînement\n",
    "# Option A : Si tu as sauvegardé les colonnes\n",
    "# train_cols = pd.read_csv('train_columns.txt', header=None)[0].tolist()\n",
    "\n",
    "# Option B : Utiliser les feature_names du modèle XGBoost\n",
    "train_cols = model.get_booster().feature_names\n",
    "\n",
    "# Vérifier les colonnes manquantes/en trop\n",
    "missing_cols = set(train_cols) - set(X_test.columns)\n",
    "extra_cols = set(X_test.columns) - set(train_cols)\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"⚠️ Colonnes manquantes dans test: {missing_cols}\")\n",
    "    # Ajouter les colonnes manquantes avec des 0\n",
    "    for col in missing_cols:\n",
    "        X_test[col] = 0\n",
    "\n",
    "if extra_cols:\n",
    "    print(f\"⚠️ Colonnes en trop dans test: {extra_cols}\")\n",
    "    # Enlever les colonnes en trop\n",
    "    X_test = X_test.drop(columns=list(extra_cols))\n",
    "\n",
    "# Réordonner les colonnes dans le même ordre que train\n",
    "X_test = X_test[train_cols]\n",
    "\n",
    "print(f\"✅ Colonnes alignées: {X_test.shape}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE 7 : PRÉDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"7. Génération des prédictions...\")\n",
    "\n",
    "# Prédire les probabilités\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Prédire les classes (avec seuil par défaut 0.5)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Ajouter au DataFrame\n",
    "predictions = features_test[['userId', 'sessionId', 'session_time']].copy()\n",
    "predictions['churn_probability'] = y_pred_proba\n",
    "predictions['will_churn_prediction'] = y_pred\n",
    "\n",
    "print(f\"✅ Prédictions générées: {predictions.shape}\")\n",
    "print()\n",
    "print(\"Aperçu des prédictions:\")\n",
    "print(predictions.head(20))\n",
    "print()\n",
    "print(\"Distribution des prédictions:\")\n",
    "print(predictions['will_churn_prediction'].value_counts())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7638f100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission structure:\n",
      "        id  sessionId        session_time  churn_probability  target\n",
      "0  1000655    19711.0 2018-10-03 09:00:59           0.473175       0\n",
      "1  1000655    36911.0 2018-10-05 22:06:38           0.758766       1\n",
      "2  1000655    49298.0 2018-10-07 19:32:55           0.799956       1\n",
      "3  1000655    54275.0 2018-10-18 04:38:45           0.722955       1\n",
      "4  1000655    96058.0 2018-10-21 14:25:04           0.733780       1\n",
      "\n",
      "Columns: ['id', 'sessionId', 'session_time', 'churn_probability', 'target']\n",
      "Shape: (43609, 5)\n",
      "\n",
      "Null values: id                   0\n",
      "sessionId            1\n",
      "session_time         1\n",
      "churn_probability    1\n",
      "target               0\n",
      "dtype: int64\n",
      "\n",
      "✓ Submission ready with columns: id, target\n"
     ]
    }
   ],
   "source": [
    "# Renommer les colonnes pour Kaggle\n",
    "submission_final = submission_complete.rename(columns={\n",
    "    'userId': 'id',\n",
    "    'will_churn_prediction': 'target'\n",
    "})\n",
    "\n",
    "# Vérifier\n",
    "print(\"Submission structure:\")\n",
    "print(submission_final.head())\n",
    "print(f\"\\nColumns: {submission_final.columns.tolist()}\")\n",
    "print(f\"Shape: {submission_final.shape}\")\n",
    "\n",
    "# Vérifier pas de null\n",
    "print(f\"\\nNull values: {submission_final.isnull().sum()}\")\n",
    "\n",
    "print(\"\\n✓ Submission ready with columns: id, target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "44b9558b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deduplication: 2904 unique users\n",
      "\n",
      "Target distribution:\n",
      "target\n",
      "1    2825\n",
      "0      79\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Grouper par userId : 1 si au moins un 1, sinon 0\n",
    "submission_final = submission_final.groupby('id').agg({\n",
    "    'target': 'max'  # max = 1 si au moins un 1, sinon 0\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"After deduplication: {len(submission_final)} unique users\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(submission_final['target'].value_counts())\n",
    "\n",
    "# Vérifier plus de duplicates\n",
    "print(f\"\\nDuplicates: {submission_final['id'].duplicated().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "85a90a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected users: 2903\n",
      "Submitted users: 2904\n",
      "\n",
      "Missing users: 0\n",
      "Missing user IDs: []\n",
      "\n",
      "Before adding missing: 2904 rows\n",
      "After adding missing: 2904 rows\n",
      "\n",
      "✓ Complete submission with all 2904 users saved!\n"
     ]
    }
   ],
   "source": [
    "# 1. Identifier les userId manquants\n",
    "expected_users = df_test['userId'].unique()\n",
    "submitted_users = submission_final['id'].unique()\n",
    "\n",
    "print(f\"Expected users: {len(expected_users)}\")\n",
    "print(f\"Submitted users: {len(submitted_users)}\")\n",
    "\n",
    "# Trouver les manquants\n",
    "missing_users = set(expected_users) - set(submitted_users)\n",
    "print(f\"\\nMissing users: {len(missing_users)}\")\n",
    "print(f\"Missing user IDs: {list(missing_users)[:10]}\")  # Afficher les 10 premiers\n",
    "\n",
    "# 2. Créer des lignes pour les utilisateurs manquants (avec target=0)\n",
    "missing_rows = pd.DataFrame({\n",
    "    'id': list(missing_users),\n",
    "    'target': 0\n",
    "})\n",
    "\n",
    "# 3. Ajouter à la submission\n",
    "submission_complete = pd.concat([submission_final, missing_rows], ignore_index=True)\n",
    "\n",
    "print(f\"\\nBefore adding missing: {len(submission_final)} rows\")\n",
    "print(f\"After adding missing: {len(submission_complete)} rows\")\n",
    "\n",
    "# 4. Vérifier qu'on a bien 2904\n",
    "assert len(submission_complete) == 2904, f\"Expected 2904, got {len(submission_complete)}\"\n",
    "\n",
    "# 5. Trier par id (optionnel mais propre)\n",
    "submission_complete = submission_complete.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "# 6. Sauvegarder\n",
    "submission_complete.to_csv('submission.csv', index=False)\n",
    "print(\"\\n✓ Complete submission with all 2904 users saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e86268",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b756af5-4802-489c-9110-9712ef5d4057",
   "metadata": {},
   "source": [
    "The competition is to be performed in groups of two. You'll have a report of 4 pages to submit by december 14th, presenting the methods you tested and used. For the defense you'll get 8 minutes of presentations + 7 minutes of questions, including on question on the labs, that may involve writing a code snippet.\n",
    "\n",
    "\n",
    "Churn prediction 25/26\n",
    "**Predict churn prediction from streaming service logs**\n",
    "\n",
    "The goal of the competition is to predict whether or not some users (whose user ids are in the test file) will **churn in the window of 10 days that follows the given observations (ie after \"2018-11-20\")**. We consider that a user churns when they visit the page **'Cancellation Confirmation'** (edited) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b63795-e760-4f0a-bae5-cf2df708a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train_test = pd.read_parquet(\"data/test.parquet\")\n",
    "df_train = pd.read_parquet(\"data/train.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701237a",
   "metadata": {},
   "source": [
    "### Creation of target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51d866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant: 18304, Après: 18304, Supprimé: 836\n"
     ]
    }
   ],
   "source": [
    "# Supprimer users qui churnent dans les 7 premiers jours\n",
    "churners = df_train[df_train['page'] == 'Cancellation Confirmation'].copy()\n",
    "churners['days_since_start'] = (churners['time'] - df_train['time'].min()).dt.days\n",
    "early_churners = churners[churners['days_since_start'] <= 7]['userId'].unique()\n",
    "\n",
    "df_train = df_train[~df_train['userId'].isin(early_churners)]\n",
    "\n",
    "print(f\"Avant: {df_train['userId'].nunique()}, Après: {df_train['userId'].nunique()}, Supprimé: {len(early_churners)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97204208-75c0-4e5e-bd4d-1ee12e661e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating cancellation in following ten days column\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cancellation_events = df_train[df_train['page'] == 'Cancellation Confirmation'].copy()\n",
    "cancellation_events = cancellation_events[['userId', 'time']].rename(columns={'time': 'churn_time'})\n",
    "\n",
    "df_train = df_train.merge(cancellation_events, on='userId', how='left')\n",
    "\n",
    "df_train['days_until_churn'] = (df_train['churn_time'] - df_train['time']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "df_train['will_churn_10days'] = ((df_train['days_until_churn'] >= 0) & \n",
    "                                   (df_train['days_until_churn'] <= 10)).astype(int)\n",
    "\n",
    "df_train = df_train.drop(['churn_time', 'days_until_churn'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1962a199-b9c6-4f5e-a34a-892fdf8e4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe() #max time is 2018-11-20 so we are going to keep only the rows that are at least 10 days old OR that have churn True\n",
    "\n",
    "df_train = df_train[(df_train[\"time\"] < \"2018-11-10\" )| (df_train[\"will_churn_10days\"] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2884f3",
   "metadata": {},
   "source": [
    "## Preliminary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fab7aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'utilisateurs qui ont churné : 3435\n",
      "Taux de churn global : 19.03%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>churn_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>2018-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000083</td>\n",
       "      <td>2018-10-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000280</td>\n",
       "      <td>2018-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000353</td>\n",
       "      <td>2018-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000503</td>\n",
       "      <td>2018-10-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>1998845</td>\n",
       "      <td>2018-10-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>1998879</td>\n",
       "      <td>2018-10-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>1999022</td>\n",
       "      <td>2018-11-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>1999847</td>\n",
       "      <td>2018-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>1999892</td>\n",
       "      <td>2018-10-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3435 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  churn_date\n",
       "0     1000025  2018-10-18\n",
       "1     1000083  2018-10-12\n",
       "2     1000280  2018-11-13\n",
       "3     1000353  2018-10-22\n",
       "4     1000503  2018-10-13\n",
       "...       ...         ...\n",
       "3430  1998845  2018-10-24\n",
       "3431  1998879  2018-10-21\n",
       "3432  1999022  2018-11-04\n",
       "3433  1999847  2018-10-18\n",
       "3434  1999892  2018-10-26\n",
       "\n",
       "[3435 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Identifier les churners et leur date de churn\n",
    "churners = df_train[df_train['page'] == 'Cancellation Confirmation'].copy()\n",
    "churn_dates = churners.groupby('userId')['time'].min().reset_index()\n",
    "churn_dates.columns = ['userId', 'churn_date']\n",
    "\n",
    "print(f\"Nombre d'utilisateurs qui ont churné : {len(churn_dates)}\")\n",
    "print(f\"Taux de churn global : {len(churn_dates) / df_train['userId'].nunique() * 100:.2f}%\")\n",
    "churn_dates['churn_date'] = churn_dates['churn_date'].dt.date\n",
    "churn_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2db3b87",
   "metadata": {},
   "source": [
    "## Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5525fd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape avant nettoyage: (14478407, 20)\n",
      "Mémoire avant: 12.33 GB\n",
      "Shape après nettoyage: (14478407, 8)\n",
      "Mémoire après: 5.21 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape avant nettoyage: {df_train.shape}\")\n",
    "print(f\"Mémoire avant: {df_train.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "# Garder uniquement les colonnes nécessaires\n",
    "colonnes_necessaires = ['userId', 'sessionId', 'level', 'userAgent', 'time', 'page', 'length', 'registration']\n",
    "df_train = df_train[colonnes_necessaires].copy()\n",
    "\n",
    "print(f\"Shape après nettoyage: {df_train.shape}\")\n",
    "print(f\"Mémoire après: {df_train.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "# Supprimer le leakage\n",
    "df_train = df_train[~df_train['page'].isin(['Cancel', 'Cancellation Confirmation'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc63e97",
   "metadata": {},
   "source": [
    "## Sous le capot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7620df",
   "metadata": {},
   "source": [
    "### Current function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c57b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction ULTRA-OPTIMISÉE par SESSION avec 24 features\n",
    "Version RAPIDE : ~2 minutes pour 1M sessions\n",
    "\n",
    "OPTIMISATIONS:\n",
    "- Feature 25 (avg_time_in_session_without_music_14d) ENLEVÉE (trop lente)\n",
    "- Features 23, 24 SIMPLIFIÉES (ratios totaux au lieu de moyennes par session)\n",
    "- consistency_score SIMPLIFIÉ\n",
    "- consecutive_days_inactive VECTORISÉ\n",
    "- has_downgraded VECTORISÉ\n",
    "- Pré-calculs maximisés\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import gc\n",
    "\n",
    "\n",
    "def create_features_per_session_optimized(\n",
    "    df: pd.DataFrame,\n",
    "    batch_size: int = 10000,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule 24 features ULTRA-RAPIDE à chaque nouvelle session\n",
    "    \n",
    "    Colonnes requises: userId, sessionId, time, page, registration, level, userAgent, length\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame avec (userId, sessionId, session_time, 24 features)\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*80)\n",
    "        print(\"CALCUL DES 24 FEATURES PAR SESSION (VERSION ULTRA-OPTIMISÉE)\")\n",
    "        print(\"=\"*80)\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PRÉPARATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['date'] = df['time'].dt.date\n",
    "    df['registration'] = pd.to_datetime(df['registration'])\n",
    "    df = df.sort_values(['userId', 'sessionId', 'time']).reset_index(drop=True)\n",
    "    \n",
    "    # Convertir length en float (en secondes)\n",
    "    df['length'] = pd.to_numeric(df['length'], errors='coerce').fillna(0)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Création des flags...\")\n",
    "    \n",
    "    # Flags (vectorisé)\n",
    "    df['is_nextsong'] = (df['page'] == 'NextSong').astype(np.int8)\n",
    "    df['is_thumbs_up'] = (df['page'] == 'Thumbs Up').astype(np.int8)\n",
    "    df['is_thumbs_down'] = (df['page'] == 'Thumbs Down').astype(np.int8)\n",
    "    df['is_error'] = (df['page'] == 'Error').astype(np.int8)\n",
    "    df['is_help'] = (df['page'] == 'Help').astype(np.int8)\n",
    "    df['is_settings'] = (df['page'] == 'Settings').astype(np.int8)\n",
    "    df['is_downgrade'] = (df['page'] == 'Downgrade').astype(np.int8)\n",
    "    \n",
    "    # Device type (simplifié)\n",
    "    df['is_mobile_action'] = df['userAgent'].str.contains('iPhone|iPad|Android', case=False, na=False).astype(np.int8)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Dataset: {df.shape}\")\n",
    "        print(f\"Users: {df['userId'].nunique()}\")\n",
    "        print(f\"Sessions: {df['sessionId'].nunique()}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # IDENTIFIER LES SESSIONS\n",
    "    # ========================================================================\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Identification des sessions...\")\n",
    "    \n",
    "    session_starts = df.groupby(['userId', 'sessionId']).agg({\n",
    "        'time': 'min',\n",
    "        'registration': 'first'\n",
    "    }).reset_index()\n",
    "    session_starts.columns = ['userId', 'sessionId', 'session_time', 'registration']\n",
    "    \n",
    "    total_sessions = len(session_starts)\n",
    "    num_batches = (total_sessions + batch_size - 1) // batch_size\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Total sessions: {total_sessions:,}\")\n",
    "        print(f\"Batch size: {batch_size:,}\")\n",
    "        print(f\"Nombre de batches: {num_batches}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PRÉ-CALCUL : GROUPBY PAR USER\n",
    "    # ========================================================================\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Pré-calcul des groupes par user...\")\n",
    "    \n",
    "    user_groups = {user_id: group for user_id, group in df.groupby('userId', sort=False)}\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TRAITEMENT PAR BATCH DE SESSIONS\n",
    "    # ========================================================================\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        if verbose:\n",
    "            print(f\"Batch {batch_idx + 1}/{num_batches}...\", end=' ')\n",
    "        \n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, total_sessions)\n",
    "        batch_sessions = session_starts.iloc[start_idx:end_idx]\n",
    "        \n",
    "        batch_features = []\n",
    "        \n",
    "        for idx, row in batch_sessions.iterrows():\n",
    "            user_id = row['userId']\n",
    "            session_id = row['sessionId']\n",
    "            session_time = row['session_time']\n",
    "            registration = row['registration']\n",
    "            \n",
    "            user_data = user_groups[user_id]\n",
    "            user_data_before = user_data[user_data['time'] < session_time]\n",
    "            \n",
    "            if len(user_data_before) == 0:\n",
    "                # Première session : features par défaut\n",
    "                features = {\n",
    "                    'userId': user_id,\n",
    "                    'sessionId': session_id,\n",
    "                    'session_time': session_time,\n",
    "                    'negative_actions_last7d_vs_avg': 1.0,\n",
    "                    'consistency_score': 0.0,\n",
    "                    'consecutive_days_inactive': 0,\n",
    "                    'session_frequency_change': 1.0,\n",
    "                    'sessions_last7d_vs_avg': 1.0,\n",
    "                    'thumbs_down_last_14days': 0,\n",
    "                    'days_without_thumbs_up': (session_time - registration).days,\n",
    "                    'has_downgrade_last_15days': 0,\n",
    "                    'activity_trend_last_14days': 1.0,\n",
    "                    'songs_listened_last_14days': 0,\n",
    "                    'help_visits_14d': 0,\n",
    "                    'error_rate_14d': 0.0,\n",
    "                    'settings_visits_14d': 0,\n",
    "                    'frustration_score': 0.0,\n",
    "                    'is_paid': 0,\n",
    "                    'has_downgraded': 0,\n",
    "                    'mobile_usage_ratio_14d': 0.5,\n",
    "                    'days_since_registration': (session_time - registration).days,\n",
    "                    'has_ever_paid': 0,\n",
    "                    'thumbs_down_lifetime': 0,\n",
    "                    'thumbs_up_lifetime': 0,\n",
    "                    'frustration_score_lifetime': 0.0,\n",
    "                    'listening_time_ratio_7d_vs_lifetime': 1.0\n",
    "                }\n",
    "                batch_features.append(features)\n",
    "                continue\n",
    "            \n",
    "            # Fenêtres temporelles\n",
    "            window_14d_start = session_time - timedelta(days=14)\n",
    "            window_7d_start = session_time - timedelta(days=7)\n",
    "            window_15d_start = session_time - timedelta(days=15)\n",
    "            \n",
    "            data_14d = user_data_before[(user_data_before['time'] >= window_14d_start)]\n",
    "            data_7d = user_data_before[(user_data_before['time'] >= window_7d_start)]\n",
    "            data_week2 = user_data_before[(user_data_before['time'] >= window_14d_start) & \n",
    "                                          (user_data_before['time'] < window_7d_start)]\n",
    "            data_15d = user_data_before[(user_data_before['time'] >= window_15d_start)]\n",
    "            data_lifetime = user_data_before\n",
    "            \n",
    "            first_activity = data_lifetime['time'].min()\n",
    "            lifetime_days = (session_time - first_activity).days + 1\n",
    "            \n",
    "            # ============================================================\n",
    "            # FEATURES ORIGINALES (OPTIMISÉES)\n",
    "            # ============================================================\n",
    "            \n",
    "            # 1. negative_actions_last7d_vs_avg\n",
    "            thumbs_down_7d = data_7d['is_thumbs_down'].sum()\n",
    "            negative_7d = thumbs_down_7d + data_7d['is_error'].sum() + data_7d['is_help'].sum() + data_7d['is_settings'].sum()\n",
    "            negative_lifetime = (data_lifetime['is_thumbs_down'].sum() + \n",
    "                                data_lifetime['is_error'].sum() + \n",
    "                                data_lifetime['is_help'].sum() + \n",
    "                                data_lifetime['is_settings'].sum())\n",
    "            actions_7d = len(data_7d)\n",
    "            actions_lifetime = len(data_lifetime)\n",
    "            negative_7d_norm = negative_7d / max(actions_7d, 1)\n",
    "            negative_lifetime_norm = negative_lifetime / max(actions_lifetime, 1)\n",
    "            negative_actions_last7d_vs_avg = negative_7d_norm / max(negative_lifetime_norm, 0.001)\n",
    "            \n",
    "            # 2. consistency_score (SIMPLIFIÉ - pas de date_range)\n",
    "            if len(data_14d) > 0:\n",
    "                daily_sessions = data_14d.groupby('date')['sessionId'].nunique()\n",
    "                mean_sessions = daily_sessions.mean()\n",
    "                std_sessions = daily_sessions.std()\n",
    "                consistency_score = std_sessions / mean_sessions if mean_sessions > 0 else 0.0\n",
    "            else:\n",
    "                consistency_score = 0.0\n",
    "            \n",
    "            # 3. consecutive_days_inactive (VECTORISÉ)\n",
    "            if len(data_14d) > 0:\n",
    "                active_dates_14d = sorted(data_14d['date'].unique())\n",
    "                if len(active_dates_14d) > 1:\n",
    "                    # Calculer les gaps entre dates actives\n",
    "                    active_dates_series = pd.Series(active_dates_14d)\n",
    "                    date_diffs = active_dates_series.diff().dt.days\n",
    "                    consecutive_days_inactive = int(date_diffs.max() - 1) if len(date_diffs) > 0 else 0\n",
    "                    consecutive_days_inactive = max(0, consecutive_days_inactive)\n",
    "                else:\n",
    "                    consecutive_days_inactive = 0\n",
    "            else:\n",
    "                consecutive_days_inactive = 14\n",
    "            \n",
    "            # 4. session_frequency_change\n",
    "            sessions_week1 = data_7d['sessionId'].nunique()\n",
    "            sessions_week2 = data_week2['sessionId'].nunique()\n",
    "            session_frequency_change = sessions_week1 / max(sessions_week2, 1)\n",
    "            \n",
    "            # 5. sessions_last7d_vs_avg\n",
    "            sessions_7d = data_7d['sessionId'].nunique()\n",
    "            sessions_lifetime = data_lifetime['sessionId'].nunique()\n",
    "            sessions_lifetime_avg = sessions_lifetime / max(lifetime_days, 1)\n",
    "            sessions_7d_avg = sessions_7d / 7\n",
    "            sessions_last7d_vs_avg = sessions_7d_avg / max(sessions_lifetime_avg, 0.01)\n",
    "            \n",
    "            # 6. thumbs_down_last_14days\n",
    "            thumbs_down_last_14days = data_14d['is_thumbs_down'].sum()\n",
    "            \n",
    "            # 7. days_without_thumbs_up\n",
    "            thumbs_up_data = data_lifetime[data_lifetime['is_thumbs_up'] == 1]\n",
    "            if len(thumbs_up_data) > 0:\n",
    "                last_thumbs_up = thumbs_up_data['time'].max()\n",
    "                days_without_thumbs_up = (session_time - last_thumbs_up).days\n",
    "            else:\n",
    "                days_without_thumbs_up = (session_time - registration).days\n",
    "            \n",
    "            # 8. has_downgrade_last_15days\n",
    "            has_downgrade_last_15days = int(data_15d['is_downgrade'].sum() > 0)\n",
    "            \n",
    "            # 9. activity_trend_last_14days\n",
    "            songs_week1 = data_7d['is_nextsong'].sum()\n",
    "            songs_week2 = data_week2['is_nextsong'].sum()\n",
    "            activity_trend_last_14days = songs_week1 / max(songs_week2, 1)\n",
    "            \n",
    "            # 10. songs_listened_last_14days\n",
    "            songs_listened_last_14days = data_14d['is_nextsong'].sum()\n",
    "            \n",
    "            # 11. help_visits_14d\n",
    "            help_visits_14d = data_14d['is_help'].sum()\n",
    "            \n",
    "            # 12. error_rate_14d\n",
    "            errors_14d = data_14d['is_error'].sum()\n",
    "            total_actions_14d = len(data_14d)\n",
    "            error_rate_14d = errors_14d / max(total_actions_14d, 1)\n",
    "            \n",
    "            # 13. settings_visits_14d\n",
    "            settings_visits_14d = data_14d['is_settings'].sum()\n",
    "            \n",
    "            # 14. frustration_score\n",
    "            frustration_score = (\n",
    "                thumbs_down_last_14days * 2.0 +\n",
    "                help_visits_14d * 1.5 +\n",
    "                settings_visits_14d * 1.5 +\n",
    "                has_downgrade_last_15days * 3.0\n",
    "            )\n",
    "            \n",
    "            # 15. is_paid\n",
    "            current_level = data_lifetime.iloc[-1]['level'] if len(data_lifetime) > 0 else 'free'\n",
    "            is_paid = 1 if current_level == 'paid' else 0\n",
    "            \n",
    "            # 16. has_downgraded (VECTORISÉ)\n",
    "            levels_series = data_lifetime.sort_values('time')['level']\n",
    "            level_changes = levels_series != levels_series.shift()\n",
    "            transitions = levels_series[level_changes]\n",
    "            has_downgraded = 0\n",
    "            if len(transitions) > 1:\n",
    "                for i in range(len(transitions) - 1):\n",
    "                    if transitions.iloc[i] == 'paid' and transitions.iloc[i+1] == 'free':\n",
    "                        has_downgraded = 1\n",
    "                        break\n",
    "            \n",
    "            # 17. mobile_usage_ratio_14d\n",
    "            mobile_actions_14d = data_14d['is_mobile_action'].sum()\n",
    "            mobile_usage_ratio_14d = mobile_actions_14d / max(total_actions_14d, 1)\n",
    "            \n",
    "            # 18. days_since_registration\n",
    "            days_since_registration = (session_time - registration).days\n",
    "            \n",
    "            # 19. has_ever_paid\n",
    "            has_ever_paid = 1 if (data_lifetime['level'] == 'paid').any() else 0\n",
    "            \n",
    "            # ============================================================\n",
    "            # NOUVELLES FEATURES (5 au lieu de 6)\n",
    "            # ============================================================\n",
    "            \n",
    "            # 20. thumbs_down_lifetime\n",
    "            thumbs_down_lifetime = data_lifetime['is_thumbs_down'].sum()\n",
    "            \n",
    "            # 21. thumbs_up_lifetime\n",
    "            thumbs_up_lifetime = data_lifetime['is_thumbs_up'].sum()\n",
    "            \n",
    "            # 22. frustration_score_lifetime\n",
    "            help_visits_lifetime = data_lifetime['is_help'].sum()\n",
    "            settings_visits_lifetime = data_lifetime['is_settings'].sum()\n",
    "            has_ever_downgraded = 1 if data_lifetime['is_downgrade'].sum() > 0 else 0\n",
    "            \n",
    "            frustration_score_lifetime = (\n",
    "                thumbs_down_lifetime * 2.0 +\n",
    "                help_visits_lifetime * 1.5 +\n",
    "                settings_visits_lifetime * 1.5 +\n",
    "                has_ever_downgraded * 3.0\n",
    "            )\n",
    "            \n",
    "            # 23. listening_time_ratio_7d_vs_lifetime (SIMPLIFIÉ - ratio des totaux)\n",
    "            # Au lieu de moyennes par session, ratio des temps totaux\n",
    "            total_listening_7d = data_7d[data_7d['is_nextsong'] == 1]['length'].sum()\n",
    "            total_listening_lifetime = data_lifetime[data_lifetime['is_nextsong'] == 1]['length'].sum()\n",
    "            \n",
    "            # Normaliser par nombre de jours\n",
    "            listening_per_day_7d = total_listening_7d / 7\n",
    "            listening_per_day_lifetime = total_listening_lifetime / max(lifetime_days, 1)\n",
    "            \n",
    "            listening_time_ratio_7d_vs_lifetime = listening_per_day_7d / max(listening_per_day_lifetime, 1.0)\n",
    "            \n",
    "            # Feature 24 (avg_listening_time_per_session_last7d) fusionnée dans 23\n",
    "            # Feature 25 (avg_time_in_session_without_music_14d) ENLEVÉE (trop lente)\n",
    "            \n",
    "            # ============================================================\n",
    "            # ASSEMBLER (24 features)\n",
    "            # ============================================================\n",
    "            \n",
    "            features = {\n",
    "                'userId': user_id,\n",
    "                'sessionId': session_id,\n",
    "                'session_time': session_time,\n",
    "                'negative_actions_last7d_vs_avg': negative_actions_last7d_vs_avg,\n",
    "                'consistency_score': consistency_score,\n",
    "                'consecutive_days_inactive': consecutive_days_inactive,\n",
    "                'session_frequency_change': session_frequency_change,\n",
    "                'sessions_last7d_vs_avg': sessions_last7d_vs_avg,\n",
    "                'thumbs_down_last_14days': int(thumbs_down_last_14days),\n",
    "                'days_without_thumbs_up': days_without_thumbs_up,\n",
    "                'has_downgrade_last_15days': has_downgrade_last_15days,\n",
    "                'activity_trend_last_14days': activity_trend_last_14days,\n",
    "                'songs_listened_last_14days': int(songs_listened_last_14days),\n",
    "                'help_visits_14d': int(help_visits_14d),\n",
    "                'error_rate_14d': error_rate_14d,\n",
    "                'settings_visits_14d': int(settings_visits_14d),\n",
    "                'frustration_score': frustration_score,\n",
    "                'is_paid': is_paid,\n",
    "                'has_downgraded': has_downgraded,\n",
    "                'mobile_usage_ratio_14d': mobile_usage_ratio_14d,\n",
    "                'days_since_registration': days_since_registration,\n",
    "                'has_ever_paid': has_ever_paid,\n",
    "                'thumbs_down_lifetime': int(thumbs_down_lifetime),\n",
    "                'thumbs_up_lifetime': int(thumbs_up_lifetime),\n",
    "                'frustration_score_lifetime': frustration_score_lifetime,\n",
    "                'listening_time_ratio_7d_vs_lifetime': listening_time_ratio_7d_vs_lifetime\n",
    "            }\n",
    "            \n",
    "            batch_features.append(features)\n",
    "        \n",
    "        if batch_features:\n",
    "            batch_df = pd.DataFrame(batch_features)\n",
    "            all_features.append(batch_df)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ ({len(batch_features)} sessions)\")\n",
    "        \n",
    "        del batch_features\n",
    "        gc.collect()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # COMBINER\n",
    "    # ========================================================================\n",
    "    \n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"Combinaison finale...\")\n",
    "    \n",
    "    final_df = pd.concat(all_features, ignore_index=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"=\"*80)\n",
    "        print(\"✅ 24 FEATURES PAR SESSION CRÉÉES (ULTRA-OPTIMISÉ)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Shape: {final_df.shape}\")\n",
    "        print(f\"Sessions par user (moyenne): {len(final_df) / final_df['userId'].nunique():.1f}\")\n",
    "        print()\n",
    "        print(\"OPTIMISATIONS:\")\n",
    "        print(\"  ✅ consistency_score simplifié (pas de date_range)\")\n",
    "        print(\"  ✅ consecutive_days_inactive vectorisé\")\n",
    "        print(\"  ✅ has_downgraded vectorisé\")\n",
    "        print(\"  ✅ listening_time_ratio_7d_vs_lifetime simplifié (ratio totaux)\")\n",
    "        print(\"  ❌ avg_time_in_session_without_music_14d enlevée (trop lente)\")\n",
    "        print()\n",
    "        print(final_df.head())\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b192a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CALCUL DES 24 FEATURES PAR SESSION (VERSION ULTRA-OPTIMISÉE)\n",
      "================================================================================\n",
      "\n",
      "Création des flags...\n",
      "Dataset: (14471537, 17)\n",
      "Users: 18048\n",
      "Sessions: 137078\n",
      "\n",
      "Identification des sessions...\n",
      "Total sessions: 175,059\n",
      "Batch size: 10,000\n",
      "Nombre de batches: 18\n",
      "\n",
      "Pré-calcul des groupes par user...\n",
      "Batch 1/18... ✓ (10000 sessions)\n",
      "Batch 2/18... ✓ (10000 sessions)\n",
      "Batch 3/18... ✓ (10000 sessions)\n",
      "Batch 4/18... ✓ (10000 sessions)\n",
      "Batch 5/18... ✓ (10000 sessions)\n",
      "Batch 6/18... ✓ (10000 sessions)\n",
      "Batch 7/18... ✓ (10000 sessions)\n",
      "Batch 8/18... ✓ (10000 sessions)\n",
      "Batch 9/18... ✓ (10000 sessions)\n",
      "Batch 10/18... ✓ (10000 sessions)\n",
      "Batch 11/18... ✓ (10000 sessions)\n",
      "Batch 12/18... ✓ (10000 sessions)\n",
      "Batch 13/18... ✓ (10000 sessions)\n",
      "Batch 14/18... ✓ (10000 sessions)\n",
      "Batch 15/18... ✓ (10000 sessions)\n",
      "Batch 16/18... ✓ (10000 sessions)\n",
      "Batch 17/18... ✓ (10000 sessions)\n",
      "Batch 18/18... ✓ (5059 sessions)\n",
      "\n",
      "Combinaison finale...\n",
      "\n",
      "================================================================================\n",
      "✅ 24 FEATURES PAR SESSION CRÉÉES (ULTRA-OPTIMISÉ)\n",
      "================================================================================\n",
      "Shape: (175059, 26)\n",
      "Sessions par user (moyenne): 9.7\n",
      "\n",
      "OPTIMISATIONS:\n",
      "  ✅ consistency_score simplifié (pas de date_range)\n",
      "  ✅ consecutive_days_inactive vectorisé\n",
      "  ✅ has_downgraded vectorisé\n",
      "  ✅ listening_time_ratio_7d_vs_lifetime simplifié (ratio totaux)\n",
      "  ❌ avg_time_in_session_without_music_14d enlevée (trop lente)\n",
      "\n",
      "    userId  sessionId        session_time  negative_actions_last7d_vs_avg  \\\n",
      "0  1000025      23706 2018-10-02 08:59:29                             1.0   \n",
      "1  1000025      31688 2018-10-02 18:12:22                             1.0   \n",
      "2  1000025      39243 2018-10-04 01:04:35                             1.0   \n",
      "3  1000025      42490 2018-10-05 01:36:46                             1.0   \n",
      "4  1000025      45191 2018-10-06 22:09:33                             1.0   \n",
      "\n",
      "   consistency_score  consecutive_days_inactive  session_frequency_change  \\\n",
      "0           0.000000                          0                       1.0   \n",
      "1                NaN                          0                       1.0   \n",
      "2           0.471405                          0                       2.0   \n",
      "3           0.433013                          0                       3.0   \n",
      "4           0.400000                          0                       4.0   \n",
      "\n",
      "   sessions_last7d_vs_avg  thumbs_down_last_14days  days_without_thumbs_up  \\\n",
      "0                1.000000                        0                      83   \n",
      "1                0.142857                        0                       0   \n",
      "2                0.285714                        5                       0   \n",
      "3                0.428571                        8                       0   \n",
      "4                0.714286                        8                       2   \n",
      "\n",
      "   ...  frustration_score  is_paid  has_downgraded  mobile_usage_ratio_14d  \\\n",
      "0  ...                0.0        0               0                     0.5   \n",
      "1  ...                1.5        0               0                     0.0   \n",
      "2  ...               19.0        1               0                     0.0   \n",
      "3  ...               26.5        1               0                     0.0   \n",
      "4  ...               26.5        1               0                     0.0   \n",
      "\n",
      "   days_since_registration  has_ever_paid  thumbs_down_lifetime  \\\n",
      "0                       83              0                     0   \n",
      "1                       84              0                     0   \n",
      "2                       85              1                     5   \n",
      "3                       86              1                     8   \n",
      "4                       88              1                     8   \n",
      "\n",
      "   thumbs_up_lifetime  frustration_score_lifetime  \\\n",
      "0                   0                         0.0   \n",
      "1                   1                         1.5   \n",
      "2                  26                        19.0   \n",
      "3                  38                        26.5   \n",
      "4                  38                        26.5   \n",
      "\n",
      "   listening_time_ratio_7d_vs_lifetime  \n",
      "0                             1.000000  \n",
      "1                             0.142857  \n",
      "2                             0.285714  \n",
      "3                             0.428571  \n",
      "4                             0.714286  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "best_features = create_features_per_session_optimized(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd307c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features.to_csv('best_features.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb49f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction améliorée pour calculer les préférences musicales par user\n",
    "+ Encodage pour le ML\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAPPING DES GENRES\n",
    "# ============================================================================\n",
    "\n",
    "GENRE_MAPPING = {\n",
    "    'Kings Of Leon': 'Rock Alternatif',\n",
    "    'Coldplay': 'Pop Rock',\n",
    "    'Florence + The Machine': 'Indie Pop',\n",
    "    'The Black Keys': 'Blues Rock',\n",
    "    'Björk': 'Electronic',\n",
    "    'BjÃƒÂ¶rk': 'Electronic',\n",
    "    'Muse': 'Rock Alternatif',\n",
    "    'Jack Johnson': 'Folk Rock',\n",
    "    'Dwight Yoakam': 'Country',\n",
    "    'Justin Bieber': 'Pop',\n",
    "    'Train': 'Pop Rock',\n",
    "    'Eminem': 'Hip-Hop',\n",
    "    'Radiohead': 'Rock Alternatif',\n",
    "    'Taylor Swift': 'Pop',\n",
    "    'Alliance Ethnik': 'Hip-Hop',\n",
    "    'The Killers': 'Rock Alternatif',\n",
    "    'Linkin Park': 'Metal Alternatif',\n",
    "    'OneRepublic': 'Pop Rock',\n",
    "    'Metallica': 'Heavy Metal',\n",
    "    'John Mayer': 'Pop Rock',\n",
    "    'Evanescence': 'Metal Alternatif'\n",
    "}\n",
    "\n",
    "\n",
    "def create_user_preferences_improved(\n",
    "    df: pd.DataFrame, \n",
    "    genre_map: dict = GENRE_MAPPING,\n",
    "    encode: bool = True,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule les préférences musicales par user de manière robuste\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame avec colonnes userId, page, artist, song\n",
    "        genre_map: Dictionnaire {artiste: genre}\n",
    "        encode: Si True, encode les variables catégorielles pour ML\n",
    "        verbose: Afficher les statistiques\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame avec userId + préférences (encodées si encode=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*80)\n",
    "        print(\"CALCUL DES PRÉFÉRENCES MUSICALES PAR USER\")\n",
    "        print(\"=\"*80)\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # VALIDATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    required_cols = ['userId', 'page', 'artist', 'song']\n",
    "    missing = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}. Disponibles: {df.columns.tolist()}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FILTRAGE ET NETTOYAGE\n",
    "    # ========================================================================\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Dataset initial: {df.shape}\")\n",
    "    \n",
    "    # Filtrer NextSong uniquement\n",
    "    df_songs = df[df['page'] == 'NextSong'].copy()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Événements NextSong: {len(df_songs):,}\")\n",
    "    \n",
    "    # Nettoyer les NaN\n",
    "    before_clean = len(df_songs)\n",
    "    df_songs = df_songs.dropna(subset=['artist', 'song', 'userId'])\n",
    "    after_clean = len(df_songs)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Après nettoyage NaN: {after_clean:,} (supprimé: {before_clean - after_clean:,})\")\n",
    "    \n",
    "    # Convertir userId en string\n",
    "    df_songs['userId'] = df_songs['userId'].astype(str)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MAPPING DES GENRES\n",
    "    # ========================================================================\n",
    "    \n",
    "    df_songs['genre'] = df_songs['artist'].map(genre_map).fillna('Autre')\n",
    "    \n",
    "    if verbose:\n",
    "        genre_dist = df_songs['genre'].value_counts()\n",
    "        print(f\"\\nDistribution des genres:\")\n",
    "        print(genre_dist.head(10))\n",
    "        print(f\"Artistes 'Autre' (non mappés): {(df_songs['genre'] == 'Autre').sum():,}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CALCUL DES PRÉFÉRENCES PAR USER\n",
    "    # ========================================================================\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Calcul des préférences par user...\")\n",
    "    \n",
    "    # Fonction robuste pour trouver le mode\n",
    "    def safe_mode(series):\n",
    "        \"\"\"Retourne le mode, ou None si vide\"\"\"\n",
    "        if series.empty:\n",
    "            return None\n",
    "        modes = series.mode()\n",
    "        return modes.iloc[0] if len(modes) > 0 else None\n",
    "    \n",
    "    # Groupby et agrégation\n",
    "    favorites = df_songs.groupby('userId').agg(\n",
    "        favorite_artist=('artist', safe_mode),\n",
    "        favorite_song=('song', safe_mode),\n",
    "        favorite_genre=('genre', safe_mode),\n",
    "        total_songs_listened=('song', 'count'),  # Bonus: volume d'écoute\n",
    "        unique_artists=('artist', 'nunique'),     # Bonus: diversité artistes\n",
    "        unique_genres=('genre', 'nunique')        # Bonus: diversité genres\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Gérer les None (users sans données valides)\n",
    "    favorites['favorite_artist'] = favorites['favorite_artist'].fillna('Unknown')\n",
    "    favorites['favorite_song'] = favorites['favorite_song'].fillna('Unknown')\n",
    "    favorites['favorite_genre'] = favorites['favorite_genre'].fillna('Unknown')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Users avec préférences: {len(favorites):,}\")\n",
    "        print()\n",
    "        print(\"Statistiques:\")\n",
    "        print(f\"  Moyenne songs écoutées: {favorites['total_songs_listened'].mean():.1f}\")\n",
    "        print(f\"  Moyenne artistes uniques: {favorites['unique_artists'].mean():.1f}\")\n",
    "        print(f\"  Moyenne genres uniques: {favorites['unique_genres'].mean():.1f}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # ENCODAGE POUR ML (si demandé)\n",
    "    # ========================================================================\n",
    "    \n",
    "    if encode:\n",
    "        if verbose:\n",
    "            print(\"Encodage des variables catégorielles...\")\n",
    "        \n",
    "        # Option 1: Label Encoding (compact, bon pour tree-based)\n",
    "        le_artist = LabelEncoder()\n",
    "        le_song = LabelEncoder()\n",
    "        le_genre = LabelEncoder()\n",
    "        \n",
    "        favorites['favorite_artist_encoded'] = le_artist.fit_transform(favorites['favorite_artist'])\n",
    "        favorites['favorite_song_encoded'] = le_song.fit_transform(favorites['favorite_song'])\n",
    "        favorites['favorite_genre_encoded'] = le_genre.fit_transform(favorites['favorite_genre'])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  favorite_artist: {len(le_artist.classes_)} classes\")\n",
    "            print(f\"  favorite_song: {len(le_song.classes_)} classes\")\n",
    "            print(f\"  favorite_genre: {len(le_genre.classes_)} classes\")\n",
    "            print()\n",
    "        \n",
    "        # Garder aussi les versions non-encodées pour référence\n",
    "        # (utile pour l'interprétation)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*80)\n",
    "        print(\"✅ PRÉFÉRENCES CALCULÉES\")\n",
    "        print(\"=\"*80)\n",
    "        print()\n",
    "        print(favorites.head(10))\n",
    "        print()\n",
    "    \n",
    "    return favorites\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FONCTION ALTERNATIVE : ONE-HOT ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "def create_user_preferences_onehot(\n",
    "    df: pd.DataFrame,\n",
    "    genre_map: dict = GENRE_MAPPING,\n",
    "    top_n_artists: int = 50,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Version avec One-Hot Encoding (meilleur pour genres, plus lourd pour artistes)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame source\n",
    "        genre_map: Mapping artistes → genres\n",
    "        top_n_artists: Garder seulement les N artistes les plus populaires\n",
    "        verbose: Afficher stats\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame avec userId + préférences one-hot encodées\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculer préférences de base\n",
    "    favorites = create_user_preferences_improved(df, genre_map, encode=False, verbose=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"One-Hot Encoding...\")\n",
    "    \n",
    "    # One-hot pour genres (peu de classes)\n",
    "    genre_dummies = pd.get_dummies(favorites['favorite_genre'], prefix='genre')\n",
    "    \n",
    "    # Pour artistes : garder seulement top N (sinon trop de colonnes)\n",
    "    top_artists = favorites['favorite_artist'].value_counts().head(top_n_artists).index\n",
    "    favorites['favorite_artist_grouped'] = favorites['favorite_artist'].apply(\n",
    "        lambda x: x if x in top_artists else 'Other'\n",
    "    )\n",
    "    artist_dummies = pd.get_dummies(favorites['favorite_artist_grouped'], prefix='artist')\n",
    "    \n",
    "    # Combiner\n",
    "    result = pd.concat([\n",
    "        favorites[['userId', 'total_songs_listened', 'unique_artists', 'unique_genres']],\n",
    "        genre_dummies,\n",
    "        artist_dummies\n",
    "    ], axis=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nShape finale: {result.shape}\")\n",
    "        print(f\"Colonnes one-hot créées: {result.shape[1] - 4}\")\n",
    "        print()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41f743ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"data/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4aebfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1. CHARGER DONNÉES\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m df_full = \u001b[43mtrain\u001b[49m.copy()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 2. CALCULER PRÉFÉRENCES MUSICALES\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m music_prefs = create_user_preferences_improved(\n\u001b[32m     16\u001b[39m     df_full,\n\u001b[32m     17\u001b[39m     encode=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     18\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     19\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Fonction pour aggréger les deux dataset de features\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CHARGER DONNÉES\n",
    "# ============================================================================\n",
    "\n",
    "df_full = train.copy()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CALCULER PRÉFÉRENCES MUSICALES\n",
    "# ============================================================================\n",
    "\n",
    "music_prefs = create_user_preferences_improved(\n",
    "    df_full,\n",
    "    encode=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. MERGER AVEC LES FEATURES DE SESSIONS\n",
    "# ============================================================================\n",
    "\n",
    "features_sessions = best_features\n",
    "\n",
    "features_final = features_sessions.merge(\n",
    "    music_prefs[[\n",
    "        'userId', \n",
    "        'favorite_genre_encoded',      # Pour ML\n",
    "        'favorite_artist_encoded',     # Pour ML\n",
    "        'total_songs_listened',        # Volume\n",
    "        'unique_artists',              # Diversité\n",
    "        'unique_genres',               # Diversité\n",
    "        'favorite_genre',              # Pour interprétation (garder aussi le texte)\n",
    "        'favorite_artist'              # Pour interprétation\n",
    "    ]], \n",
    "    on='userId', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remplir les NaN (users sans préférences)\n",
    "features_final['favorite_genre_encoded'].fillna(-1, inplace=True)\n",
    "features_final['favorite_artist_encoded'].fillna(-1, inplace=True)\n",
    "features_final['total_songs_listened'].fillna(0, inplace=True)\n",
    "features_final['unique_artists'].fillna(0, inplace=True)\n",
    "features_final['unique_genres'].fillna(0, inplace=True)\n",
    "\n",
    "print(f\"Shape finale: {features_final.shape}\")\n",
    "print(features_final.head())\n",
    "\n",
    "# ============================================================================\n",
    "# 4. SAUVEGARDER\n",
    "# ============================================================================\n",
    "\n",
    "features_final.to_csv('features_with_music_encoded.csv', index=False)\n",
    "print(\"✅ Sauvegardé avec préférences musicales encodées\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d9470ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>session_time</th>\n",
       "      <th>negative_actions_last7d_vs_avg</th>\n",
       "      <th>consistency_score</th>\n",
       "      <th>consecutive_days_inactive</th>\n",
       "      <th>session_frequency_change</th>\n",
       "      <th>sessions_last7d_vs_avg</th>\n",
       "      <th>thumbs_down_last_14days</th>\n",
       "      <th>days_without_thumbs_up</th>\n",
       "      <th>...</th>\n",
       "      <th>has_ever_paid</th>\n",
       "      <th>thumbs_down_lifetime</th>\n",
       "      <th>thumbs_up_lifetime</th>\n",
       "      <th>frustration_score_lifetime</th>\n",
       "      <th>listening_time_ratio_7d_vs_lifetime</th>\n",
       "      <th>favorite_genre_encoded</th>\n",
       "      <th>favorite_artist_encoded</th>\n",
       "      <th>total_songs_listened</th>\n",
       "      <th>unique_artists</th>\n",
       "      <th>unique_genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>23706</td>\n",
       "      <td>2018-10-02 08:59:29</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000025</td>\n",
       "      <td>31688</td>\n",
       "      <td>2018-10-02 18:12:22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000025</td>\n",
       "      <td>39243</td>\n",
       "      <td>2018-10-04 01:04:35</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000025</td>\n",
       "      <td>42490</td>\n",
       "      <td>2018-10-05 01:36:46</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000025</td>\n",
       "      <td>45191</td>\n",
       "      <td>2018-10-06 22:09:33</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175054</th>\n",
       "      <td>1999905</td>\n",
       "      <td>94383</td>\n",
       "      <td>2018-10-20 15:00:08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.250745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175055</th>\n",
       "      <td>1999905</td>\n",
       "      <td>105570</td>\n",
       "      <td>2018-10-23 13:25:14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.353185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175056</th>\n",
       "      <td>1999905</td>\n",
       "      <td>115721</td>\n",
       "      <td>2018-10-26 06:51:51</td>\n",
       "      <td>0.622857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.935355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175057</th>\n",
       "      <td>1999905</td>\n",
       "      <td>126577</td>\n",
       "      <td>2018-10-26 15:25:08</td>\n",
       "      <td>0.498990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.178571</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.231033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175058</th>\n",
       "      <td>1999905</td>\n",
       "      <td>127830</td>\n",
       "      <td>2018-10-31 15:37:27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.601064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175059 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userId  sessionId        session_time  \\\n",
       "0       1000025      23706 2018-10-02 08:59:29   \n",
       "1       1000025      31688 2018-10-02 18:12:22   \n",
       "2       1000025      39243 2018-10-04 01:04:35   \n",
       "3       1000025      42490 2018-10-05 01:36:46   \n",
       "4       1000025      45191 2018-10-06 22:09:33   \n",
       "...         ...        ...                 ...   \n",
       "175054  1999905      94383 2018-10-20 15:00:08   \n",
       "175055  1999905     105570 2018-10-23 13:25:14   \n",
       "175056  1999905     115721 2018-10-26 06:51:51   \n",
       "175057  1999905     126577 2018-10-26 15:25:08   \n",
       "175058  1999905     127830 2018-10-31 15:37:27   \n",
       "\n",
       "        negative_actions_last7d_vs_avg  consistency_score  \\\n",
       "0                             1.000000           0.000000   \n",
       "1                             1.000000                NaN   \n",
       "2                             1.000000           0.471405   \n",
       "3                             1.000000           0.433013   \n",
       "4                             1.000000           0.400000   \n",
       "...                                ...                ...   \n",
       "175054                        0.000000           0.433013   \n",
       "175055                        0.000000           0.000000   \n",
       "175056                        0.622857           0.000000   \n",
       "175057                        0.498990           0.000000   \n",
       "175058                        0.000000           0.400000   \n",
       "\n",
       "        consecutive_days_inactive  session_frequency_change  \\\n",
       "0                               0                  1.000000   \n",
       "1                               0                  1.000000   \n",
       "2                               0                  2.000000   \n",
       "3                               0                  3.000000   \n",
       "4                               0                  4.000000   \n",
       "...                           ...                       ...   \n",
       "175054                          4                  0.333333   \n",
       "175055                          4                  2.000000   \n",
       "175056                          4                  1.000000   \n",
       "175057                          2                  3.000000   \n",
       "175058                          2                  0.666667   \n",
       "\n",
       "        sessions_last7d_vs_avg  thumbs_down_last_14days  \\\n",
       "0                     1.000000                        0   \n",
       "1                     0.142857                        0   \n",
       "2                     0.285714                        5   \n",
       "3                     0.428571                        8   \n",
       "4                     0.714286                        8   \n",
       "...                        ...                      ...   \n",
       "175054                0.457143                        1   \n",
       "175055                0.904762                        1   \n",
       "175056                0.857143                        2   \n",
       "175057                1.178571                        1   \n",
       "175058                0.857143                        1   \n",
       "\n",
       "        days_without_thumbs_up  ...  has_ever_paid  thumbs_down_lifetime  \\\n",
       "0                           83  ...              0                     0   \n",
       "1                            0  ...              0                     0   \n",
       "2                            0  ...              1                     5   \n",
       "3                            0  ...              1                     8   \n",
       "4                            2  ...              1                     8   \n",
       "...                        ...  ...            ...                   ...   \n",
       "175054                      12  ...              0                     2   \n",
       "175055                       2  ...              0                     2   \n",
       "175056                       2  ...              0                     3   \n",
       "175057                       3  ...              0                     3   \n",
       "175058                       4  ...              0                     3   \n",
       "\n",
       "        thumbs_up_lifetime  frustration_score_lifetime  \\\n",
       "0                        0                         0.0   \n",
       "1                        1                         1.5   \n",
       "2                       26                        19.0   \n",
       "3                       38                        26.5   \n",
       "4                       38                        26.5   \n",
       "...                    ...                         ...   \n",
       "175054                   5                         7.0   \n",
       "175055                   6                         7.0   \n",
       "175056                   8                         9.0   \n",
       "175057                   8                         9.0   \n",
       "175058                   9                         9.0   \n",
       "\n",
       "        listening_time_ratio_7d_vs_lifetime  favorite_genre_encoded  \\\n",
       "0                                  1.000000                     0.0   \n",
       "1                                  0.142857                     0.0   \n",
       "2                                  0.285714                     0.0   \n",
       "3                                  0.428571                     0.0   \n",
       "4                                  0.714286                     0.0   \n",
       "...                                     ...                     ...   \n",
       "175054                             0.250745                     0.0   \n",
       "175055                             0.353185                     0.0   \n",
       "175056                             0.935355                     0.0   \n",
       "175057                             1.231033                     0.0   \n",
       "175058                             0.601064                     0.0   \n",
       "\n",
       "        favorite_artist_encoded  total_songs_listened  unique_artists  \\\n",
       "0                         405.0                1662.0          1162.0   \n",
       "1                         405.0                1662.0          1162.0   \n",
       "2                         405.0                1662.0          1162.0   \n",
       "3                         405.0                1662.0          1162.0   \n",
       "4                         405.0                1662.0          1162.0   \n",
       "...                         ...                   ...             ...   \n",
       "175054                     35.0                 259.0           230.0   \n",
       "175055                     35.0                 259.0           230.0   \n",
       "175056                     35.0                 259.0           230.0   \n",
       "175057                     35.0                 259.0           230.0   \n",
       "175058                     35.0                 259.0           230.0   \n",
       "\n",
       "        unique_genres  \n",
       "0                11.0  \n",
       "1                11.0  \n",
       "2                11.0  \n",
       "3                11.0  \n",
       "4                11.0  \n",
       "...               ...  \n",
       "175054           10.0  \n",
       "175055           10.0  \n",
       "175056           10.0  \n",
       "175057           10.0  \n",
       "175058           10.0  \n",
       "\n",
       "[175059 rows x 31 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "76e47acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_time = churners[['userId', 'time']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b470f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_clean  = features_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3398cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_clean.to_csv('features_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ffa59116",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_time.to_csv('churn_time.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2c149e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>1749042</td>\n",
       "      <td>2018-10-21 01:16:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5923</th>\n",
       "      <td>1222580</td>\n",
       "      <td>2018-10-30 23:17:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17878</th>\n",
       "      <td>1385500</td>\n",
       "      <td>2018-11-17 04:54:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46340</th>\n",
       "      <td>1144647</td>\n",
       "      <td>2018-10-28 02:43:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58317</th>\n",
       "      <td>1240184</td>\n",
       "      <td>2018-11-14 21:43:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17123383</th>\n",
       "      <td>1835694</td>\n",
       "      <td>2018-10-09 14:49:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17167280</th>\n",
       "      <td>1895668</td>\n",
       "      <td>2018-11-15 14:21:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17233170</th>\n",
       "      <td>1353786</td>\n",
       "      <td>2018-11-12 23:15:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17269806</th>\n",
       "      <td>1652329</td>\n",
       "      <td>2018-11-02 17:32:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17299902</th>\n",
       "      <td>1030763</td>\n",
       "      <td>2018-11-18 18:51:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3435 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           userId                time\n",
       "1222      1749042 2018-10-21 01:16:24\n",
       "5923      1222580 2018-10-30 23:17:30\n",
       "17878     1385500 2018-11-17 04:54:09\n",
       "46340     1144647 2018-10-28 02:43:58\n",
       "58317     1240184 2018-11-14 21:43:36\n",
       "...           ...                 ...\n",
       "17123383  1835694 2018-10-09 14:49:14\n",
       "17167280  1895668 2018-11-15 14:21:59\n",
       "17233170  1353786 2018-11-12 23:15:05\n",
       "17269806  1652329 2018-11-02 17:32:12\n",
       "17299902  1030763 2018-11-18 18:51:15\n",
       "\n",
       "[3435 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a1efdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Drop des colonnes de textes si besoin\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mfeatures_final\u001b[49m.drop(columns = [\u001b[33m'\u001b[39m\u001b[33mfavorite_genre\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfavorite_artist\u001b[39m\u001b[33m'\u001b[39m], inplace = \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'features_final' is not defined"
     ]
    }
   ],
   "source": [
    "# Drop des colonnes de textes si besoin\n",
    "features_final.drop(columns = ['favorite_genre', 'favorite_artist'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3438812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will_churn_10days\n",
      "0    156402\n",
      "1     18657\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# On rajoute la colonne target\n",
    "import pandas as pd\n",
    "\n",
    "# Charger\n",
    "features = pd.read_csv('features_clean.csv')\n",
    "churn_time = pd.read_csv('churn_time.csv')\n",
    "\n",
    "# Convertir\n",
    "features['session_time'] = pd.to_datetime(features['session_time'])\n",
    "churn_time['time'] = pd.to_datetime(churn_time['time'])\n",
    "churn_time = churn_time.rename(columns={'time': 'churn_date'})\n",
    "\n",
    "# Merger\n",
    "features = features.merge(churn_time[['userId', 'churn_date']], on='userId', how='left')\n",
    "\n",
    "# Target\n",
    "features['days_until_churn'] = (features['churn_date'] - features['session_time']).dt.days\n",
    "features['will_churn_10days'] = ((features['days_until_churn'] >= 0) & \n",
    "                                  (features['days_until_churn'] <= 10)).astype(int)\n",
    "\n",
    "# Drop\n",
    "features = features.drop(['churn_date', 'days_until_churn'], axis=1)\n",
    "\n",
    "print(features['will_churn_10days'].value_counts())\n",
    "features.to_csv('features_with_target.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33114570",
   "metadata": {},
   "source": [
    "### Application of the function to the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656a563",
   "metadata": {},
   "source": [
    "### Remove all previous sessions from churners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd85b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def keep_only_consistent_users(features):\n",
    "    \"\"\"\n",
    "    Conserve uniquement :\n",
    "    - Les utilisateurs qui n'ont JAMAIS churné (tous les will_churn_10days=0).\n",
    "    - Les sessions avec will_churn_10days=1 (provenant des utilisateurs qui churnent).\n",
    "    \n",
    "    Retire : les sessions avec will_churn_10days=0 provenant d'utilisateurs qui churnent ultérieurement.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Calculer le statut de churn maximal pour chaque utilisateur (1 si l'utilisateur a churné au moins une fois)\n",
    "    max_churn_per_user = features.groupby('userId')['will_churn_10days'].transform('max')\n",
    "    \n",
    "    # 2. Créer le masque de filtrage\n",
    "    # On garde les lignes si :\n",
    "    # A) L'utilisateur est un \"non-churner\" (max_churn_per_user == 0)\n",
    "    # OU\n",
    "    # B) La session elle-même est une session de churn (features['will_churn_10days'] == 1)\n",
    "    mask = (max_churn_per_user == 0) | (features['will_churn_10days'] == 1)\n",
    "    \n",
    "    # 3. Appliquer le filtre\n",
    "    features_filtered = features[mask].copy()\n",
    "    \n",
    "    return features_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dab557b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('features_with_target.csv')  # Déjà nettoyé des post-churn\n",
    "df_consistant = keep_only_consistent_users(features)\n",
    "df_consistant.to_csv('df_consistant.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1a49f",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a7139dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def XGB_training(df_features):\n",
    "\n",
    "    # Prepare data\n",
    "    feature_cols = [col for col in df_features.columns \n",
    "                    if col not in ['userId', 'prediction_date','session_start','session_id', 'session_time','time','registration', 'will_churn_10days']]\n",
    "\n",
    "    X = df_features[feature_cols].fillna(0)\n",
    "    y = df_features['will_churn_10days']\n",
    "\n",
    "    # Split (NO resampling here!)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Calculate scale_pos_weight: ratio of negative to positive class\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "    print(f\"Class distribution in train:\")\n",
    "    print(y_train.value_counts())\n",
    "    print(f\"\\nCalculated scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "    # XGBoost with optimal parameters for churn prediction\n",
    "    model = xgb.XGBClassifier(\n",
    "        # Handle imbalanced classes\n",
    "        scale_pos_weight=scale_pos_weight,  # CRITICAL for imbalanced data\n",
    "        \n",
    "        # Model complexity\n",
    "        n_estimators=200,                   # Number of boosting rounds\n",
    "        max_depth=6,                        # Tree depth (prevent overfitting)\n",
    "        learning_rate=0.01,                 # Lower = more robust but slower\n",
    "        \n",
    "        # Optimization\n",
    "        objective='binary:logistic',        # Binary classification,                 \n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Train (simplified - no need to pass eval_metric again)\n",
    "    print(\"\\nTraining XGBoost model...\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "    )\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"balanced accuracy: {balanced_accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "    # Feature importance\n",
    "    import pandas as pd\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    print(\"\\nTop 30 Most Important Features:\")\n",
    "    print(feature_importance.head(30).to_string(index=False))\n",
    "    return X_test, y_test, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cf6b2ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in train:\n",
      "will_churn_10days\n",
      "0    105729\n",
      "1     14926\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Calculated scale_pos_weight: 7.08\n",
      "\n",
      "Training XGBoost model...\n",
      "balanced accuracy: 0.8045054171390154\n",
      "ROC-AUC: 0.8896\n",
      "\n",
      "Confusion Matrix:\n",
      "[[20951  5482]\n",
      " [  685  3046]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Churn       0.97      0.79      0.87     26433\n",
      "       Churn       0.36      0.82      0.50      3731\n",
      "\n",
      "    accuracy                           0.80     30164\n",
      "   macro avg       0.66      0.80      0.68     30164\n",
      "weighted avg       0.89      0.80      0.83     30164\n",
      "\n",
      "\n",
      "Top 30 Most Important Features:\n",
      "                            feature  importance\n",
      "                  frustration_score    0.186231\n",
      "               thumbs_down_lifetime    0.175407\n",
      "         frustration_score_lifetime    0.092902\n",
      "                     unique_artists    0.054654\n",
      "               total_songs_listened    0.054346\n",
      "         songs_listened_last_14days    0.045495\n",
      "             mobile_usage_ratio_14d    0.038870\n",
      "                  consistency_score    0.038488\n",
      "          has_downgrade_last_15days    0.035252\n",
      "                          sessionId    0.034902\n",
      "                     has_downgraded    0.030838\n",
      "                      unique_genres    0.029894\n",
      "                 thumbs_up_lifetime    0.028253\n",
      "          consecutive_days_inactive    0.022665\n",
      "           session_frequency_change    0.021772\n",
      "             days_without_thumbs_up    0.018059\n",
      "                            is_paid    0.016663\n",
      "         activity_trend_last_14days    0.016485\n",
      "                      has_ever_paid    0.011087\n",
      "             sessions_last7d_vs_avg    0.010118\n",
      "                    help_visits_14d    0.006277\n",
      "            favorite_artist_encoded    0.005909\n",
      "listening_time_ratio_7d_vs_lifetime    0.005423\n",
      "            days_since_registration    0.005264\n",
      "                settings_visits_14d    0.005093\n",
      "            thumbs_down_last_14days    0.004158\n",
      "     negative_actions_last7d_vs_avg    0.002914\n",
      "                     error_rate_14d    0.002579\n",
      "             favorite_genre_encoded    0.000000\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test, model_XGB = XGB_training(df_consistant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
